<%
  require "../eruby_util.rb"
%>
<%
  chapter(
    '13',
    %q{Quantum Physics},
    'ch:quantum',
    '',
    {'opener'=>''}
  )
%>

<% begin_sec("Rules of Randomness",0,'randomness') %>
\epigraphlong{Given for one instant an intelligence which could comprehend
all the forces by which nature is animated and the
respective positions of the things which compose it...nothing
would be uncertain, and the future as the past would be laid
out before its eyes.}{Pierre Simon de Laplace, 1776}\index{Laplace, Pierre Simon de}

\epigraphlong{The energy produced by the atom is a very poor kind of
thing. Anyone who expects a source of power from the
transformation of these atoms is talking moonshine.}{Ernest Rutherford, 1933}
\index{Rutherford!Ernest}

\epigraphlong{The Quantum Mechanics is very imposing. But an inner voice
tells me that it is still not the final truth. The theory
yields much, but it hardly brings us nearer to the secret of
the Old One. In any case, I am convinced that He does not play 
dice.}{Albert Einstein}\index{Einstein, Albert}

<% marg(50) %>
<%
  fig(
    'volcano',
    %q{%
      In 1980, the continental U.S. got its first
      taste of active volcanism in recent memory with the eruption of
      Mount St. Helens.
    }
  )
%>
<% end_marg %>
However radical \index{Newton!Isaac}Newton's clockwork
universe seemed to his contemporaries, by the early
twentieth century it had become a sort of smugly accepted
dogma. Luckily for us, this deterministic picture of the
universe breaks down at the atomic level. The clearest
demonstration that the laws of physics contain elements of
randomness is in the behavior of radioactive atoms. Pick two
identical atoms of a radioactive isotope, say the naturally
occurring uranium 238, and watch them carefully. They will
decay at different times, even though there was no
difference in their initial behavior.

We would be in big trouble if these atoms' behavior was as
predictable as expected in the Newtonian world-view, because
radioactivity is an important source of heat for our planet.
In reality, each atom chooses a random moment at which to
release its energy, resulting in a nice steady heating
effect. The earth would be a much colder planet if only
sunlight heated it and not radioactivity. Probably there
would be no volcanoes, and the oceans would never have been
liquid. The deep-sea \index{geothermal vents}geothermal
vents in which life first evolved would never have existed.
But there would be an even worse consequence if radioactivity
was deterministic: after a few billion years of peace, all
the uranium 238 atoms in our planet would presumably pick
the same moment to decay. The huge amount of stored nuclear
energy, instead of being spread out over eons, would all be
released at one instant, blowing our whole planet to 
Kingdom Come.\footnote{This is under the assumption that all the uranium
atoms were created at the same time. In reality, we have only
a general idea of the processes that might have created the
heavy elements in the nebula from which our solar system
condensed. Some portion of them may have come from nuclear reactions
in supernova explosions in that particular nebula, but some may
have come from previous supernova explosions throughout our
galaxy, or from exotic events like collisions of white dwarf
stars.}

The new version of physics, incorporating certain kinds of
randomness, is called quantum physics (for reasons that will
become clear later). It represented such a dramatic break
with the previous, deterministic tradition that everything
that came before is considered ``\index{classical physics}classical,''
even the theory of relativity. This chapter is
a basic introduction to \index{quantum physics}quantum physics.

\startdq

\begin{dq}
I said ``Pick two identical atoms of a radioactive
isotope.'' Are two atoms really identical? If their
electrons are orbiting the nucleus, can we distinguish each
atom by the particular arrangement of its electrons at
some instant in time?
\end{dq}

<% begin_sec("Randomness isn't random.") %>

\index{Einstein!and randomness}Einstein's distaste for
\index{randomness}randomness, and his association of
determinism with divinity, goes back to the \index{Enlightenment}Enlightenment
conception of the universe as a gigantic piece of clockwork
that only had to be set in motion initially by the Builder.
Many of the founders of quantum mechanics were interested in
possible links between physics and Eastern and Western
religious and philosophical thought, but every educated
person has a different concept of religion and philosophy.
Bertrand \index{Russell!Bertrand}Russell remarked, ``Sir
Arthur \index{Eddington!Arthur}Eddington deduces religion
from the fact that atoms do not obey the laws of mathematics.
Sir James \index{Jeans!James}Jeans deduces it from the
fact that they do.''

Russell's witticism, which implies incorrectly that
mathematics cannot describe randomness, remind us how
important it is not to oversimplify this question of
randomness. You should not simply surmise, ``Well, it's all
random, anything can happen.'' For one thing, certain things
simply cannot happen, either in classical physics or quantum
physics. The conservation laws of mass, energy, momentum,
and angular momentum are still valid, so for instance
processes that create energy out of nothing are not just
unlikely according to quantum physics, they are impossible.

A useful analogy can be made with the role of randomness in
\index{evolution!randomness in}evolution. \index{Darwin, Charles}Darwin
was not the first biologist to suggest that species changed
over long periods of time. His two new fundamental ideas
were that (1) the changes arose through random genetic
variation, and (2) changes that enhanced the organism's
ability to survive and reproduce would be preserved, while
maladaptive changes would be eliminated by natural
selection. Doubters of evolution often consider only the
first point, about the randomness of natural variation, but
not the second point, about the systematic action of natural
selection. They make statements such as, ``the development
of a complex organism like Homo sapiens via random chance
would be like a whirlwind blowing through a junkyard and
spontaneously assembling a jumbo jet out of the scrap
metal.'' The flaw in this type of reasoning is that it
ignores the deterministic constraints on the results of
random processes. For an atom to violate conservation of
energy is no more likely than the conquest of the world by
chimpanzees next year.

\startdq

\begin{dq}
Economists often behave like wannabe physicists, probably
because it seems prestigious to make numerical calculations
instead of talking about human relationships and organizations
like other social scientists. Their striving to make
economics work like Newtonian physics extends to a parallel
use of mechanical metaphors, as in the concept of a market's
supply and demand acting like a self-adjusting machine, and
the idealization of people as economic automatons who
consistently strive to maximize their own wealth. What
evidence is there for randomness rather than mechanical
determinism in economics?
\end{dq}

<% end_sec() %>
<% begin_sec("Calculating randomness") %>
You should also realize that even if something is random, we
can still understand it, and we can still calculate
probabilities numerically. In other words, physicists are
good bookmakers. A good bookmaker can calculate the odds
that a horse will win a race much more accurately that an
inexperienced one, but nevertheless cannot predict what will
happen in any particular race.

<% begin_sec("Statistical independence") %>\index{independence!statistical}
As an illustration of a general technique for calculating
odds, suppose you are playing a 25-cent slot machine. Each
of the three wheels has one chance in ten of coming up with
a cherry. If all three wheels come up cherries, you win
\$100. Even though the results of any particular trial are
random, you can make certain quantitative predictions.
First, you can calculate that your odds of winning on any
given trial are $1/10\times1/10\times1/10=1/1000=0.001$.\label{slot-machine-game}
Here, I am
representing the probabilities as numbers from 0 to 1, which
is clearer than statements like ``The odds are 999 to 1,''
and makes the calculations easier. A probability of 0
represents something impossible, and a probability of 1
represents something that will definitely happen.  

Also, you can say that any given trial is equally likely to
result in a win, and it doesn't matter whether you have won
or lost in prior games. Mathematically, we say that each
trial is statistically independent, or that separate games
are uncorrelated.  Most gamblers are mistakenly convinced
that, to the contrary, games of chance are correlated. If
they have been playing a slot machine all day, they are
convinced that it is ``getting ready to pay,'' and they do
not want anyone else playing the machine and ``using up''
the jackpot that they ``have coming.'' In other words, they
are claiming that a series of trials at the slot machine is
negatively correlated, that losing now makes you more likely
to win later. Craps players claim that you should go to a
table where the person rolling the dice is ``hot,'' because
she is likely to keep on rolling good numbers. Craps
players, then, believe that rolls of the dice are positively
correlated, that winning now makes you more likely to win later.

My method of calculating the probability of winning on the
slot machine was an example of the following important rule
for calculations based on independent probabilities:

\index{independent probabilities!law of}
\begin{lessimportant}[the law of independent probabilities]
If the probability of one event happening is $P_A$, and the
probability of a second statistically independent event
happening is $P_B$, then the probability that they will both
occur is the product of the probabilities, $P_AP_B$. If
there are more than two events involved, you simply
keep on multiplying.
\end{lessimportant}

This can be taken as the definition of statistical independence.\index{independence!statistical}

Note that this only applies to independent probabilities.
For instance, if you have a nickel and a dime in your
pocket, and you randomly pull one out, there is a probability
of 0.5 that it will be the nickel. If you then replace the
coin and again pull one out randomly, there is again a
probability of 0.5 of coming up with the nickel, because the
probabilities are independent. Thus, there is a probability
of 0.25 that you will get the nickel both times.

Suppose instead that you do not replace the first coin
before pulling out the second one. Then you are bound to
pull out the other coin the second time, and there is no way
you could pull the nickel out twice. In this situation, the
two trials are not independent, because the result of the
first trial has an effect on the second trial. The law of
independent probabilities does not apply, and the probability
of getting the nickel twice is zero, not 0.25.

Experiments have shown that in the case of radioactive
decay, the probability that any nucleus will decay during a
given time interval is unaffected by what is happening to
the other nuclei, and is also unrelated to how long it has
gone without decaying. The first observation makes sense,
because nuclei are isolated from each other at the centers
of their respective atoms, and therefore have no physical
way of influencing each other. The second fact is also
reasonable, since all atoms are identical. Suppose we wanted
to believe that certain atoms were ``extra tough,'' as
demonstrated by their history of going an unusually long
time without decaying. Those atoms would have to be
different in some physical way, but nobody has ever
succeeded in detecting differences among atoms. There is no
way for an atom to be changed by the experiences it
has in its lifetime.

<% end_sec() %>
<% begin_sec("Addition of probabilities") %>\index{probabilities!addition of}

The law of independent probabilities tells us to use
multiplication to calculate the probability that both A and
B will happen, assuming the probabilities are independent.
What about the probability of an ``or'' rather than an
``and''? If two events A and $B$ are mutually exclusive,
then the probability of one or the other occurring is the
sum $P_A+P_B$. For instance, a bowler might have a 30\%
chance of getting a strike (knocking down all ten pins) and
a 20\% chance of knocking down nine of them. The bowler's
chance of knocking down either nine pins or ten pins is therefore 50\%.

It does not make sense to add probabilities of things that
are not mutually exclusive, i.e., that could both happen. Say
I have a 90\% chance of eating lunch on any given day, and a
90\% chance of eating dinner. The probability that I will
eat either lunch or dinner is not 180\%.  

<% end_sec() %>
<% begin_sec("Normalization") %>\index{probabilities!normalization of}\index{normalization}
If I spin a globe and randomly pick a point on it, I have
about a 70\% chance of picking a point that's in an ocean
and a 30\% chance of picking a point on land. The probability
of picking either water or land is $70\%+30\%=100\%$. Water
and land are mutually exclusive, and there are no other
possibilities, so the probabilities had to add up to 100\%.
It works the same if there are more than two possibilities
---  if you can classify all possible outcomes into a list
of mutually exclusive results, then all the probabilities
have to add up to 1, or 100\%. This property of probabilities
is known as normalization.
<% marg(20) %>
<%
  fig(
    'globe',
    %q{%
      Normalization: the probability of picking land plus
      the probability of picking water adds up to 1.
    }
  )
%>
<% end_marg %>

\index{averages}<% end_sec() %>
<% begin_sec("Averages") %>

Another way of dealing with randomness is to take averages.
The casino knows that in the long run, the number of times
you win will approximately equal the number of times you
play multiplied by the probability of winning. In the slot-machine game
described on page \pageref{slot-machine-game}, where the probability of winning is 0.001,
if you spend a week playing, and pay \$2500 to play 10,000
times, you are likely to win about 10 times $(10,000\times0.001=10)$,
and collect \$1000. On the average, the casino will make a
profit of \$1500 from you. This is an example of the following rule.

\index{averages!rule for calculating}
\begin{lessimportant}[Rule for Calculating Averages]
If you conduct $N$ identical, statistically independent
trials, and the probability of success in each trial is $P$,
then on the average, the total number of successful trials
will be $NP$. If $N$ is large enough, the relative
error in this estimate will become small.
\end{lessimportant}

The statement that the rule for calculating averages gets
more and more accurate for larger and larger $N$(known
popularly as the ``law of averages'') often provides a
correspondence principle that connects classical and quantum
physics. For instance, the amount of power produced by a
nuclear power plant is not random at any detectable level,
because the number of atoms in the reactor is so large. In
general, random behavior at the atomic level tends to
average out when we consider large numbers of atoms, which
is why physics seemed deterministic before physicists
learned techniques for studying atoms individually.

We can achieve great precision with averages in quantum
physics because we can use identical atoms to reproduce
exactly the same situation many times. If we were betting on
horses or dice, we would be much more limited in our
precision. After a thousand races, the horse would be ready
to retire. After a million rolls, the dice would be worn out.

<% self_check('independence',<<-'SELF_CHECK'
Which of the following things \emph{must} be independent,
which \emph{could} be independent, and which definitely are
\emph{not} independent?
(1) the probability of successfully making two free-throws
in a row in basketball; (2) the probability that it will rain in London tomorrow and
the probability that it will rain on the same day in a
certain city in a distant galaxy; 
(3) your probability of dying today and of dying tomorrow.
  SELF_CHECK
  ) %>

\startdqs

\begin{dq}
Newtonian physics is an essentially perfect approximation
for describing the motion of a pair of dice. If Newtonian
physics is deterministic, why do we consider the result of
rolling dice to be random?
\end{dq}

<% marg(10) %>
<%
  fig(
    'dice',
    %q{Why are dice random?}
  )
%>
<% end_marg %>

\begin{dq}
Why isn't it valid to define randomness by saying that
randomness is when all the outcomes are equally likely?
\end{dq}

\begin{dq}
The sequence of digits 121212121212121212 seems clearly
nonrandom, and 41592653589793 seems random. The latter
sequence, however, is the decimal form of pi, starting with
the third digit. There is a story about the Indian
mathematician Ramanujan, a self-taught prodigy, that a
friend came to visit him in a cab, and remarked that the
number of the cab, 1729, seemed relatively uninteresting.
Ramanujan replied that on the contrary, it was very
interesting because it was the smallest number that could be
represented in two different ways as the sum of two cubes.
The Argentine author Jorge Luis Borges wrote a short story
called ``The Library of Babel,'' in which he imagined a
library containing every book that could possibly be written
using the letters of the alphabet. It would include a book
containing only the repeated letter ``a;'' all the ancient
Greek tragedies known today, all the lost Greek tragedies,
and millions of Greek tragedies that were never actually
written; your own life story, and various incorrect versions
of your own life story; and countless anthologies containing
a short story called ``The Library of Babel.'' Of course, if
you picked a book from the shelves of the library, it would
almost certainly look like a nonsensical sequence of letters
and punctuation, but it's always possible that the seemingly
meaningless book would be a science-fiction screenplay
written in the language of a Neanderthal tribe, or the lyrics to a set of
incomparably beautiful love songs written in a language that
never existed. In view of these examples, what does it
really mean to say that something is random?
\end{dq}

<% end_sec() %>
<% end_sec() %>
<% begin_sec("Probability distributions") %>\index{probability distributions}
So far we've discussed random processes having only two
possible outcomes: yes or no, win or lose, on or off. More
generally, a random process could have a result that is a
number. Some processes yield integers, as when you roll a
die and get a result from one to six, but some are not
restricted to whole numbers, for example the number of
seconds that a uranium-238 atom will exist before undergoing radioactive decay.

<% marg(0) %>
<%
  fig(
    'single-die',
    %q{Probability distribution for the result of rolling a single die.}
  )
%>
\spacebetweenfigs
<%
  fig(
    'two-dice',
    %q{Rolling two dice and adding them up.}
  )
%>
<% end_marg %>
Consider a throw of a die. If the die is ``honest,'' then we
expect all six values to be equally likely. Since all six
probabilities must add up to 1, then probability of any
particular value coming up must be 1/6. We can summarize
this in a graph, \figref{single-die}. Areas under the curve can be
interpreted as total probabilities. For instance, the area
under the curve from 1 to 3 is $1/6+1/6+1/6=1/2$, so the
probability of getting a result from 1 to 3 is 1/2. The
function shown on the graph is called the probability distribution.

Figure \figref{two-dice} shows the probabilities of various results
obtained by rolling two dice and adding them together, as in
the game of craps. The probabilities are not all the same.
There is a small probability of getting a two, for example,
because there is only one way to do it, by rolling a one and
then another one. The probability of rolling a seven is high
because there are six different ways to do it: 1+6, 2+5, etc.

If the number of possible outcomes is large but finite, for
example the number of hairs on a dog, the graph would start
to look like a smooth curve rather than a ziggurat.

What about probability distributions for random numbers that
are not integers? We can no longer make a graph with
probability on the $y$ axis, because the probability of
getting a given exact number is typically zero. For
instance, there is zero probability that a radioactive atom
will last for \emph{exactly} 3 seconds, since there is are
infinitely many possible results that are close to 3 but not
exactly three: 2.999999999999999996876876587658465436, for
example. It doesn't usually make sense, therefore, to talk
about the probability of a single numerical result, but it
does make sense to talk about the probability of a certain
range of results. For instance, the probability that an atom
will last more than 3 and less than 4 seconds is a perfectly
reasonable thing to discuss. We can still summarize the
probability information on a graph, and we can still
interpret areas under the curve as probabilities.

But the $y$ axis can no longer be a unitless probability
scale. In radioactive decay, for example, we want the $x$
axis to have units of time, and we want areas under the
curve to be unitless probabilities. The area of a single
square on the graph paper is then
\begin{gather*}
\text{(unitless area of a square)}   \\
= \text{(width of square with time units)}\\
         \times \text{(height of square)}\eqquad.
\end{gather*}
If the units are to cancel out, then the height of the
square must evidently be a quantity with units of inverse
time. In other words, the $y$ axis of the graph is to be
interpreted as probability per unit time, not probability.

<% marg(30) %>
<%
  fig(
    'human-height',
    %q{%
      A probability distribution for height of human adults 
      (not real data).
    }
  )
%>
\spacebetweenfigs
<%
  fig(
    'human-height-tail',
    %q{Example \ref{eg:basketball}.}
  )
%>
\spacebetweenfigs
<%
  fig(
    'average',
    %q{The average of a probability distribution.}
  )
%>
<% end_marg %>
Figure \figref{human-height} shows another example, a probability distribution
for people's height. This kind of bell-shaped curve is quite common.

<% self_check('rangeofheights',<<-'SELF_CHECK'

Compare the number of people with heights in the range of
130-135 cm to the number in the range 135-140.
  SELF_CHECK
  ) %>

%%%%%%%%%%% basketball example %%%%%%%%%%%%%
__incl(eg/basketball)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\index{probability  distributions!widths of}
\index{probability  distributions!averages of}
<% begin_sec("Average and width of a probability distribution") %>
If the next Martian you meet asks you, ``How tall is an
adult human?,'' you will probably reply with a statement
about the average human height, such as ``Oh, about 5 feet 6
inches.'' If you wanted to explain a little more, you could
say, ``But that's only an average. Most people are somewhere
between 5 feet and 6 feet tall.'' Without bothering to draw
the relevant bell curve for your new extraterrestrial
acquaintance, you've summarized the relevant information by
giving an average and a typical range of variation.  

The average of a probability distribution can be defined
geometrically as the horizontal position at which it could
be balanced if it was constructed out of cardboard. A
convenient numerical measure of the amount of variation
about the average, or amount of uncertainty, is the full
width at half maximum, or FWHM, shown in figure \figref{fwhm}.\index{FWHM}\index{full width at half maximum}

A great deal more could be said about this topic, and indeed
an introductory statistics course could spend months on ways
of defining the center and width of a distribution. Rather
than force-feeding you on mathematical detail or techniques
for calculating these things, it is perhaps more relevant to
point out simply that there are various ways of defining
them, and to inoculate you against the misuse of certain definitions.

<% marg(0) %>
<%
  fig(
    'fwhm',
    %q{The full width at half maximum (FWHM) of a probability distribution.}
  )
%>
<% end_marg %>
The average is not the only possible way to say what is a
typical value for a quantity that can vary randomly; another
possible definition is the median,\index{median} defined as the value that
is exceeded with 50\% probability. When discussing incomes
of people living in a certain town, the average could be
very misleading, since it can be affected massively if a
single resident of the town is Bill Gates. Nor is the FWHM
the only possible way of stating the amount of random
variation; another possible way of measuring it is the
standard deviation (defined as the square root of the
average squared deviation from the average value).

 % 
<% end_sec() %>
<% end_sec() %>
<% begin_sec("Exponential decay and half-life",nil,'half-life') %>
\index{decay!exponential}\index{exponential decay}
<% begin_sec("Half-life") %>\index{half-life}
Most people know that radioactivity ``lasts a certain amount
of time,'' but that simple statement leaves out a lot. As an
example, consider the following medical procedure used to
diagnose thyroid function. A very small quantity of the
isotope $^{131}\zu{I}$, produced in a nuclear reactor, is fed to
or injected into the patient. The body's biochemical systems
treat this artificial, radioactive isotope exactly the same
as $^{127}\zu{I}$, which is the only naturally occurring type.
(Nutritionally, iodine is a necessary trace element. Iodine
taken into the body is partly excreted, but the rest becomes
concentrated in the thyroid gland. Iodized salt has had
iodine added to it to prevent the nutritional deficiency
known as \index{goiters}goiters, in which the \index{iodine}iodine-starved
thyroid becomes swollen.) As the $^{131}\zu{I}$ undergoes beta
decay, it emits electrons, neutrinos, and gamma rays. The
gamma rays can be measured by a detector passed over the
patient's body. As the radioactive iodine becomes concentrated
in the thyroid, the amount of gamma radiation coming from
the thyroid becomes greater, and that emitted by the rest of
the body is reduced. The rate at which the iodine concentrates
in the thyroid tells the doctor about the health of the thyroid.

If you ever undergo this procedure, someone will presumably
explain a little about radioactivity to you, to allay your
fears that you will turn into the Incredible Hulk, or that
your next child will have an unusual number of limbs. Since
iodine stays in your thyroid for a long time once it gets
there, one thing you'll want to know is whether your thyroid
is going to become radioactive forever. They may just tell
you that the radioactivity ``only lasts a certain amount of
time,'' but we can now carry out a quantitative derivation
of how the radioactivity really will die out.

Let $P_{surv}(t)$ be the probability that an iodine atom
will survive without decaying for a period of at least $t$.
It has been experimentally measured that half all $^{131}\zu{I}$
atoms decay in 8 hours, so we have
\begin{equation*}
                P_{surv}(8\ \zu{hr})         =  0.5\eqquad.  
\end{equation*}

Now using the law of independent probabilities, the
probability of surviving for 16 hours equals the probability
of surviving for the first 8 hours multiplied by the
probability of surviving for the second 8 hours,
\begin{align*}
                P_{surv}(16\ \zu{hr})         &=  0.50\times0.50  \\
                                 &=  0.25\eqquad.  
\end{align*}
Similarly we have
\begin{align*}
                P_{surv}(24\ \zu{hr})         &=  0.50\times0.5\times0.5  \\
                                 &=  0.125\eqquad.  
\end{align*}
Generalizing from this pattern, the probability of surviving
for any time $t$ that is a multiple of 8 hours is
\begin{equation*}
                P_{surv}(t)                 =            0.5^{t/8\ \zu{hr}}\eqquad.
\end{equation*}
We now know how to find the probability of survival at
intervals of 8 hours, but what about the points in time in
between? What would be the probability of surviving for 4
hours? Well, using the law of independent probabilities again, we have
\begin{equation*}
  P_{surv}(8\ \zu{hr})  =  P_{surv}(4\ \zu{hr}) \times P_{surv}(4\ \zu{hr})\eqquad,  
\end{equation*}
which can be rearranged to give
\begin{align*}
  P_{surv}(4\ \zu{hr})          &= \sqrt{P_{surv}(8\ \zu{hr})}   \\
                                 &=  \sqrt{0.5}  \\
                                 &=  0.707\eqquad.  
\end{align*}
This is exactly what we would have found simply by plugging
in $P_{surv}(t)=0.5^{t/8\ \zu{hr}}$ and ignoring the restriction to
multiples of 8 hours. Since 8 hours is the amount of time
required for half of the atoms to decay, it is known as the
half-life, written $t_{1/2}$. The general rule is as follows:

\begin{lessimportant}[Exponential Decay Equation]
\begin{equation*}
                P_{surv}(t)  =    0.5^{t/t_{1/2}}
\end{equation*}
\end{lessimportant}

Using the rule for calculating averages, we can also find
the number of atoms, $N(t)$, remaining in a sample at time $t\/$:
\begin{equation*}
                N(t)  =  N(0)    \times 0.5^{t/t_{1/2}} 
\end{equation*}
Both of these equations have graphs that look like dying-out
exponentials, as in the example below.

\begin{eg}{Radioactive contamination at Chernobyl}
\egquestion One of the most dangerous radioactive isotopes
released by the Chernobyl disaster in 1986 was $^{90}\zu{Sr}$,
whose half-life is 28 years. (a) How long will it be before
the contamination is reduced to one tenth of its original
level? (b) If a total of $10^{27}$  atoms was released,
about how long would it be before not a single atom was left?

\eganswer (a) We want to know the amount of time that a
$^{90}\zu{Sr}$ nucleus has a probability of 0.1 of surviving.
Starting with the exponential decay formula,
\begin{equation*}
                P_{surv}  =    0.5^{t/t_{1/2}}\eqquad,  
\end{equation*}
we want to solve for $t$. Taking natural logarithms of both sides,
\begin{equation*}
                \ln  P  = \frac{t}{t_{1/2}}\ln 0.5\eqquad,  
\end{equation*}
so
\begin{equation*}
                t  =    \frac{t_{1/2}}{\ln 0.5}\ln P
\end{equation*}
Plugging in $P=0.1$ and $t_{1/2}=28$ years, we get $t=93$ years.

(b) This is just like the first part, but $P=10^{-27}$ . The
result is about 2500 years.
\end{eg}

<%
  fig(
    'carbon-fourteen',
    %q{%
      Calibration of the $^{14}\zu{C}$ dating method using
      tree rings and artifacts whose ages were known from
      other methods. Redrawn from Emilio Segr\`{e}, \textbf{Nuclei and Particles}, 1965.
    },
    {
      'width'=>'wide'
    }
  )
%>
\begin{eg}{$^{14}\zu{C}$ \index{carbon-14 dating}Dating}
Almost all the carbon on Earth is $^{12}\zu{C}$, but not quite.
The isotope $^{14}\zu{C}$, with a half-life of 5600 years, is
produced by cosmic rays in the atmosphere. It decays
naturally, but is replenished at such a rate that the
fraction of $^{14}\zu{C}$ in the atmosphere remains constant, at
$1.3\times10^{-12}$ . Living plants and animals take in both
$^{12}\zu{C}$ and $^{14}\zu{C}$ from the atmosphere and incorporate
both into their bodies. Once the living organism dies, it no
longer takes in C atoms from the atmosphere, and the
proportion of $^{14}\zu{C}$ gradually falls off as it undergoes
radioactive decay. This effect can be used to find the age
of dead organisms, or human artifacts made from plants or
animals. Figure \figref{carbon-fourteen} on page
\pageref{fig:carbon-fourteen} shows the exponential decay
curve of $^{14}\zu{C}$ in various objects. Similar methods, using
longer-lived isotopes, provided the first firm proof that
the earth was billions of years old, not a few thousand as
some had claimed on religious grounds.
\end{eg}

<% end_sec() %>
<% begin_sec("Rate of decay") %>\label{rate-of-decay}\index{exponential decay!rate of}
If you want to find how many radioactive decays occur within
a time interval lasting from time $t$ to time $t+\Delta t$,
the most straightforward approach is to calculate it like this:
\begin{align*}
        (\text{number of}&\text{ decays between } t \text{ and } t+\Delta t) \\
                         &=  N(t) - N(t+\Delta t)
\end{align*}
Usually we're interested in the case where $\Delta t$ is small compared to $t_{1/2}$,
and in this limiting case the calculation starts to look exactly like the limit that goes into
the definition of the derivative $\der N/\der t$. It is therefore more convenient to
talk about the \emph{rate} of decay $-\der N/\der t$ rather than the \emph{number} of decays
in some finite time interval.
Doing calculus on the function $e^x$ is also easier than with $0.5^x$, so we rewrite the
function $N(t)$ as
\begin{equation*}
  N = N(0) e^{-t/\tau}\eqquad,
\end{equation*}
where $\tau=t_{1/2}/\ln 2$ is shown in example \ref{eg:average-lifetime} on
p.~\pageref{eg:average-lifetime} to be the average time of survival. The rate of decay
is then
\begin{equation*}
  -\frac{\der N}{\der t} = \frac{N(0)}{\tau} e^{-t/\tau}\eqquad.
\end{equation*}
Mathematically, differentating an exponential just gives back another exponential.
Physically, this is telling us that as $N$ falls off exponentially, the rate of
decay falls off at the same exponential rate, because a lower $N$ means fewer
atoms that remain available to decay.

<% self_check('rate-of-decay-units',<<-'SELF_CHECK'
  Check that both sides of the equation for the rate of decay have units of $\sunit^{-1}$,
  i.e., decays per unit time.
  SELF_CHECK
  ) %>


\begin{eg}{The hot potato}
\egquestion A nuclear physicist with a demented sense of humor
tosses you a cigar box, yelling ``hot potato.'' The label on
the box says ``contains $10^{20}$  atoms of $^{17}\zu{F}$,
half-life of 66 s, produced today in our reactor at 1
p.m.'' It takes you two seconds to read the label, after
which you toss it behind some lead bricks and run away. The
time is 1:40 p.m. Will you die?

\eganswer The time elapsed since the radioactive fluorine
was produced in the reactor was 40 minutes, or 2400 s. The
number of elapsed half-lives is therefore $t/t_{1/2}=  36$. The
initial number of atoms was $N(0)=10^{20}$ . The number of
decays per second is now about $10^7\ \zu{s}^{-1}$, so it produced
about $2\times10^7$  high-energy electrons while you held it
in your hands. Although twenty million electrons sounds like
a lot, it is not really enough to be dangerous.
\end{eg}

By the way, none of the equations we've derived so far was
the actual probability distribution for the time at which a
particular radioactive atom will decay. That probability
distribution would be found by substituting $N(0)=1$ into
the equation for the rate of decay.

\startdqs

\begin{dq}
In the medical procedure involving $^{131}\zu{I}$, why is it
the gamma rays that are detected, not the electrons or
neutrinos that are also emitted?
\end{dq}

\begin{dq}
For 1 s, Fred holds in his hands 1 kg of radioactive
stuff with a half-life of 1000 years. Ginger holds 1 kg of a
different substance, with a half-life of 1 min, for the same
amount of time. Did they place themselves in equal danger, or not?
\end{dq}

\begin{dq}
How would you interpret it if you calculated $N(t)$, and
found it was less than one?
\end{dq}

\begin{dq}
Does the half-life depend on how much of the substance
you have? Does the expected time until the sample decays
completely depend on how much of the substance you have?
\end{dq}

<% end_sec() %>
<% end_sec() %>
<% begin_sec("Applications of calculus") %>
The area under the probability distribution is of course an
integral. If we call the random number $x$ and the
probability distribution $D(x)$, then the probability that
$x$ lies in a certain range is given by 
\begin{equation*}
        \text{(probability of $a\le x \le b$)}=\int_a^b D(x) \der x\eqquad.
\end{equation*}
What about averages? If $x$ had a finite number of equally
probable values, we would simply add them up and divide by
how many we had. If they weren't equally likely, we'd make
the weighted average $x_1P_1+x_2P_2+$... But we need to
generalize this to a variable $x$ that can take on any of a
continuum of values. The continuous version of a sum is an
integral, so the average is
\begin{equation*}
                \text{(average value of $x$)}  = \int x D(x) \der x\eqquad,  
\end{equation*}
where the integral is over all possible values of $x$.

\begin{eg}{Probability distribution for radioactive decay}
Here is a rigorous justification for the statement in subsection \ref{subsec:half-life}
that the probability distribution for
radioactive decay is found by substituting $N(0)=1$ into the
equation for the rate of decay. We know that the probability
distribution must be of the form
\begin{equation*}
                D(t)          =   k 0.5^{t/t_{1/2}}\eqquad,  
\end{equation*}
where $k$ is a constant that we need to determine. The atom
is guaranteed to decay eventually, so normalization gives us
\begin{align*}
        \text{(probability of $0\le t < \infty$)}
                         &=  1  \\
                         &=  \int_0^\infty D(t) \der t\eqquad.
\end{align*}
The integral is most easily evaluated by converting the
function into an exponential with $e$ as the base
\begin{align*}
                D(t)          &=  k \exp\left[\ln\left(0.5^{t/t_{1/2}}\right)\right]  \\
                         &=  k \exp\left[\frac{t}{t_{1/2}}\ln 0.5\right]  \\
                         &=  k \exp\left(-\frac{\ln 2}{t_{1/2}}t\right)\eqquad,
\end{align*}
which gives an integral of the familiar form $\int e^{cx}\der x=(1/c)e^{cx}$. We thus have
\begin{equation*}
                1         =    \left.-\frac{kt_{1/2}}{\ln 2}\exp\left(-\frac{\ln 2}{t_{1/2}}t\right)\right]_0^\infty\eqquad,  
\end{equation*}
which gives the desired result:
\begin{equation*}
                k         =   \frac{\ln 2}{t_{1/2}}\eqquad.  
\end{equation*}
\end{eg}

\begin{eg}{Average lifetime}\label{eg:average-lifetime}
You might think that the half-life would also be the average
lifetime of an atom, since half the atoms' lives are shorter
and half longer. But the half whose lives are longer include
some that survive for many half-lives, and these rare
long-lived atoms skew the average. We can calculate the
average lifetime as follows:
\begin{equation*}
        (\text{average lifetime})    = \int_0^\infty t\: D(t)\der t
\end{equation*}
Using the convenient base-$e$ form again, we have
\begin{equation*}
        (\text{average lifetime})
                = \frac{\ln 2}{t_{1/2}}
                         \int_0^\infty t \exp\left(-\frac{\ln 2}{t_{1/2}}t\right) \der t\eqquad.  
\end{equation*}
This integral is of a form that can either be attacked with
integration by parts or by looking it up in a table. The
result is $\int x e^{cx}\der x=\frac{x}{c}e^{cx}-\frac{1}{c^2}e^{cx}$, 
and the first term can be ignored for our
purposes because it equals zero at both limits of integration. We end up with
\begin{align*}
\text{(average lifetime)} &= \frac{\ln 2}{t_{1/2}}\left(\frac{t_{1/2}}{\ln 2}\right)^2    \\
                         &=  \frac{t_{1/2}}{\ln 2}  \\
                         &=  1.443 \: t_{1/2}\eqquad,  
\end{align*}
which is, as expected, longer than one half-life.
\end{eg}

 % --------------------------------------------------------------------- 
 % --------------------------------------------------------------------- 
 % --------------------------------------------------------------------- 
\vfill\pagebreak[4]
<%
  fig(
    'ozone',
    %q{%
      In recent decades, a huge hole in the ozone
      layer has spread out from Antarctica. Left: November 1978. Right: November 1992
    },
    {
      'width'=>'wide'
    }
  )
%>
<% end_sec() %>
<% end_sec() %>
<% begin_sec("Light as a Particle",0,'light-as-a-particle') %>

\epigraph{The only thing that interferes with my learning is my 
education.}{Albert \index{Einstein, Albert}Einstein}

Radioactivity is random, but do the laws of physics exhibit
randomness in other contexts besides radioactivity? Yes.
Radioactive decay was just a good playpen to get us started
with concepts of randomness, because all atoms of a given
isotope are identical. By stocking the playpen with an
unlimited supply of identical atom-toys, nature helped us to
realize that their future behavior could be different
regardless of their original identicality. We are now ready
to leave the playpen, and see how randomness fits into the
structure of physics at the most fundamental level.

The laws of physics describe light and matter, and the
quantum revolution rewrote both descriptions. Radioactivity
was a good example of matter's behaving in a way that was
inconsistent with classical physics, but if we want to get
under the hood and understand how nonclassical things
happen, it will be easier to focus on light rather than
matter. A radioactive atom such as uranium-235 is after all
an extremely complex system, consisting of 92 protons, 143
neutrons, and 92 electrons. Light, however, can be a simple sine wave.

However successful the classical wave theory of light had
been --- allowing the creation of \index{radio}radio and
\index{radar}radar, for example --- it still failed to
describe many important phenomena. An example that is
currently of great interest is the way the \index{ozone
layer}ozone layer protects us from the dangerous short-wavelength
\index{ultraviolet light}ultraviolet part of the sun's
spectrum. In the classical description, light is a wave.
When a wave passes into and back out of a medium, its
frequency is unchanged, and although its wavelength is
altered while it is in the medium, it returns to its
original value when the wave reemerges. Luckily for us, this
is not at all what ultraviolet light does when it passes
through the ozone layer, or the layer would offer no protection at all!

<% begin_sec("Evidence for light as a particle") %>
For a long time, physicists tried to explain away the
problems with the classical theory of light as arising from
an imperfect understanding of atoms and the interaction of
light with individual atoms and molecules. The ozone
paradox, for example, could have been attributed to the
incorrect assumption that one could think of the ozone layer
as a smooth, continuous substance, when in reality it was
made of individual ozone molecules. It wasn't until 1905
that Albert Einstein threw down the gauntlet, proposing that
the problem had nothing to do with the details of light's
interaction with atoms and everything to do with the
fundamental nature of light itself.

<%
  fig(
    'ccd-spot',
    %q{%
      Digital camera images of dimmer and dimmer sources
      of light. The dots are records of individual photons.
    },
    {
      'width'=>'wide'
    }
  )
%>
In those days the data were sketchy, the ideas vague, and
the experiments difficult to interpret; it took a genius
like Einstein to cut through the thicket of confusion and
find a simple solution. Today, however, we can get right to
the heart of the matter with a piece of ordinary consumer
electronics, the \index{digital camera}digital camera.
Instead of film, a digital camera has a computer chip with
its surface divided up into a grid of light-sensitive
squares, called ``pixels.'' Compared to a grain of the
silver compound used to make regular photographic film, a
digital camera pixel is activated by an amount of light
energy orders of magnitude smaller. We can learn something
new about light by using a digital camera to detect smaller
and smaller amounts of light, as shown in figure \figref{ccd-spot}.
Figure \figref{ccd-spot}/1 is fake, but \figref{ccd-spot}/2 and \figref{ccd-spot}/3 are
real digital-camera images made by Prof. Lyman Page of
Princeton University as a classroom demonstration.\label{lymanpage} Figure
\figref{ccd-spot}/1 is what we would see if we used the digital camera to
take a picture of a fairly dim source of light. In figures
\figref{ccd-spot}/2 and \figref{ccd-spot}/3, the intensity of the light was  drastically
reduced by inserting semitransparent absorbers like the
tinted plastic used in sunglasses. Going from \figref{ccd-spot}/1 to \figref{ccd-spot}/2 to
\figref{ccd-spot}/3, more and more light energy is being thrown away by the absorbers.

<% marg(0) %>

<%
  fig(
    'attenuation-wave',
    %q{A wave is partially absorbed.}
  )
%>
\spacebetweenfigs
<%
  fig(
    'attenuation-bullets',
    %q{A stream of particles is partially absorbed.}
  )
%>
\spacebetweenfigs
<%
  fig(
    'seurat',
    %q{%
      Einstein and Seurat: twins separated at birth?
      \emph{Seine Grande Jatte} by Georges Seurat (19th century).
    }
  )
%>
<% end_marg %>
The results are drastically different from what we would
expect based on the wave theory of light. If light was a
wave and nothing but a wave, \figref{attenuation-wave},
then the absorbers would
simply cut down the wave's amplitude across the whole
wavefront. The digital camera's entire chip would be
illuminated uniformly, and weakening the wave with an
absorber would just mean that every pixel would take a long
time to soak up enough energy to register a signal.

But figures \figref{ccd-spot}/2 and \figref{ccd-spot}/3 
show that some pixels take strong
hits while others pick up no energy at all. Instead of the
wave picture, the image that is naturally evoked by the data
is something more like a hail of bullets from a machine gun,
\figref{attenuation-bullets}.
 Each ``bullet'' of light apparently carries only a tiny
amount of energy, which is why detecting them individually
requires a sensitive digital camera rather than an
eye or a piece of film.

Although Einstein was interpreting different observations,
this is the conclusion he reached in his 1905 paper: that
the pure wave theory of light is an oversimplification, and
that the energy of a beam of light comes in finite chunks
rather than being spread smoothly throughout a region of space.

We now think of these chunks as particles of light, and call
them ``\index{photon!Einstein's early theory}photons,''
although Einstein avoided the word ``particle,'' and the
word ``photon'' was invented later. Regardless of words, the
trouble was that waves and particles seemed like inconsistent
categories. The reaction to Einstein's paper could be kindly
described as vigorously skeptical. Even twenty years later,
Einstein wrote, ``There are therefore now two theories of
light, both indispensable, and --- as one must admit today
despite twenty years of tremendous effort on the part of
theoretical physicists --- without any logical connection.''
In the remainder of this chapter we will learn how the
seeming paradox was eventually resolved.

\startdqs

\begin{dq}
Suppose someone rebuts the digital camera data in figure \figref{ccd-spot}, claiming
that the random pattern of dots occurs not because of
anything fundamental about the nature of light but simply
because the camera's pixels are not all exactly the same  --- some are just more sensitive
than others.
How could we test this interpretation?
\end{dq}

\begin{dq}
Discuss how the correspondence principle applies to the
observations and concepts discussed in this section.
\end{dq}

<% end_sec() %>
<% begin_sec("How much light is one photon?",nil,'how-much-light-is-one-photon') %>
<% begin_sec("The photoelectric effect") %>\index{photoelectric effect}
We have seen evidence that light energy comes in little
chunks, so the next question to be asked is naturally how
much energy is in one chunk. The most straightforward
experimental avenue for addressing this question is a
phenomenon known as the photoelectric effect. The photoelectric
effect occurs when a photon strikes the surface of a solid
object and knocks out an electron. It occurs continually all
around you. It is happening right now at the surface of your
skin and on the paper or computer screen from which you are
reading these words. It does not ordinarily lead to any
observable electrical effect, however, because on the
average free electrons are wandering back in just as
frequently as they are being ejected. (If an object did
somehow lose a significant number of electrons, its growing
net positive charge would begin attracting the electrons
back more and more strongly.)

<% marg(0) %>
<%
  fig(
    'photoelectric-apparatus-a',
    %q{%
      Apparatus for observing the photoelectric
       effect. A beam of light strikes a capacitor plate inside a vacuum
       tube, and electrons are ejected (black arrows).
    }
  )
%>
<% end_marg %>
Figure \figref{photoelectric-apparatus-a} shows a practical method for detecting the
photoelectric effect. Two very clean parallel metal plates
(the electrodes of a capacitor) are sealed inside a vacuum
tube, and only one plate is exposed to light. Because there
is a good vacuum between the plates, any ejected electron
that happens to be headed in the right direction will almost
certainly reach the other capacitor plate without colliding
with any air molecules.

The illuminated (bottom) plate is left with a net positive
charge, and the unilluminated (top) plate acquires a
negative charge from the electrons deposited on it. There is
thus an electric field between the plates, and it is because
of this field that the electrons' paths are curved, as shown
in the diagram. However, since vacuum is a good insulator,
any electrons that reach the top plate are prevented from
responding to the electrical attraction by jumping back
across the gap. Instead they are forced to make their way
around the circuit, passing through an ammeter. The ammeter
allows a measurement of the strength of the photoelectric effect.

<% end_sec() %>
<% begin_sec("An unexpected dependence on frequency") %>

The photoelectric effect was discovered serendipitously by
Heinrich \index{Hertz, Heinrich}Hertz in 1887, as he was
experimenting with radio waves. He was not particularly
interested in the phenomenon, but he did notice that the
effect was produced strongly by ultraviolet light and more
weakly by lower frequencies. Light whose frequency was lower
than a certain critical value did not eject any electrons at
all. (In fact this was all prior to Thomson's discovery of
the electron, so Hertz would not have described the effect
in terms of electrons --- we are discussing everything with
the benefit of hindsight.) This dependence on frequency
didn't make any sense in terms of the classical wave theory
of light. A light wave consists of electric and magnetic
fields. The stronger the fields, i.e., the greater the wave's
amplitude, the greater the forces that would be exerted on
electrons that found themselves bathed in the light. It
should have been amplitude (brightness) that was relevant,
not frequency. The dependence on frequency not only proves
that the wave model of light needs modifying, but with the
proper interpretation it allows us to determine how much
energy is in one photon, and it also leads to a connection
between the wave and particle models that we need in
order to reconcile them.

<% marg(0) %>
<%
  fig(
    'photoelectric-hamster',
    %q{%
      The hamster in her hamster ball
      is like an electron emerging from the metal (tiled kitchen floor)
      into the surrounding vacuum (wood floor). The wood floor is higher
      than the tiled floor, so as she rolls up the step, the hamster will
      lose a certain amount of kinetic energy, analogous to $E_s$. If her
      kinetic energy is too small, she won't even make it up the step.
    }
  )
%>
<% end_marg %>
 % 
To make any progress, we need to consider the physical
process by which a photon would eject an electron from the
metal electrode. A metal contains electrons that are free to
move around. Ordinarily, in the interior of the metal, such
an electron feels attractive forces from atoms in every
direction around it. The forces cancel out. But if the
electron happens to find itself at the surface of the metal,
the attraction from the interior side is not balanced out by
any attraction from outside.
In popping out through the surface the electron therefore loses
some amount of energy $E_s$, which depends on the type of metal used.

Suppose a photon strikes an electron, annihilating itself
and giving up all its energy to the electron. (We now know
that this is what always happens in the photoelectric
effect, although it had not yet been established in 1905
whether or not the photon was completely annihilated.) The
electron will (1) lose kinetic energy through collisions
with other electrons as it plows through the metal on its
way to the surface; (2) lose an amount of kinetic energy
equal to $E_s$ as it emerges through the surface; and (3) lose
more energy on its way across the gap between the plates,
due to the electric field between the plates. Even if the
electron happens to be right at the surface of the metal
when it absorbs the photon, and even if the electric field
between the plates has not yet built up very much, $E_s$ is
the bare minimum amount of energy that it must receive from
the photon if it is to contribute to a measurable current.
The reason for using very clean electrodes is to minimize
$E_s$ and make it have a definite value characteristic of the
metal surface, not a mixture of values due to the various
types of dirt and crud that are present in tiny amounts on
all surfaces in everyday life.

We can now interpret the frequency dependence of the
photoelectric effect in a simple way: apparently the amount
of energy possessed by a photon is related to its frequency.
A low-frequency red or infrared photon has an energy less
than $E_s$, so a beam of them will not produce any current.  A
high-frequency blue or violet photon, on the other hand,
packs enough of a punch to allow an electron to make it to
the other plate. At frequencies higher than the minimum, the
photoelectric current continues to increase with the
frequency of the light because of effects (1) and (3).

<% end_sec() %>
<% begin_sec("Numerical relationship between energy and frequency") %>
Prompted by Einstein's photon paper, Robert \index{Millikan, Robert}Millikan
(whom we first encountered in chapter \ref{ch:atomem}) figured out
how to use the photoelectric effect to probe precisely the
link between frequency and photon energy. Rather than going
into the historical details of Millikan's actual experiments
(a lengthy experimental program that occupied a large part
of his professional career) we will describe a simple
version, shown in figure \figref{photoelectric-apparatus-b}, that is used sometimes in
college laboratory courses.\footnote{
What I'm presenting in this chapter is a simplified explanation of how the photon could have been discovered. The actual history is more complex.
Max Planck (1858-1947) began the photon saga with a theoretical investigation of the spectrum of light emitted by a hot, glowing object. He introduced quantization of the energy of light waves, in multiples of $hf$, purely as a mathematical trick that happened to produce the right results. Planck did not believe that his procedure could have any physical significance. In his 1905 paper Einstein took Planck's quantization as a description of reality, and applied it to various theoretical and experimental puzzles, including the photoelectric effect.
Millikan then subjected Einstein's ideas to a series of rigorous experimental tests. Although his results matched Einstein's predictions perfectly, Millikan was skeptical about photons, and his papers conspicuously omit  any reference to them. Only in his autobiography did Millikan rewrite history and claim that he had given experimental proof for photons.
} The idea is simply to illuminate
one plate of the vacuum tube with light of a single
wavelength and monitor the voltage difference between the
two plates as they charge up. Since the resistance of a
voltmeter is very high (much higher than the resistance of
an ammeter), we can assume to a good approximation that
electrons reaching the top plate are stuck there permanently,
so the voltage will keep on increasing for as long as
electrons are making it across the vacuum tube.

<% marg(0) %>

<%
  fig(
    'photoelectric-apparatus-b',
    %q{A different way of studying the photoelectric effect.}
  )
%>
\spacebetweenfigs
<%
  fig(
    'photoelectric-graph',
    %q{%
      The quantity $E_s+e\Delta V$ indicates the energy of one photon. It
       is found to be proportional to the frequency of the light.
    }
  )
%>

<% end_marg %>
At a moment when the voltage difference has a reached a
value $\Delta $V, the minimum energy required by an electron
to make it out of the bottom plate and across the gap to the
other plate is $E_s+e\Delta $V. As $\Delta V$ increases, we
eventually reach a point at which $E_s+e\Delta V$ \emph{equals} the
energy of one photon. No more electrons can cross the gap,
and the reading on the voltmeter stops rising. The quantity
$E_s+e\Delta V$ now tells us the energy of one photon. If we
determine this energy for a variety of wavelengths, \figref{photoelectric-graph}, we
find the following simple relationship between the energy of
a \index{photon!energy of}photon and the frequency of the light:
\begin{equation*}
                E  =  hf\eqquad,  
\end{equation*}
where $h$ is a constant with the value
$6.63\times10^{-34}\ \zu{J}\cdot\zu{s}$. Note how the equation brings the
wave and particle models of light under the same roof: the
left side is the energy of one \emph{particle} of light,
while the right side is the frequency of the same light,
interpreted as a \emph{wave}. The constant $h$ is known as
\index{Planck, Max}\index{Planck's constant}Planck's constant,
for historical reasons explained in the footnote beginning on the preceding
page.

\pagebreak

<% self_check('extracth',<<-'SELF_CHECK'

How would you extract $h$ from the graph in figure \figref{photoelectric-graph}?
What if you didn't even know $E_s$ in advance, and could only graph
$e\Delta V$ versus $f$?
  SELF_CHECK
  ) %>

Since the energy of a photon is $hf$, a beam of light
can only have energies of $hf$, $2hf$, $3hf$,
etc. Its energy is quantized --- there is no such thing as a
fraction of a photon. Quantum physics gets its name from the
fact that it quantizes quantities like energy, momentum, and
angular momentum that had previously been thought to be
smooth, continuous and infinitely divisible.

\begin{eg}{Number of photons emitted by a lightbulb per second}\label{eg:photons-from-lightbulb}
\egquestion Roughly how many photons are emitted by a 100-W
lightbulb in 1 second?

\eganswer People tend to remember wavelengths rather than
frequencies for visible light. The bulb emits photons with a
range of frequencies and wavelengths, but let's take 600 nm
as a typical wavelength for purposes of estimation. The
energy of a single photon is
\begin{align*}
                E_{photon}         &=    hf  \\
                         &=    hc/\lambda   
\end{align*}
A power of 100 W means 100 joules per second, so the
number of photons is
\begin{align*}
        (100\ \zu{J})/E_{photon}
                &=    (100\ \zu{J}) / (hc/\lambda )  \\
                &\approx 3\times10^{20}
\end{align*}
This hugeness of this number is consistent with the correspondence principle.
The experiments that established the classical theory of optics
weren't wrong. They were right, within their domain of applicability, in which
the number of photons was so large as to be indistinguishable from a continuous beam.
\end{eg}


\begin{eg}{Measuring the wave}\label{eg:am-radio-photon-density}
When surfers are out on the water waiting for their chance to catch a wave, they're interested
in both the height of the waves and when the waves are going to arrive. In other words, they
observe both the amplitude and phase of the waves, and it doesn't matter to them that
the water is granular at the molecular level. The correspondence principle requires that we
be able to do the same thing for electromagnetic waves, since the classical theory of electricity
and magnetism was all stated and verified experimentally in terms of the fields $\vc{E}$ and $\vc{B}$,
which are the amplitude of an electromagnetic wave. The phase is also necessary, since the induction effects
predicted by Maxwell's equation would flip their signs depending on whether an oscillating field is on its way up or
on its way back down.

This is a more demanding application of the correspondence principle than the one in 
example \ref{eg:photons-from-lightbulb}, since amplitudes and phases constitute more detailed
information than the over-all intensity of a beam of light. Eyeball measurements can't detect
this type of information, since the eye is much bigger than a wavelength,
but for example an AM radio receiver can do it with radio waves, since
the wavelength for a station at 1000 kHz is about 300 meters, which is much larger than the antenna.
The correspondence principle demands that we be able to explain this in terms of the photon
theory, and this requires not just that we have a large number of photons emitted by the transmitter
per second, as in example \ref{eg:photons-from-lightbulb}, but that even by the time they spread out
and reach the receiving antenna, there should be many photons overlapping each other within a space
of one cubic wavelength. Problem \ref{hw:am-radio-photon-density} on p.~\pageref{hw:am-radio-photon-density}
verifies that the number is in fact extremely large.
\end{eg}

\begin{eg}{Momentum of a photon}
\egquestion According to the theory of relativity, the
momentum of a beam of light is given by $p=E/c$.
Apply this to find the momentum of a
single photon in terms of its frequency, and in terms of its wavelength.

\eganswer Combining the equations $p=E/c$ and $E=hf$, we find
\begin{align*}
                p         &=    E/c  \\
                         &= \frac{h}{c}f\eqquad.  
\end{align*}
To reexpress this in terms of wavelength, we use $c=f\lambda $:
\begin{align*}
                p         &=  \frac{h}{c}\cdot\frac{c}{\lambda}    \\
                         &=  \frac{h}{\lambda}    
\end{align*}
The second form turns out to be simpler.
\end{eg}

\startdqs

\begin{dq}
The photoelectric effect only ever ejects a
very tiny percentage of the electrons available
near the surface of an object.
How well does this agree with the wave
model of light, and how well with the particle model?
Consider the two different distance scales involved:
the wavelength of the light, and the size of an atom,
which is on the order of $10^{-10}$ or $10^{-9}$ m.
\end{dq}

\begin{dq}
What is the significance of the fact that Planck's
constant is numerically very small? How would our everyday
experience of light be different if it was not so small?
\end{dq}

\begin{dq}
How would the experiments described above be affected if
a single electron was likely to get hit by more than one photon?
\end{dq}

\begin{dq}
Draw some representative trajectories of electrons for
$\Delta V=0$, $\Delta V$ less than the maximum value, and
$\Delta V$ greater than the maximum value.
\end{dq}

\begin{dq}
Explain based on the photon theory of light why
ultraviolet light would be more likely than visible or
infrared light to cause cancer by damaging DNA molecules.
How does this relate to discussion question C?
\end{dq}

\begin{dq}
Does $E=hf$ imply that a photon changes its energy
when it passes from one transparent material into another
substance with a different index of refraction?
\end{dq}

<% end_sec() %>
<% end_sec() %>
<% begin_sec("Wave-particle duality") %>
\index{duality!wave-particle}\index{wave-particle duality}

How can light be both a particle and a wave? We are now
ready to resolve this seeming contradiction. Often in
science when something seems paradoxical, it's because we
(1) don't define our terms carefully, or (2) don't test our
ideas against any specific real-world situation. Let's
define particles and waves as follows:

\begin{itemize}
\item \index{wave!definition of}Waves exhibit superposition, and
specifically interference phenomena.
\item \index{particle!definition of}Particles can only exist in
whole numbers, not fractions.
\end{itemize}

As a real-world check on our philosophizing, there is one
particular experiment that works perfectly. We set up a
double-slit interference experiment that we know will
produce a diffraction pattern if light is an honest-to-goodness
wave, but we detect the light with a detector that is
capable of sensing individual photons, e.g., a digital
camera. To make it possible to pick out individual dots due to
individual photons, we must use filters to cut down the
intensity of the light to a very low level, just as in the
photos by Prof. Page on p.~\pageref{lymanpage}. The whole thing is
sealed inside a light-tight box. The results are shown in
figure \figref{ccd-diffraction}. (In fact, the similar
figures in on page \pageref{lymanpage}
 are simply cutouts from these figures.)

<%
  fig(
    'ccd-diffraction',
    %q{%
      Wave interference patterns photographed by Prof. Lyman
       Page with a digital camera. Laser light with a single well-defined wavelength
       passed through a series of absorbers to cut down its intensity, then through a
       set of slits to produce interference, and finally into
       a digital camera chip. (A triple slit was actually
       used, but for conceptual simplicity we discuss the results
       in the main text as if it was a
       double slit.) In panel 2 the intensity has been
       reduced relative to 1, and even more so for panel 3.
    },
    {
      'width'=>'wide',
      'sidecaption'=>true
    }
  )
%>

Neither the pure wave theory nor the pure particle theory
can explain the results. If light was only a particle and
not a wave, there would be no interference effect. The
result of the experiment would be like firing a hail of
bullets through a double slit, \figref{double-slit-bullets}. Only two spots directly
behind the slits would be hit.

If, on the other hand, light was only a wave and not a
particle, we would get the same kind of diffraction pattern
that would happen with a water wave, \figref{interference}. There would be no
discrete dots in the photo, only a  diffraction pattern that
shaded smoothly between light and dark.

Applying the definitions to this experiment, light must be
both a particle and a wave. It is a wave because it exhibits
interference effects. At the same time, the fact that the
photographs contain discrete dots is a direct demonstration
that light refuses to be split into units of less than a
single photon. There can only be whole numbers of photons:
four photons in figure \figref{ccd-diffraction}/3, for example.

<% begin_sec("A wrong interpretation: photons interfering with each other") %>
One possible interpretation of wave-particle duality that
occurred to physicists early in the game was that perhaps
the interference effects came from photons interacting with
each other. By analogy, a water wave consists of moving
water molecules, and interference of water waves results
ultimately from all the mutual pushes and pulls of the
molecules. This interpretation was conclusively disproved by
G.I. \index{Taylor, G.I.}Taylor, a student at \index{Cambridge}Cambridge.
The demonstration by Prof. Page that we've just been
discussing is essentially a modernized version of Taylor's
work. Taylor reasoned that if interference effects came from
photons interacting with each other, a bare minimum of two
photons would have to be present at the same time to produce
interference. By making the light source extremely dim, we
can be virtually certain that there are never two photons in
the box at the same time. In figure \figref{ccd-diffraction}/3, however, the
intensity of the light has been cut down so much by the
absorbers that if it was in the open, the average separation
between photons would be on the order of a kilometer! At any
given moment, the number of photons in the box is most
likely to be zero. It is virtually certain that there were
never two photons in the box at once.

\subsubsection{The concept of a photon's
 \index{path of a photon undefined}path is undefined.}
If a single photon can demonstrate double-slit interference,
then which slit did it pass through? The unavoidable answer
must be that it passes through both! This might not seem so
strange if we think of the photon as a wave, but it is
highly counterintuitive if we try to visualize it as a
particle. The moral is that we should not think in terms of
the path of a photon. Like the fully human and fully divine
Jesus of Christian theology, a photon is supposed to be
100\% wave and 100\% particle. If a photon had a well
defined path, then it would not demonstrate wave superposition
and interference effects, contradicting its wave nature. (In
subsection \ref{subsec:uncertainty-principle}
we will discuss the Heisenberg uncertainty
principle, which gives a numerical way of approaching this issue.)

<% end_sec() %>
<% begin_sec("Another wrong interpretation: the pilot wave hypothesis",nil,'pilot-wave') %>\index{pilot wave hypothesis}\index{wave-particle duality!pilot-wave interpretation of}
A second possible explanation of wave-particle duality was
taken seriously in the early history of quantum mechanics.
What if the photon \emph{particle} is like a surfer riding
on top of its accompanying \emph{wave}? As the wave travels
along, the particle is pushed, or ``piloted'' by it.
Imagining the particle and the wave as two separate entities
allows us to avoid the seemingly paradoxical idea that a
photon is both at once. The wave happily does its wave
tricks, like superposition and interference, and the
particle acts like a respectable particle, resolutely
refusing to be in two different places at once. If the wave,
for instance, undergoes destructive interference, becoming
nearly zero in a particular region of space, then the
particle simply is not guided into that region.
<% marg(100) %>
<%
  fig(
    'double-slit-bullets',
    %q{Bullets pass through a double slit. }
  )
%>
\spacebetweenfigs
<%
  fig(
    'interference',
    %q{A water wave passes through a double slit.}
  )
%>
\spacebetweenfigs
<%
  fig(
    'skier',
    %q{A single photon can go through both slits.}
  )
%>
<% end_marg %>

The problem with the pilot wave interpretation is that the
only way it can be experimentally tested or verified is if
someone manages to detach the particle from the wave, and
show that there really are two entities involved, not just
one. Part of the scientific method is that hypotheses are
supposed to be experimentally testable. Since nobody has
ever managed to separate the wavelike part of a photon from
the particle part, the interpretation is not useful or
meaningful in a scientific sense.

<% end_sec() %>
<% begin_sec("The probability interpretation") %>
\index{wave-particle duality!probability interpretation of}\index{probability interpretation}

The correct interpretation of wave-particle duality is
suggested by the random nature of the experiment we've been
discussing: even though every photon wave/particle is
prepared and released in the same way, the location at which
it is eventually detected by the digital camera is different
every time. The idea of the probability interpretation of
wave-particle duality is that the location of the photon-particle
is random, but the probability that it is in a certain
location is higher where the photon-wave's amplitude is greater.

\enlargethispage{-3\baselineskip}


More specifically, the probability distribution of the
particle must be proportional to the \emph{square} of
the wave's amplitude,
\begin{equation*}
    (\text{probability distribution}) \propto (\text{amplitude})^2\eqquad.
\end{equation*}
This follows from the correspondence principle and from the
fact that a wave's energy density is proportional to the
square of its amplitude. If we run the double-slit
experiment for a long enough time, the pattern of dots fills
in and becomes very smooth as would have been expected in
classical physics. To preserve the correspondence between
classical and quantum physics, the amount of energy
deposited in a given region of the picture over the long run
must be proportional to the square of the wave's amplitude.
The amount of energy deposited in a certain area depends on
the number of photons picked up, which is proportional to
the probability of finding any given photon there.


<% marg(0) %>
<%
  fig(
    'carrot',
    %q{Example \ref{eg:carrot}.}
  )
%>
<% end_marg %>

\begin{eg}{A microwave oven}\label{eg:carrot}
\egquestion The figure shows two-dimensional (top) and
one-dimensional (bottom) representations of the standing
wave inside a microwave oven. Gray represents zero field,
and white and black signify the strongest fields, with white
being a field that is in the opposite direction compared to
black. Compare the probabilities of detecting a microwave
photon at points A, B, and C.

\eganswer A and C are both extremes of the wave, so the
probabilities of detecting a photon at A and C are equal.
It doesn't matter that we have represented C as negative
and A as positive, because it is the square of the amplitude
that is relevant. The amplitude at B is about 1/2 as much
as the others, so the probability of detecting a photon
there is about 1/4 as much.
\end{eg}

The probability interpretation was disturbing to physicists
who had spent their previous careers working in the
deterministic world of classical physics, and ironically the
most strenuous objections against it were raised by
Einstein, who had invented the photon concept in the first
place. The probability interpretation has nevertheless
passed every experimental test, and is now as well
established as any part of physics.

An aspect of the probability interpretation that has made
many people uneasy is that the process of detecting and
recording the photon's position seems to have a magical
ability to get rid of the wavelike side of the photon's
personality and force it to decide for once and for all
where it really wants to be. But detection or measurement is
after all only a physical process like any other, governed
by the same laws of physics. We will postpone a detailed
discussion of this issue until p.~\pageref{qm-measurement}, since
a measuring device like a digital camera is made of matter,
but we have so far only discussed how quantum mechanics relates to light.\label{postpone-measurement}

\begin{eg}{What is the proportionality constant?}
\egquestion What is the proportionality constant that would
make an actual equation out of 
$(\text{probability distribution})\propto(\text{amplitude})^2$?

\eganswer The probability that the photon is in a certain
small region of volume $v$ should equal the fraction of the
wave's energy that is within that volume. For a sinusoidal wave, which has
a single, well-defined frequency $f$, this gives
\begin{align*}
        P         &=  \frac{\text{energy in volume $v$}}{\text{energy of photon}}    \\
                 &=  \frac{\text{energy in volume $v$}}{hf}\eqquad.
\end{align*}
We assume $v$ is small enough so that the electric and
magnetic fields are nearly constant throughout it. We then have 
\begin{equation*}
        P         =  \frac{\left(\frac{1}{8\pi k}|\vc{E}|^2
                        +\frac{c^2}{8\pi k}|\vc{B}|^2\right)v}{hf}\eqquad.  
\end{equation*}
We can simplify this formidable looking expression by
recognizing that in a plane wave, $|\vc{E}|$ and $|\vc{B}|$ are
related by $|\vc{E}|=c|\vc{B}|$. This implies (problem \ref{hw:poynting},
p.~\pageref{hw:poynting}), that
the electric and magnetic fields each contribute half the
total energy, so we can simplify the result to
\begin{align*}
        P         &=  2\frac{\left(\frac{1}{8\pi k}|\vc{E}|^2\right)v}{hf} \\
                 &=  \frac{v}{4\pi khf}|\vc{E}|^2\eqquad.  
\end{align*}
The probability is proportional to the square
of the wave's amplitude, as advertised.\footnote{But note that along the way, we had to make
two crucial assumptions: that the wave was sinusoidal, and that it was a plane wave.
These assumptions will not prevent us from describing examples such as double-slit diffraction, 
in which the wave is approximately sinusoidal within some sufficiently small region such as
one pixel of a camera's imaging chip. Nevertheless, these issues turn out to be symptoms of
deeper problems, beyond the scope of this book, involving the way in which relativity and quantum
mechanics should be combined. As a taste of the ideas involved, consider what happens when a photon
is reflected from a conducting surface, as in example \ref{eg:em-wave-refl-conductor} on 
p.~\pageref{eg:em-wave-refl-conductor}, so that the electric field at the surface is zero, but the magnetic
field isn't. The superposition is a standing wave, not a plane wave, so $|\vc{E}|=c|\vc{B}|$ need not
hold, and doesn't. A detector's probability of detecting a photon near the surface could be zero if
the detector sensed electric fields, but nonzero if it sensed magnetism. It doesn't make sense to
say that either of these is the probability that the photon ``was really there.''}
\end{eg}

\pagebreak

\startdqs

\begin{dq}
Referring back to the example of the carrot in the
microwave oven, show that it would be nonsensical to have
probability be proportional to the field itself, rather than
the square of the field.
\end{dq}

\begin{dq}
Einstein did not try to reconcile the wave and particle
theories of light, and did not say much about their apparent
inconsistency. Einstein basically visualized a beam of light
as a stream of bullets coming from a machine gun. In the
photoelectric effect, a photon ``bullet'' would only hit one
atom, just as a real bullet would only hit one person.
Suppose someone reading his 1905 paper wanted to interpret
it by saying that Einstein's so-called particles of light
are simply short wave-trains that only occupy a small
region of space.  Comparing the wavelength of visible light
(a few hundred nm) to the size of an atom (on the order of
0.1 nm), explain why this poses a difficulty for reconciling
the particle and wave theories.
\end{dq}

\begin{dq}
Can a white photon exist?
\end{dq}

\begin{dq}
In double-slit diffraction of photons, would you get the
same pattern of dots on the digital camera image if you
covered one slit? Why should it matter whether you give the
photon two choices or only one?
\end{dq}

<% end_sec() %>
<% end_sec() %>
<% begin_sec("Photons in three dimensions") %>
\index{photon!in three dimensions}

Up until now I've been sneaky and avoided a full discussion
of the three-dimensional aspects of the probability
interpretation. The example of the carrot in the microwave
oven, for example, reduced to a one-dimensional situation
because we were considering three points along the same line
and because we were only comparing ratios of probabilities.
The purpose of bringing it up now is to head off any feeling
that you've been cheated conceptually rather than to prepare
you for mathematical problem solving in three dimensions,
which would not be appropriate for the level of this course.

A typical example of a probability distribution in section \ref{sec:randomness}
was the distribution of heights of human beings. The thing
that varied randomly, height, $h$, had units of meters, and
the probability distribution was a graph of a function
$D(h)$. The units of the probability distribution had to be
$\zu{m}^{-1}$ (inverse meters) so that areas under the curve,
interpreted as probabilities, would be unitless:
$(\text{area})=(\text{height})(\text{width})=\zu{m}^{-1}\cdot\zu{m}$.

<% marg(80) %>
<%
  fig(
    'volume-under-surface',
    %q{%
      Probability is the volume under
      a surface defined by $D(x,y)$.
    }
  )
%>
<% end_marg %>
Now suppose we have a two-dimensional problem, e.g., the
probability distribution for the place on the surface of a
digital camera chip where a photon will be detected. The
point where it is detected would be described with two
variables, $x$ and $y$, each having units of meters. The
probability distribution will be a function of both
variables, $D(x,y)$. A probability is now visualized as the
volume under the surface described by the function $D(x,y)$,
as shown in figure \figref{volume-under-surface}.
The units of $D$ must be $\zu{m}^{-2}$ so
that probabilities will be unitless:
$(\text{probability})=(\text{depth})(\text{length})(\text{width})
=\zu{m}^{-2}\cdot\zu{m}\cdot\zu{m}$. In terms of calculus,
we have $P\:=\:\int D\der x \der y$.

Generalizing finally to three dimensions, we find by analogy
that the probability distribution will be a function of all
three coordinates, $D(x,y,z)$, and will have units of $\zu{m}^{-3}$.
It is unfortunately impossible to visualize the graph
unless you are a mutant with a natural feel for life in four
dimensions. If the probability distribution is nearly
constant within a certain volume of space $v$, the
probability that the photon is in that volume is simply
$vD$. If not, then we can use an integral,
$P\:=\:\int D\der x \der y\der z$.

 % --------------------------------------------------------------------- 
 % --------------------------------------------------------------------- 
 % --------------------------------------------------------------------- 
\pagebreak[4]
\inlinefignocaption{melting-witch}
<% end_sec() %>
<% end_sec() %>
<% begin_sec("Matter as a Wave",0) %>\index{matter!as a wave}

\epigraphlong{[In] a few minutes I shall be all melted... I have been
wicked in my day, but I never thought a little girl like you
would ever be able to melt me and end my wicked deeds.
Look out --- here I go!}{The \index{Wicked Witch of the West}Wicked Witch
 of the West}

As the Wicked Witch learned the hard way, losing molecular
cohesion can be unpleasant. That's why we should be very
grateful that the concepts of quantum physics apply to
matter as well as light. If matter obeyed the laws of
classical physics, \index{molecules!nonexistence in
classical physics}molecules wouldn't exist.

Consider, for example, the simplest atom, hydrogen. Why does
one hydrogen atom form a chemical bond with another hydrogen
atom? Roughly speaking, we'd expect a neighboring pair of
hydrogen atoms, A and B, to exert no force on each other
at all, attractive or repulsive: there are two repulsive
interactions (proton A with proton B and electron A with
electron B) and two attractive interactions (proton A with
electron B and electron A with proton B). Thinking a
little more precisely, we should even expect that once the
two atoms got close enough, the interaction would be
repulsive. For instance, if you squeezed them so close
together that the two protons were almost on top of each
other, there would be a tremendously strong repulsion
between them due to the $1/r^2$ nature of the electrical
force. The repulsion between the electrons would not be as
strong, because each electron ranges over a large area, and
is not likely to be found right on top of the other
electron. Thus hydrogen molecules should not exist according
to classical physics.

Quantum physics to the rescue! As we'll see shortly, the
whole problem is solved by applying the same quantum
concepts to electrons that we have already used for photons.
<% begin_sec("Electrons as waves") %>
\index{electron!as a wave}
We started our journey into quantum physics by studying the
random behavior of \emph{matter} in radioactive decay, and
then asked how randomness could be linked to the basic laws
of nature governing \emph{light}. The probability interpretation
of wave-particle duality was strange and hard to accept, but
it provided such a link. It is now natural to ask whether
the same explanation could be applied to matter. If the
fundamental building block of light, the photon, is a
particle as well as a wave, is it possible that the basic
units of matter, such as electrons, are waves as well as particles?

A young French aristocrat studying physics, Louis \index{de
Broglie!Louis}de Broglie (pronounced ``broylee''), made
exactly this suggestion in his 1923 Ph.D. thesis. His idea
had seemed so farfetched that there was serious doubt about
whether to grant him the degree. Einstein was asked for his
opinion, and with his strong support, de Broglie got his degree.

Only two years later, American physicists C.J. \index{Davisson!C.J.}Davisson
and L. \index{Germer, L.}Germer confirmed de Broglie's idea
by accident. They had been studying the scattering of
electrons from the surface of a sample of nickel, made of
many small crystals. (One can often see such a crystalline
pattern on a brass doorknob that has been polished by
repeated handling.) An accidental explosion occurred, and
when they put their apparatus back together they observed
something entirely different: the scattered electrons were
now creating an interference pattern! This dramatic proof of
the wave nature of matter came about because the nickel
sample had been melted by the explosion and then resolidified
as a single crystal. The nickel atoms, now nicely arranged
in the regular rows and columns of a crystalline lattice,
were acting as the lines of a diffraction grating. The new
crystal was analogous to the type of ordinary diffraction
grating in which the lines are etched on the surface of a
mirror (a reflection grating) rather than the kind in which
the light passes through the transparent gaps between the
lines (a transmission grating).

<%
  fig(
    'neutron-interference',
    %q{%
      A double-slit interference pattern
       made with neutrons. (A. Zeilinger, R. G\"{a}hler, C.G. Shull,
       W. Treimer, and W. Mampe, \emph{Reviews of Modern Physics}, Vol. 60, 1988.)
    },
    {
      'width'=>'wide'
    }
  )
%>
Although we will concentrate on the wave-particle duality of
electrons because it is important in chemistry and the
physics of atoms, all the other ``particles'' of matter
you've learned about show wave properties as well.
Figure \figref{neutron-interference}, for instance, shows a wave interference
pattern of neutrons.  

It might seem as though all our work was already done for
us, and there would be nothing new to understand about
electrons: they have the same kind of funny wave-particle
duality as photons. That's almost true, but not quite. There
are some important ways in which electrons differ significantly from photons:
\begin{enumerate}
\item Electrons have mass, and photons don't. 
\item Photons always move at the speed of light, but electrons
can move at any speed less than $c$. 
\item Photons don't have electric charge, but electrons do, so
electric forces can act on them. The most important example
is the atom, in which the electrons are held by the electric
force of the nucleus.
\item Electrons cannot be absorbed or emitted as photons are.
Destroying an electron or creating one out of nothing would
violate conservation of charge.
\end{enumerate}
(In section \ref{sec:atom} we will learn of one more fundamental way in
which electrons differ from photons, for a total of five.)

Because electrons are different from photons, it is not
immediately obvious which of the photon equations from
chapter \ref{ch:em} can be applied to electrons as well. A
particle property, the energy of one photon, is related to
its wave properties via $E=hf$ or, equivalently,
$E=hc/\lambda $. The momentum of a photon was given
by $p=hf/c$ or $p=h/\lambda $. Ultimately it was a
matter of experiment to determine which of these equations,
if any, would work for electrons, but we can make a quick
and dirty guess simply by noting that some of the equations
involve $c$, the speed of light, and some do not. Since $c$
is irrelevant in the case of an electron, we might guess
that the equations of general validity are those that do
not have $c$ in them:
\begin{align*}
                E  &=  hf  \\
                p  &=  h/\lambda   
\end{align*}

This is essentially the reasoning that de Broglie went
through, and experiments have confirmed these two equations
for all the fundamental building blocks of light and matter,
not just for photons and electrons.

The second equation, which I soft-pedaled in the previous
chapter, takes on a greater important for electrons. This is
first of all because the momentum of matter is more likely
to be significant than the momentum of light under ordinary
conditions, and also because force is the transfer of
momentum, and electrons are affected by electrical forces.

\begin{eg}{The wavelength of an elephant}
\egquestion What is the wavelength of a trotting elephant?

\eganswer One may doubt whether the equation should be
applied to an elephant, which is not just a single particle
but a rather large collection of them. Throwing caution to
the wind, however, we estimate the elephant's mass at $10^3$
 kg and its trotting speed at 10 m/s. Its wavelength
is therefore roughly
\begin{align*}
        \lambda         &=    \frac{h}{p}  \\
          &=    \frac{h}{mv}  \\
  &= \frac{6.63\times10^{-34}\ \zu{J}\unitdot\sunit}{(10^3\ \kgunit)(10\ \munit/\sunit)} \\
  &\sim 10^{-37}\ \frac{\left(\kgunit\unitdot\munit^2/\sunit^2\right)\unitdot\sunit}{\kgunit\unitdot\munit/\sunit} \\
        &= 10^{-37}\ \munit
\end{align*}
\end{eg}

The wavelength found in this example is so fantastically
small that we can be sure we will never observe any
measurable wave phenomena with elephants or any other
human-scale objects. The result is numerically small because
Planck's constant is so small, and as in some examples
encountered previously, this smallness is in accord with the
correspondence principle.

Although a smaller mass in the equation $\lambda =h/mv$
does result in a longer wavelength, the wavelength is still
quite short even for individual electrons under typical
conditions, as shown in the following example.

\begin{eg}{The typical wavelength of an electron}
\egquestion Electrons in circuits and in atoms are typically
moving through voltage differences on the order of 1 V,
so that a typical energy is $(e)(1\ \zu{V})$, which is on the
order of $10^{-19}\ \junit$. What is the wavelength of an electron
with this amount of kinetic energy?

\eganswer This energy is nonrelativistic, since it is much
less than $mc^2$. Momentum and energy are therefore related
by the nonrelativistic equation $K=p^2/2m$. Solving
for $p$ and substituting in to the equation for the wavelength, we find
\begin{align*}
                \lambda          &=  \frac{h}{\sqrt{2mK}}    \\
                         &=    1.6\times10^{-9}\ \zu{m}\eqquad.
\end{align*}
This is on the same order of magnitude as the size of an
atom, which is no accident: as we will discuss in the next
chapter in more detail, an electron in an atom can be
interpreted as a standing wave. The smallness of the
wavelength of a typical electron also helps to explain why
the wave nature of electrons wasn't discovered until a
hundred years after the wave nature of light. To scale the
usual wave-optics devices such as diffraction gratings down
to the size needed to work with electrons at ordinary
energies, we need to make them so small that their parts are
comparable in size to individual atoms. This is essentially
what Davisson and Germer did with their nickel crystal.
\end{eg}

<% self_check('longwavelengthelectron',<<-'SELF_CHECK'

These remarks about the inconvenient smallness of electron
wavelengths apply only under the assumption that the
electrons have typical energies. What kind of energy would
an electron have to have in order to have a longer
wavelength that might be more convenient to work with?
  SELF_CHECK
  ) %>

<% begin_sec("What kind of wave is it?") %>
\index{wavefunction!of the electron}\index{electron!wavefunction}
If a sound wave is a vibration of matter, and a photon is a
vibration of electric and magnetic fields, what kind of a
wave is an electron made of? The disconcerting answer is
that there is no experimental ``observable,'' i.e., directly
measurable quantity, to correspond to the electron wave
itself. In other words, there are devices like microphones
that detect the oscillations of air pressure in a sound
wave, and devices such as radio receivers that measure the
oscillation of the electric and magnetic fields in a light
wave, but nobody has ever found any way to measure the
electron wave directly.

<% marg(30) %>
<%
  fig(
    'electron-wave-phase',
    %q{%
      These two electron
       waves are not distinguishable by any measuring device.
    }
  )
%>
<% end_marg %>
We can of course detect the energy (or momentum) possessed
by an electron just as we could detect the energy of a
photon using a digital camera. (In fact I'd imagine that an
unmodified digital camera chip placed in a vacuum chamber
would detect electrons just as handily as photons.) But this
only allows us to determine where the wave carries high
probability and where it carries low probability. Probability
is proportional to the square of the wave's amplitude, but
measuring its square is not the same as measuring the wave
itself. In particular, we get the same result by squaring
either a positive number or its negative, so there is no way
to determine the positive or negative sign of an electron wave.
This unobservability of the phase of the wavefunction is discussed
in more detail on p.~\pageref{subsubsec:linearity-of-schrodinger}.\label{phase-unobservable-basic}

Most physicists tend toward the school of philosophy known
as operationalism, which says that a concept is only
meaningful if we can define some set of operations for
observing, measuring, or testing it. According to a strict
operationalist, then, the electron wave itself is a
meaningless concept. Nevertheless, it turns out to be one of
those concepts like love or humor that is impossible to
measure and yet very useful to have around. We therefore
give it a symbol, $\Psi $ (the capital Greek letter psi),
and a special name, the electron \emph{wavefunction}
(because it is a function of the coordinates $x$, $y$, and $z$
that specify where you are in space). It would be impossible,
for example, to calculate the shape of the electron wave in
a hydrogen atom without having some symbol for the wave. But
when the calculation produces a result that can be compared
directly to experiment, the final algebraic result will turn
out to involve only $\Psi^2$, which is what is observable, not $\Psi $ itself.

Since $\Psi $, unlike $E$ and $B$, is not directly
measurable, we are free to make the probability equations
have a simple form: instead of having the probability
density equal to some funny constant multiplied by $\Psi^2$,
we simply define $\Psi $ so that the constant of
proportionality is one:
\begin{equation*}
  (\text{probability distribution})  =  |\Psi| ^2\eqquad.  
\end{equation*}
Since the probability distribution has units of $\zu{m}^{-3}$, the units
of $\Psi $ must be $\zu{m}^{-3/2}$. The square of a negative
number is still positive, so the absolute value signs may seem unnecessary,
but as we'll see on p.~\pageref{subsubsec:complex-wavefunction} in sec.~\ref{subsec:schrodinger},
the wavefunction may in general be a complex number. In fact, only standing waves, not traveling waves,
can really be represented by real numbers, although we will often cheat and draw
pictures of traveling waves as if they were real-valued functions.\label{cheat-with-real-psi}

\startdq

\begin{dq}
Frequency is oscillations per second, whereas wavelength is
meters per oscillation. How could the equations $E=hf$
and $p=h/\lambda$  be made to look more alike by
using quantities that were more closely analogous?
(This more symmetric treatment makes it easier to
incorporate relativity into quantum mechanics, since
relativity says that space and time are not entirely
separate.)
\end{dq}

<% end_sec() %>
<% end_sec() %>
<% begin_sec("Dispersive waves",nil,'dispersive-waves') %>
\index{wave!dispersive}\index{dispersion}
A colleague of mine who teaches chemistry loves to tell the
story about an exceptionally bright student who, when told
of the equation $p=h/\lambda $, protested, ``But when I
derived it, it had a factor of 2!'' The issue that's
involved is a real one, albeit one that could be glossed
over (and is, in most textbooks) without raising any alarms
in the mind of the average student. The present optional
section addresses this point; it is intended for the student
who wishes to delve a little deeper.

Here's how the now-legendary student was presumably
reasoning. We start with the equation $v=f\lambda $, which
is valid for any sine wave, whether it's quantum or
classical. Let's assume we already know $E=hf$, and
are trying to derive the relationship between wavelength and momentum:
\begin{align*}
        \lambda         &=    \frac{v}{f}  \\
                 &=    \frac{vh}{E}  \\
                 &=    \frac{vh}{\frac{1}{2}mv^2}  \\
                 &=    \frac{2h}{mv}  \\
                 &=    \frac{2h}{p}\eqquad.  
\end{align*}

The reasoning seems valid, but the result does contradict
the accepted one, which is after all solidly based on experiment.

<% marg(0) %>
<%
  fig(
    'sine-wave',
    %q{Part of an infinite sine wave.}
  )
%>
\spacebetweenfigs
<%
  fig(
    'sine-wave-pulse',
    %q{A finite-length sine wave.}
  )
%>
\spacebetweenfigs
<%
  fig(
    'beats',
    %q{A beat pattern created by superimposing two sine waves with slightly different wavelengths.}
  )
%>

<% end_marg %>
The mistaken assumption is that we can figure everything out
in terms of pure sine waves. Mathematically, the only wave
that has a perfectly well defined wavelength and frequency
is a sine wave, and not just any sine wave but an infinitely
long sine wave, \figref{sine-wave}.
 The unphysical thing about such a wave
is that it has no leading or trailing edge, so it can never
be said to enter or leave any particular region of space.
Our derivation made use of the velocity, $v$, and if
velocity is to be a meaningful concept, it must tell us how
quickly stuff (mass, energy, momentum, \ldots) is transported
from one region of space to another. Since an infinitely
long sine wave doesn't remove any stuff from one region and
take it to another, the ``velocity of its stuff'' is not a
well defined concept.

Of course the individual wave peaks do travel through space,
and one might think that it would make sense to associate
their speed with the ``speed of stuff,'' but as we will see,
the two velocities are in general unequal when a wave's
velocity depends on wavelength. Such a wave is called a
\emph{dispersive} wave, because a wave pulse consisting of a
superposition of waves of different wavelengths will
separate (disperse) into its separate wavelengths as the
waves move through space at different speeds.  Nearly all
the waves we have encountered have been nondispersive. For
instance, sound waves and light waves (in a vacuum) have
speeds independent of wavelength. A water wave is one good
example of a dispersive wave. Long-wavelength water waves
travel faster, so a ship at sea that encounters a storm
typically sees the long-wavelength parts of the wave first.
When dealing with dispersive waves, we need symbols and
words to distinguish the two \index{velocity!group}\index{velocity!phase}\index{group
velocity}\index{phase velocity}speeds. The speed at which
wave peaks move is called the phase velocity, $v_p$, and the
speed at which ``stuff'' moves is called the group velocity, $v_g$.

An infinite sine wave can only tell us about the phase
velocity, not the group velocity, which is really what we
would be talking about when we refer to the speed of an
electron. If an infinite sine wave is the simplest possible
wave, what's the next best thing? We might think the runner
up in simplicity would be a wave train consisting of a
chopped-off segment of a sine wave, \figref{sine-wave-pulse}. However, this kind
of wave has kinks in it at the end. A simple wave should be
one that we can build by superposing a small number of
infinite sine waves, but a kink can never be produced by
superposing any number of infinitely long sine waves.

Actually the simplest wave that transports stuff from place
to place is the pattern shown in figure 
\figref{beats}. Called a beat
pattern, it is formed by superposing two sine waves whose
wavelengths are similar but not quite the same. If you have
ever heard the pulsating howling sound of musicians in the
process of tuning their instruments to each other, you have
heard a beat pattern. The beat pattern gets stronger and
weaker as the two sine waves go in and out of phase with
each other. The beat pattern has more ``stuff'' (energy, for
example) in the areas where constructive interference
occurs, and less in the regions of cancellation. As the
whole pattern moves through space, stuff is transported from
some regions and into other ones.

If the frequency of the two sine waves differs by 10\%, for
instance, then ten periods will be occur between times when
they are in phase. Another way of saying it is that the
sinusoidal ``envelope'' (the dashed lines in figure \figref{beats}) has
a frequency equal to the difference in frequency between the
two waves. For instance, if the waves had frequencies of 100
Hz and 110 Hz, the frequency of the envelope would be 10 Hz.

To apply similar reasoning to the wavelength, we must define
a quantity $z=1/\lambda $ that relates to wavelength in the
same way that frequency relates to period. In terms of this
new variable, the $z$ of the envelope equals the difference
between the $z's$ of the two sine waves.

The group velocity is the speed at which the envelope moves
through space. Let $\Delta f$ and $\Delta z$ be the
differences between the frequencies and $z's$ of the two
sine waves, which means that they equal the frequency and
$z$ of the envelope. The group velocity is $v_g=f_{envelope}\lambda_{envelope}=\Delta
f/\Delta $z. If $\Delta f$  and $\Delta z$ are sufficiently
small, we can approximate this expression as a derivative,
\begin{equation*}
                v_g         =    \frac{\der f}{\der z}\eqquad.  
\end{equation*}
This expression is usually taken as the definition of the
group velocity for wave patterns that consist of a
superposition of sine waves having a narrow range of
frequencies and wavelengths. In quantum mechanics, with
$f=E/h$ and $z=p/h$, we have $v_g=\der E/\der p$. In the case of a
nonrelativistic electron the relationship between energy and
momentum is $E=p^2/2m$, so the group velocity is $\der E/\der p=p/m=v$,
exactly what it should be. It is only the phase velocity
that differs by a factor of two from what we would have expected,
but the phase velocity is not the physically important thing.

<% end_sec() %>
<% begin_sec("Bound states") %>
\index{states!bound}\index{bound states}
Electrons are at their most interesting when they're in
atoms, that is, when they are bound within a small region of
space. We can understand a great deal about atoms and
molecules based on simple arguments about such bound states,
without going into any of the realistic details of atom. The
simplest model of a bound state is known as the particle in
a box: like a ball on a pool table, the electron feels zero
force while in the interior, but when it reaches an edge it
encounters a wall that pushes back inward on it with a large
force. In particle language, we would describe the electron
as bouncing off of the wall, but this incorrectly assumes
that the electron has a certain path through space. It is
more correct to describe the electron as a wave that
undergoes 100\% reflection at the boundaries of the box.

<% marg(0) %>
<%
  fig(
    'particle-in-a-box',
    %q{Three possible standing-wave patterns for a particle in a box.}
  )
%>
<% end_marg %>
Like a generation of physics students before me, I rolled my
eyes when initially introduced to the unrealistic idea of
putting a particle in a box. It seemed completely impractical,
an artificial textbook invention. Today, however, it has
become routine to study electrons in rectangular boxes in
actual laboratory experiments. The ``box'' is actually just
an empty cavity within a solid piece of silicon, amounting
in volume to a few hundred atoms. The methods for creating
these \index{box!particle in a}\index{particle in a
box}electron-in-a-box setups (known as ``\index{quantum
dot}quantum dots'') were a by-product of the development of
technologies for fabricating computer chips.

For simplicity let's imagine a one-dimensional electron in a
box, i.e., we assume that the electron is only free to move
along a line. The resulting standing wave patterns, of which
the first three are shown in the figure, are just like some
of the patterns we encountered with sound waves in musical
instruments. The wave patterns must be zero at the ends of
the box, because we are assuming the walls are impenetrable,
and there should therefore be zero probability of finding
the electron outside the box. Each wave pattern is labeled
according to $n$, the number of peaks and valleys it has. In
quantum physics, these wave patterns are referred to as
``states'' of the particle-in-the-box system.

The following seemingly innocuous observations about the
particle in the box lead us directly to the solutions to
some of the most vexing failures of classical physics:

\noindent\emph{The particle's \index{energy!quantization of for bound
states}energy is quantized (can only have certain values).}
Each wavelength corresponds to a certain momentum, and a
given momentum implies  a definite kinetic energy,
$E=p^2/2m$. (This is the second type of energy quantization
we have encountered. The type we studied previously had to
do with restricting the number of particles to a whole
number, while assuming some specific wavelength and energy
for each particle. This type of quantization refers to the
energies that a single particle can have. Both photons and
matter particles demonstrate both types of quantization
under the appropriate circumstances.)

\noindent\emph{The particle has a minimum kinetic energy.} Long wavelengths
correspond to low momenta and low energies. There can be no
state with an energy lower than that of the $n=1$ state, 
called the ground state.

\noindent\emph{The smaller the space in which the particle is confined, the
higher its kinetic energy must be.} Again, this is because
long wavelengths give lower energies.

<% marg(0) %>
<%
  fig(
    'sirius-spectrum',
    %q{The spectrum of the light from the star Sirius.}
  )
%>
<% end_marg %>
\begin{eg}{Spectra of thin gases}
\index{spectrum!absorption}\index{spectrum!emission}\index{absorption}
\index{emission spectrum}\index{gas!spectrum of}
A fact that was inexplicable by classical physics was
that thin gases absorb and emit light only at certain
wavelengths. This was observed both in earthbound laboratories
and in the spectra of stars. The figure on the left shows
the example of the spectrum of the star \index{Sirius}Sirius,
in which there are ``gap teeth'' at certain wavelengths.
Taking this spectrum as an example, we can give a straightforward
explanation using quantum physics.

   Energy is released in the dense interior of the star, but
the outer layers of the star are thin, so the atoms are far
apart and electrons are confined within individual atoms.
Although their standing-wave patterns are not as simple as
those of the particle in the box, their energies are quantized.

   When a photon is on its way out through the outer layers,
it can be absorbed by an electron in an atom, but only if
the amount of energy it carries happens to be the right
amount to kick the electron from one of the allowed energy
levels to one of the higher levels. The photon energies that
are missing from the spectrum are the ones that equal the
difference in energy between two electron energy levels.
(The most prominent of the absorption lines in Sirius's
spectrum are absorption lines of the hydrogen atom.)
\end{eg}

\begin{eg}{The stability of atoms}
   In many \index{Star Trek}Star Trek episodes the
Enterprise, in orbit around a planet, suddenly lost engine
power and began spiraling down toward the planet's surface.
This was utter nonsense, of course, due to conservation of
energy: the ship had no way of getting rid of energy, so it
did not need the engines to replenish it.

   Consider, however, the electron in an atom as it orbits
the nucleus. The electron \emph{does} have a way to release
energy:  it has an acceleration due to its continuously
changing direction of motion, and according to classical
physics, any accelerating charged particle emits electromagnetic
waves. According to classical physics, atoms should collapse!

   The solution lies in the observation that a bound state
has a minimum energy. An electron in one of the higher-energy
atomic states can and does emit photons and hop down step by
step in energy. But once it is in the ground state, it
cannot emit a photon because there is no lower-energy
state for it to go to.
\end{eg}

<% marg(0) %>
<%
  fig(
    'h-molecule',
    %q{%
      Two hydrogen atoms bond to form
       an $\zu{H}_2$ molecule. In the molecule, the two electrons' wave patterns overlap
      , and are about twice as wide.
    }
  )
%>
<% end_marg %>
\begin{eg}{Chemical bonds}\label{h2-bond}\index{chemical bonds!quantum explanation for hydrogen}
I began this section with a classical argument that chemical
bonds, as in an $\zu{H}_2$ molecule, should not exist. Quantum
physics explains why this type of bonding does in fact
occur. When the atoms are next to each other, the electrons
are shared between them. The ``box'' is about twice as wide,
and a larger box allows a smaller energy. Energy is required
in order to separate the atoms. (A qualitatively different
type of bonding is discussed on page
\pageref{ionicbonds}.m4_ifelse(__sn,1,[: Example \ref{eg:h2-details} on page 
\pageref{eg:h2-details} revisits the $\zu{H}_2$ bond in more detail.:],[::]))
\end{eg}

\startdqs

__incl(dq/dineutron)

\begin{dq}
The following table shows the energy gap between the
ground state and the first excited state for four nuclei, in
units of picojoules. (The nuclei were chosen to be ones
that have similar structures, e.g., they are all spherical in shape.)

\begin{tabular}{ll}
    nucleus    &  energy gap (picojoules)\\
    $^4\zu{He}$       & 3.234\\
    $^{16}\zu{O}$     & 0.968\\
    $^{40}\zu{Ca}$    & 0.536\\
    $^{208}\zu{Pb}$   & 0.418\\
\end{tabular}

\noindent Explain the trend in the data.
\end{dq}

<% end_sec() %>
<% begin_sec('The uncertainty principle and measurement',nil,'uncertainty-principle') %>
\index{Heisenberg!Werner}\index{Heisenberg uncertainty
principle}\index{uncertainty principle}


<% begin_sec("Eliminating randomness through measurement?") %>
A common reaction to quantum physics, among both early-twentieth-century
physicists and modern students, is that we should be able to
get rid of randomness through accurate measurement. If I
say, for example, that it is meaningless to discuss the path
of a photon or an electron, one might suggest that we simply
measure the particle's position and velocity many times in a
row. This series of snapshots would amount to a description of its path.

A practical objection to this plan is that the process of
measurement will have an effect on the thing we are trying
to measure. This may not be of much concern, for example,
when a traffic cop measure's your car's motion with a radar
gun, because the energy and momentum of the radar pulses are
insufficient to change the car's motion significantly. But
on the subatomic scale it is a very real problem. Making a
videotape through a microscope of an electron orbiting a
nucleus is not just difficult, it is theoretically
impossible. The video camera makes pictures of things using
light that has bounced off them and come into the camera. If
even a single photon of visible light was to bounce off of
the electron we were trying to study, the electron's recoil
would be enough to change its behavior significantly.  

<% end_sec() %>

<% marg(100) %>
<%
  fig(
    'heisenberg',
    %q{Werner Heisenberg (1901-1976). Heisenberg helped to develop the foundations of quantum mechanics,
       including the Heisenberg uncertainty principle. He was the scientific leader of
       the Nazi atomic-bomb program up until its cancellation in 1942, when the
       military decided that it was too ambitious a project
       to undertake in wartime, and too unlikely to produce results.}
  )
%>
<% end_marg %>

<% begin_sec("The Heisenberg uncertainty principle") %>
This insight, that measurement changes the thing being
measured, is the kind of idea that clove-cigarette-smoking
intellectuals outside of the physical sciences like to claim
they knew all along. If only, they say, the physicists had
made more of a habit of reading literary journals, they
could have saved a lot of work. The anthropologist Margaret
Mead has recently been accused of inadvertently encouraging
her teenaged Samoan informants to exaggerate the freedom of
youthful sexual experimentation in their society. If this is
considered a damning critique of her work, it is because she
could have done better: other anthropologists claim to have
been able to eliminate the observer-as-participant problem
and collect untainted data.

The German physicist Werner Heisenberg, however, showed that
in quantum physics, \emph{any} measuring technique runs into
a brick wall when we try to improve its accuracy beyond a
certain point. Heisenberg showed that the limitation is a
question of \emph{what there is to be known}, even in
principle, about the system itself, not of the ability or
inability of a specific measuring device to ferret out
information that is knowable but not previously hidden.

Suppose, for example, that we have constructed an electron
in a box (quantum dot) setup in our laboratory, and we are
able to adjust the length $L$ of the box as desired. All the
standing wave patterns pretty much fill the box, so our
knowledge of the electron's position is of limited accuracy.
If we write $\Delta x$ for the range of uncertainty in our
knowledge of its position, then $\Delta x$ is roughly the
same as the length of the box:
\begin{equation*}
        \Delta x \approx L
\end{equation*}
If we wish to know its position more accurately, we can
certainly squeeze it into a smaller space by reducing $L$,
but this has an unintended side-effect. A standing wave is
really a superposition of two traveling waves going in
opposite directions. The equation $p=h/\lambda $ really only
gives the magnitude of the momentum vector, not its
direction, so we should really interpret the wave as a 50/50
mixture of a right-going wave with momentum $p=h/\lambda $
and a left-going one with momentum $p=-h/\lambda $. The
uncertainty in our knowledge of the electron's momentum is
$\Delta p=2h/\lambda$, covering the range between these two
values. Even if we make sure the electron is in the ground
state, whose wavelength $\lambda =2L$ is the longest
possible, we have an uncertainty in momentum of 
$\Delta p=h/L$. In general, we find
\begin{equation*}
        \Delta p \gtrsim h/L\eqquad,
\end{equation*}
with equality for the ground state and inequality for the
higher-energy states. Thus if we reduce $L$ to improve our
knowledge of the electron's position, we do so at the cost
of knowing less about its momentum. This trade-off is neatly
summarized by multiplying the two equations to give
\begin{equation*}
        \Delta p\Delta x \gtrsim h\eqquad.
\end{equation*}
Although we have derived this in the special case of a
particle in a box, it is an example of a principle of
more general validity:
\begin{important}[The Heisenberg uncertainty principle]
It is not possible, even in principle, to know the momentum
and the position of a particle simultaneously and with
perfect accuracy. The uncertainties in these two quantities
are always such that $\Delta p\Delta x \gtrsim h$.
\end{important}
\noindent (This approximation can be made into a strict inequality,
$\Delta p\Delta x>h/4\pi$, but only with more careful
definitions, which we will not bother with.)

Note that although I encouraged you to think of this
derivation in terms of a specific real-world system, the
quantum dot, no reference was ever made to any specific
laboratory equipment or procedures. The argument is simply
that we cannot \emph{know} the particle's position very
accurately unless it \emph{has} a very well defined
position, it cannot have a very well defined position unless
its wave-pattern covers only a very small amount of space,
and its wave-pattern cannot be thus compressed without
giving it a short wavelength and a correspondingly uncertain
momentum. The uncertainty principle is therefore a
restriction on how much there is to know about a particle,
not just on what we can know about it with a certain technique.

\begin{eg}{An estimate for electrons in atoms}
\egquestion A typical energy for an electron in an atom is on
the order of $(\text{1 volt})\cdot e$, which corresponds to a speed of
about 1\% of the speed of light. If a typical atom has a
size on the order of 0.1 nm, how close are the electrons to
the limit imposed by the uncertainty principle?

\eganswer If we assume the electron moves in all directions
with equal probability, the uncertainty in its momentum is
roughly twice its typical momentum. This only an order-of-magnitude
estimate, so we take $\Delta p$ to be the same as a typical momentum:
\begin{align*}
 \Delta p \Delta x         &=    p_{typical} \Delta x   \\
                         &=    (m_{electron}) (0.01c) (0.1\times10^{-9}\ \munit)  \\
                         &=    3\times 10^{-34}\ \zu{J}\unitdot\zu{s}  
\end{align*}
This is on the same order of magnitude as Planck's constant,
so evidently the electron is ``right up against the wall.''
(The fact that it is somewhat less than $h$ is of no concern
since this was only an estimate, and we have not stated the
uncertainty principle in its most exact form.)
\end{eg}

<% self_check('smallh',<<-'SELF_CHECK'

If we were to apply the uncertainty principle to human-scale
objects, what would be the significance of the small
numerical value of Planck's constant?
  SELF_CHECK
  ) %>


<% end_sec() %>
<% begin_sec("Measurement and Schr\\\"odinger's cat",nil,'qm-measurement') %>\label{qm-measurement}
\index{measurement in quantum physics}
\index{cat!Schr\"odinger's}\index{Schr\"odinger's cat}
\index{Schr\"odinger!Erwin}
On p.~\pageref{postpone-measurement} I briefly mentioned an issue
concerning measurement that we are now ready to address
carefully. If you hang around a laboratory where quantum-physics
experiments are being done and secretly record the
physicists' conversations, you'll hear them say many things
that assume the probability interpretation of quantum
mechanics. Usually they will speak as though the randomness
of quantum mechanics enters the picture when something is
measured. In the digital camera experiments of section \ref{sec:light-as-a-particle},
for example, they would casually describe the
detection of a photon at one of the pixels as if the moment
of detection was when the photon was forced to ``make up its
mind.'' Although this mental cartoon usually works fairly
well as a description of things they experience in the lab,
it cannot ultimately be correct, because it attributes a
special role to measurement, which is really just a physical
process like all other physical\label{copenhagen}
processes.\footnote{This interpretation of quantum
mechanics is called the Copenhagen interpretation, because it was originally developed by a
school of physicists centered in Copenhagen and led by Niels Bohr.}\index{Copenhagen interpretation}\index{Bohr, Niels}

If we are to find an interpretation that avoids giving any
special role to measurement processes, then we must think of
the entire laboratory, including the measuring devices and
the physicists themselves, as one big quantum-mechanical
system made out of protons, neutrons, electrons, and
photons. In other words, we should take quantum physics
seriously as a description not just of microscopic objects
like atoms but of human-scale (``macroscopic'') things like
the apparatus, the furniture, and the people.

The most celebrated example is called the Schr\"odinger's cat
experiment. Luckily for the cat, there probably was no
actual experiment --- it was simply a ``thought experiment''
that the German theorist Schr\"odinger discussed
with his colleagues. Schr\"odinger wrote:

\begin{quote}
One can even construct quite burlesque cases. A cat is shut
up in a steel container, together with the following
diabolical apparatus (which one must keep out of the direct
clutches of the cat): In a Geiger tube [radiation detector]
there is a tiny mass of radioactive substance, so little
that in the course of an hour perhaps one atom of it
disintegrates, but also with equal probability not even one;
if it does happen, the counter [detector] responds and ...
activates a hammer that shatters a little flask of prussic
acid [filling the chamber with poison gas]. If one has left
this entire system to itself for an hour, then one will say
to himself that the cat is still living, if in that time no
atom has disintegrated. The first atomic disintegration
would have poisoned it.
\end{quote}

<% marg(50) %>
<%
  fig(
    'cat',
    %q{Schr\"odinger's cat.}
  )
%>
<% end_marg %>

Now comes the strange part. Quantum mechanics describes the
particles the cat is made of as having wave properties,
including the property of superposition. Schr\"odinger
describes the wavefunction of the box's contents at
the end of the hour:

The wavefunction of the entire system would express this
situation by having the living and the dead cat mixed ... in
equal parts [50/50 proportions]. The uncertainty originally
restricted to the atomic domain has been transformed into a
macroscopic uncertainty...

 At first Schr\"odinger's description seems like nonsense.
When you opened the box, would you see two ghostlike cats,
as in a doubly exposed photograph, one dead and one alive?
Obviously not. You would have a single, fully material cat,
which would either be dead or very, very upset. But
Schr\"odinger has an equally strange and logical answer for
that objection. In the same way that the quantum randomness
of the radioactive atom spread to the cat and made its
wavefunction a random mixture of life and death, the
randomness spreads wider once you open the box, and your own
wavefunction becomes a mixture of a person who has just
killed a cat and a person who hasn't.\footnote{This interpretation, known as the
many-worlds interpretation, was developed by Hugh Everett in 1957.}\label{many-worlds interpretation}\label{Everett, Hugh}

\startdqs

\begin{dq}
Compare $\Delta p$  and $\Delta x$ for the two lowest
energy levels of the one-dimensional particle in a box, and
discuss how this relates to the uncertainty principle.
\end{dq}

\begin{dq}
On a graph of $\Delta p$ versus $\Delta $x, sketch the
regions that are allowed and forbidden by the Heisenberg
uncertainty principle. Interpret the graph: Where does an
atom lie on it? An elephant? Can either $p$ or $x$ be
measured with perfect accuracy if we don't care about the other?
\end{dq}

<% end_sec('qm-measurement') %>
<% end_sec() %>
<% begin_sec("Electrons in electric fields",nil,'electrons-in-fields') %>

So far the only electron wave patterns we've considered have
been simple sine waves, but whenever an electron finds
itself in an electric field, it must have a more complicated
wave pattern. Let's consider the example of an electron
being accelerated by the electron gun at the back of a TV
tube. Newton's laws are not useful, because they implicitly
assume that the path taken by the particle is a meaningful
concept. Conservation of energy is still valid in quantum
physics, however. In terms of energy, the electron is moving
from a region of low voltage into a region of higher
voltage. Since its charge is negative, it loses electrical energy by moving
to a higher voltage, so its kinetic energy increases. As its electrical
energy goes down, its kinetic energy goes up by an equal
amount, keeping the total energy constant. Increasing
kinetic energy implies a growing momentum, and therefore a
shortening wavelength, \figref{accelerating-electron}.

The wavefunction as a whole does not have a single
well-defined wavelength, but the wave changes so gradually
that if you only look at a small part of it you can still
pick out a wavelength and relate it to the momentum and
energy. (The picture actually exaggerates by many orders of
magnitude the rate at which the wavelength changes.)

<% marg(100) %>
<%
  fig(
    'accelerating-electron',
    %q{%
      An electron in a gentle electric 
      field gradually shortens its wavelength as it gains energy.
      (As discussed on p.~\pageref{cheat-with-real-psi}, it is actually not quite correct to
      graph the wavefunction of an electron as a real number unless it is a standing wave, which
      isn't the case here.)
    }
  )
%>
\spacebetweenfigs
<%
  fig(
    'kinks',
    %q{%
      The wavefunction's tails go where classical
      physics says they shouldn't.
    }
  )
%>
<% end_marg %>

But what if the electric field was stronger? The electric
field in an old-fashioned vacuum tube TV screen is only $\sim10^5$  N/C, but the electric field
within an atom is more like $10^{12}$  N/C. In figure \figref{osculating},
the wavelength changes so rapidly that there is nothing that
looks like a sine wave at all. We could get a rough idea of
the wavelength in a given region by measuring the distance
between two peaks, but that would only be a rough approximation.
Suppose we want to know the wavelength at point $P$. The
trick is to construct a sine wave, like the one shown with
the dashed line, which matches the curvature of the actual
wavefunction as closely as possible near $P$. The sine wave
that matches as well as possible is called the ``osculating''
curve, from a Latin word meaning ``to kiss.'' The wavelength
of the osculating curve is the wavelength that will relate
correctly to conservation of energy.

<%
  fig(
    'osculating',
    %q{%
      A typical wavefunction of
       an electron in an atom (heavy curve) and the
       osculating sine wave (dashed curve) that matches its curvature at point P.
    },
    {
      'width'=>'wide'
    }
  )
%>

\index{tunneling}<% begin_sec("Tunneling") %>
We implicitly assumed that the particle-in-a-box wavefunction
would cut off abruptly at the sides of the box, \figref{kinks}/1, but
that would be unphysical. A kink has infinite curvature, and
curvature is related to energy, so it can't be infinite. A
physically realistic wavefunction must always ``tail off''
gradually, \figref{kinks}/2. In classical physics, a particle can never
enter a region in which its interaction energy $U$ would be
greater than the amount of energy it has available. But in
quantum physics the wavefunction will always have a tail
that reaches into the classically forbidden region. If it
was not for this effect, called tunneling, the fusion
reactions that power the sun would not occur due to the high
electrical energy nuclei need in order to get close together!
Tunneling is discussed in more detail in the following subsection.

<% end_sec() %>
<% end_sec() %>
<% begin_sec("The Schr\\\"odinger equation",nil,'schrodinger') %>
\index{Schr\"odinger equation}
In subsection \ref{subsec:electrons-in-fields} we were able to apply conservation
of energy to an electron's wavefunction, but only by using
the clumsy graphical technique of osculating sine waves as a
measure of the wave's curvature. You have learned a more
convenient measure of curvature in calculus: the second
derivative. To relate the two approaches, we take the second
derivative of a sine wave:
\begin{align*}
 \frac{\der^2}{\der x^2}\sin(2\pi x/\lambda)
 &= \frac{\der}{\der x}\left(\frac{2\pi}{\lambda}\cos\frac{2\pi x}{\lambda}\right)     \\
         &= -\left(\frac{2\pi}{\lambda}\right)^2 \sin\frac{2\pi x}{\lambda}
\end{align*}

Taking the second derivative gives us back the same
function, but with a minus sign and a constant out in front
that is related to the wavelength. We can thus relate the
second derivative to the osculating wavelength:

\begin{equation}\label{eq:schreqna}
        \frac{\der^2\Psi}{\der x^2} = -\left(\frac{2\pi}{\lambda}\right)^2\Psi
\end{equation}

This could be solved for $\lambda $ in terms of $\Psi $, but
it will turn out below to be more convenient to leave it in this form.

Applying this to conservation of energy, we have
\begin{align}\label{eq:schreqnb}
\begin{split}
           E         &=    K  +  U  \\
                 &=   \frac{p^2}{2m}  + U  \\
                 &=   \frac{(h/\lambda)^2}{2m}  + U        
\end{split}
\end{align}

\noindent Note that both equation \eqref{eq:schreqna} and equation \eqref{eq:schreqnb} have $\lambda^2$
in the denominator. We can simplify our algebra by
multiplying both sides of equation \eqref{eq:schreqnb} by $\Psi $ to make it
look more like equation \eqref{eq:schreqna}:

\begin{align*}
        E \cdot \Psi          &=    \frac{(h/\lambda)^2}{2m}\Psi    +   U \cdot \Psi   \\
 &=   \frac{1}{2m}\left(\frac{h}{2\pi}\right)^2\left(\frac{2\pi}{\lambda}\right)^2\Psi
                                 +   U \cdot \Psi  \\
 &=  -\frac{1}{2m}\left(\frac{h}{2\pi}\right)^2 \frac{\der^2\Psi}{\der x^2} 
                                 +   U \cdot \Psi
\end{align*}

\noindent Further simplification is achieved by using the symbol $\hbar$ ($h$
with a slash through it, read ``h-bar'') as an abbreviation
for $h/2\pi $. We then have the important result known as
the \labelimportantintext{Schr\"odinger equation}:

\begin{important}\label{s-eqn-simplest-initial-statement}
\begin{equation*}
        E \cdot \Psi = -\frac{\hbar^2}{2m}\frac{\der^2\Psi}{\der x^2}  +   U \cdot \Psi
\end{equation*}
\end{important}
\noindent (Actually this is a simplified version of the Schr\"odinger
equation, applying only to standing waves in one dimension.)
Physically it is a statement of conservation of energy. The
total energy $E$ must be constant, so the equation tells us
that a change in interaction energy $U$ must be accompanied by a
change in the curvature of the wavefunction. This change in
curvature relates to a change in wavelength, which
corresponds to a change in momentum and kinetic energy.

<% self_check('schrodingerassumptions',<<-'SELF_CHECK'
Considering the assumptions that were made in deriving the
Schr\"odinger equation, would it be correct to apply it to a
photon? To an electron moving at relativistic speeds?
  SELF_CHECK
  ) %>

Usually we know right off the bat how $U$ depends on
$x$, so the basic mathematical problem of quantum physics is
to find a function $\Psi (x$) that satisfies the Schr\"odinger
equation for a given interaction-energy function $U(x)$.
An equation, such as the Schr\"odinger equation, that
specifies a relationship between a function and its
derivatives is known as a differential equation.

The detailed study of the solution of the Schr\"odinger equation is
beyond the scope of this book,
but we can gain some
important insights by considering the easiest version of the
Schr\"odinger equation, in which the interaction energy $U$ is
constant. We can then rearrange the Schr\"odinger equation as follows:
\begin{align*}
   \frac{\der^2\Psi}{\der x^2} &= \frac{2m(U-E)}{\hbar^2} \Psi\eqquad,
\intertext{which boils down to}
   \frac{\der^2\Psi}{\der x^2} &= a\Psi\eqquad,
\end{align*}
where, according to our assumptions, $a$ is independent of
$x$. We need to find a function whose second derivative is
the same as the original function except for a multiplicative
constant. The only functions with this property are sine
waves and exponentials:

\begin{align*}
 \frac{\der^2}{\der x^2}\left[\:q\sin(rx+s)\:\right] &= -qr^2\sin(rx+s)     \\
 \frac{\der^2}{\der x^2}\left[qe^{rx+s}\right] &= qr^2e^{rx+s}
\end{align*}

The sine wave gives negative values of $a$, $a=-r^2$, and the
exponential gives positive ones, $a=r^2$. The former applies
to the classically allowed region with $U<E$.

\label{quantitativetunneling}
<% marg(60) %>
<%
  fig(
    'barrier-with-u-notation',
    %q{Tunneling through a barrier.
      (As discussed on p.~\pageref{cheat-with-real-psi}, it is actually not quite correct to
      graph the wavefunction of an electron as a real number unless it is a standing wave, which
      isn't the case here.)
    }
  )
%>
<% end_marg %>
This leads us to a quantitative calculation of the tunneling
effect discussed briefly in the preceding subsection. The
wavefunction evidently tails off exponentially in the
classically forbidden region. Suppose, as shown in
figure \figref{barrier-with-u-notation},
a wave-particle traveling to the right encounters a
barrier that it is classically forbidden to enter. Although
the form of the Schr\"odinger equation we're using technically
does not apply to traveling waves (because it makes no
reference to time), it turns out that we can still use it to
make a reasonable calculation of the probability that the
particle will make it through the barrier. If we let the
barrier's width be $w$, then the ratio of the wavefunction
on the left side of the barrier to the wavefunction on the right is
\begin{equation*}
        \frac{qe^{rx+s}}{qe^{r(x+w)+s}}  = e^{-rw}\eqquad.  
\end{equation*}
\pagebreak
Probabilities are proportional to the squares of wavefunctions,
so the probability of making it through the barrier is\label{tunneling-probability}
\begin{align*}
   P         &=  e^{-2rw}    \\
         &= \exp\left(-\frac{2w}{\hbar}\sqrt{2m(U-E)}\right) .
\end{align*}

<% self_check('walkthroughwall',<<-'SELF_CHECK'
If we were to apply this equation to find the probability
that a person can walk through a wall, what would the small
value of Planck's constant imply?
  SELF_CHECK
  ) %>

\begin{eg}{Tunneling in alpha decay}\label{eg:alpha-tunneling}
Naively, we would expect alpha decay to be a very fast process. The typical speeds of neutrons and
protons inside a nucleus are extremely high (see problem \ref{hw:lead}). If we imagine an alpha
particle coalescing out of neutrons and protons inside the nucleus, then at the typical speeds we're
talking about, it takes a ridiculously small amount of time for them to reach the surface and try
to escape. Clattering back and forth inside the nucleus, we could imagine them making a vast number of
these ``escape attempts'' every second.

Consider 
figure \figref{alpha-potential}, however, which shows the interaction energy for an alpha particle escaping from a nucleus.
The electrical energy is $kq_1q_2/r$ when the alpha is outside the nucleus, while its variation inside
the nucleus has the shape of a parabola, as a consequence of the shell theorem.
The nuclear energy is constant when the alpha is inside the nucleus, because the forces from all the
neighboring neutrons and protons cancel out; it rises sharply near the surface, and flattens out
to zero over a distance of $\sim 1$ fm, which is the maximum distance scale at which the strong force
can operate.
There is a classically forbidden region immediately outside the nucleus, so the alpha particle can only
escape by quantum mechanical tunneling. (It's true, but somewhat counterintuitive, that a \emph{repulsive}
electrical force can make it more difficult for the alpha to get \emph{out}.) 

In reality, alpha-decay half-lives are often extremely long --- sometimes billions of years --- because
the tunneling probability is so small. Although the shape of the barrier is not a rectangle, the equation
for the tunneling probability on page \pageref{tunneling-probability}
can still be used as a rough guide to our thinking. Essentially the tunneling probability is so small
because $U-E$ is fairly big, typically about 30 MeV at the peak of the barrier.
\end{eg}
<% marg(190) %>
<%
  fig(
    'alpha-potential',
    %q{The electrical, nuclear, and total interaction energies for an alpha particle escaping from a nucleus.}
  )
%>
<% end_marg %>

\pagebreak

\begin{eg}{The correspondence principle for $E>U$}
The correspondence principle demands that in the classical limit $h\rightarrow0$,
we recover the correct result for a particle encountering a barrier $U$,
for both $E<U$ and $E>U$.
The $E<U$ case was analyzed in self-check \ref{sc:walkthroughwall}
on p.~\pageref{sc:walkthroughwall}.
In the remainder of this example, we analyze $E>U$, which
turns out to be a little trickier. 

\vfill

The particle has enough energy to
get over the barrier, and the classical result is that it continues forward
at a different speed (a reduced speed if $U>0$, or an increased one if $U<0$), then
regains its original speed as it emerges from the other side.
What happens quantum-mechanically in this case? We would like to get a ``tunneling''
probability of 1 in the classical limit. The expression derived on p.~\pageref{tunneling-probability}, however,
doesn't apply here, because it was derived under the assumption that the wavefunction
inside the barrier was an exponential; in the classically allowed case, the barrier
isn't classically forbidden, and the wavefunction inside it is a sine wave.
<% marg(10) %>
<%
  fig(
    'step-potential',
    %q{A particle encounters a step of height $U<E$ in the interaction energy. Both sides are
       classically allowed. A reflected wave exists, but is not shown in the figure.}
  )
%>
\spacebetweenfigs
<%
  fig(
    'marble-no-reflection',
    %q{The marble has zero probability of being reflected from the edge of the table.
       (This example has $U<0$, not $U>0$ as in figures \figref{step-potential} and \figref{step-potential-smoother}).}
  )
%>
\spacebetweenfigs
<%
  fig(
    'step-potential-smoother',
    %q{Making the step more gradual reduces the probability of reflection.}
  )
%>
<% end_marg %>

\vfill

We can simplify things a little by letting the width $w$ of the
barrier go to infinity.  Classically, after all, there is no
possibility that the particle will turn around, no matter how wide the
barrier. We then have the situation shown in figure
\figref{step-potential}.\footnote{As in several previous examples, we
cheat by representing a traveling wave as a real-valued function. See
p.~\pageref{cheat-with-real-psi}.}

The analysis is the same as for any other wave being partially reflected at the boundary
between two regions where its velocity differs, and the result is the same as the one
found on  p.~\pageref{reflected-wave-amplitude}. The ratio of the amplitude of the reflected
wave to that of the incident wave is $R =  (v_2-v_1)/(v_2+v_1)$. The probability
of reflection is $R^2$. (Counterintuitively, $R^2$ is nonzero even if $U<0$, i.e., $v_2>v_1$.)

\vfill

This seems to violate the correspondence principle. There is
no $m$ or $h$ anywhere in the result, so we seem to have the result that, even classically,
the marble in figure \figref{marble-no-reflection} can be reflected!

\vfill

The solution to this paradox is that the step in figure \figref{step-potential} was taken to
be completely abrupt --- an idealized mathematical discontinuity. Suppose we make the
transition a little more gradual, as in figure \figref{step-potential-smoother}.
As shown in problem \ref{hw:maxtransmission} on p.~\pageref{hw:maxtransmission},
this reduces the amplitude with which a wave is reflected.
By smoothing out the step more and
more, we continue to reduce the probability of reflection, until finally we arrive at
a barrier shaped like a smooth ramp. More detailed calculations show that this
results in zero reflection in the limit where the width of the ramp is large
compared to the wavelength.
\end{eg}

\enlargethispage{-\baselineskip}

<% begin_sec("Three dimensions",4) %>
For simplicity, we've been considering the Schr\"odinger equation in one dimension,
so that $\Psi$ is only a function of $x$, and has units of $\munit^{-1/2}$ rather
than  $\munit^{-3/2}$.
Since the Schr\"odinger equation is a statement of conservation of energy, and
energy is a scalar, the generalization to three dimensions isn't particularly
complicated. The total energy term $E\cdot\Psi$ and the interaction energy term
$U\cdot\Psi$ involve nothing but scalars, and don't need to be changed at all.
In the kinetic energy term, however, we're essentially basing our computation of
the kinetic energy on the squared magnitude of the momentum, $p_x^2$,
and in three dimensions this would clearly have to be generalized to
$p_x^2+p_y^2+p_z^2$. The obvious way to achieve this is to replace
the second derivative $\der^2\Psi/\der x^2$ with the sum
 $\partial^2\Psi/\partial x^2+ \partial^2\Psi/\partial y^2+ \partial^2\Psi/\partial z^2$. Here the
partial derivative symbol $\partial$, introduced on page \pageref{partial-der}, indicates that
when differentiating with respect to a particular variable, the other variables are to be considered
as constants. This operation
on the function $\Psi$ is notated $\nabla^2\Psi$, and the derivative-like
operator $\nabla^2=\partial^2/\partial x^2+ \partial^2/\partial y^2+ \partial^2/\partial z^2$ is called the Laplacian.\index{Laplacian} It occurs elswehere in physics.
For example, in classical electrostatics,
the voltage in a region of vacuum must be a solution of the equation $\nabla^2V=0$.
Like the second derivative, the Laplacian is essentially a measure of curvature.
Or, as shown in figure \figref{laplacian-geometrical}, we can think of it as a measure of how much the value
of a function at a certain point differs from the average of its value on nearby points.

<% marg(-300) %>
<%
  fig(
    'laplacian-geometrical',
    %q{%
      1.~The one-dimensional version of the Laplacian is the second derivative. It is positive here
          because the average of the two nearby points is greater than the value at the center.
      2.~The Laplacian of the function $A$ in example \ref{eg:laplacian-2d} is positive because
         the average of the four nearby points along the perpendicular axes is greater than the
         function's value at the center.
      3.~$\nabla^2 C=0$. The average is the same as the value at the center.
    }
  )
%>
<% end_marg %>


\enlargethispage{-\baselineskip}

\begin{eg}{Examples of the Laplacian in two dimensions}\label{eg:laplacian-2d}
\egquestion
Compute the Laplacians of the following functions in two dimensions, and interpret them:
$A=x^2+y^2$, $B=-x^2-y^2$, $C=x^2-y^2$.

\eganswer
The first derivative of function $A$ with respect to $x$ is $\partial A/\partial x=2x$. Since $y$
is treated as a constant in the computation of the partial derivative $\partial/\partial x$, the
second term goes away. The second derivative of $A$ with respect to $x$ is
$\partial^2 A/\partial x^2=2$. Similarly we have $\partial^2 A/\partial y^2=2$, so
$\nabla^2 A=4$.

All derivative operators, including $\nabla^2$, have the linear property that multiplying
the input function by a constant just multiplies the output function by the same constant.
Since $B=-A$, and we have $\nabla^2 B=-4$.

For function $C$, the $x$ term contributes a second derivative of 2, but the $y$ term
contributes $-2$, so $\nabla^2 C=0$.

The interpretation of the positive sign in $\nabla^2 A=4$ is that $A$'s graph is shaped
like a trophy cup, and the cup is concave up. 
$\nabla^2 B<0$ is because $B$ is concave down. Function $C$ is shaped like a saddle.
Since its curvature along one axis is concave up, but the curvature along the other is
down and equal in magnitude, the function is considered to have zero concavity over all.
\end{eg}

\pagebreak

\begin{eg}{A classically allowed region with constant $U$}
In a classically allowed region with constant $U$, we expect the solutions
to the Schr\"odinger equation to be sine waves. A sine wave in three dimensions
has the form
\begin{equation*}
  \Psi = \sin\left( k_x x + k_y y + k_z z  \right)\eqquad.
\end{equation*}
When we compute $\partial^2\Psi/\partial x^2$, double differentiation of
$\sin$ gives $-\sin$, and the chain rule brings out a factor of $k_x^2$.
Applying all three second derivative operators, we get
\begin{align*}
  \nabla^2\Psi &= \left(-k_x^2-k_y^2-k_z^2\right)\sin\left( k_x x + k_y y + k_z z  \right) \\
               &=  -\left(k_x^2+k_y^2+k_z^2\right)\Psi\eqquad.
\end{align*}
The Schr\"odinger equation gives
\begin{align*}
  E\cdot\Psi &= -\frac{\hbar^2}{2m}\nabla^2\Psi + U\cdot\Psi \\
             &= -\frac{\hbar^2}{2m}\cdot -\left(k_x^2+k_y^2+k_z^2\right)\Psi + U\cdot\Psi \\
  E-U        &= \frac{\hbar^2}{2m}\left(k_x^2+k_y^2+k_z^2\right)\eqquad,
\end{align*}
which can be satisfied since we're in a classically allowed region with $E-U>0$, and the right-hand
side is manifestly positive.
\end{eg}

<% end_sec %>

<%
  fig(
    'complex-wavefunction',
    %q{%
      1. Oscillations can go back and forth, but it's also possible for them to move along a path that bites its
      own tail, like a circle. Photons act like one, electrons like the other.\\\\
      2. Back-and-forth oscillations can naturally be described by a segment taken from the real number line, and
      we visualize the corresponding type of wave as a sine wave. Oscillations around a closed path relate more
      naturally to the complex number system. The complex number system has rotation built into its
      structure, e.g., the sequence 1, $i$, $i^2$, $i^3$, \ldots rotates around the unit circle in 90-degree increments.\\\\
      3. The double slit experiment embodies the one and only mystery of quantum physics. Either type of wave
      can undergo double-slit interference.
    },
    {
      'width'=>'wide',
      'sidecaption'=>true
    }
  )
%>

<% begin_sec("Use of complex numbers",nil,'complex-wavefunction') %>
\index{wavefunction!complex numbers in}\index{complex numbers!in quantum physics}
In a classically forbidden region, a particle's total
energy, $U+K$, is less than its $U$, so
its $K$ must be negative. If we want to keep believing
in the equation $K=p^2/2m$, then apparently the
momentum of the particle is the square root of a negative
number. This is a symptom of the fact that the Schr\"odinger
equation fails to describe all of nature unless the
wavefunction and various other quantities are allowed to be
complex numbers. In particular it is not possible to
describe traveling waves correctly without using complex wavefunctions.
Complex numbers were reviewed in subsection \ref{subsec:complex-numbers}, p.~\pageref{subsec:complex-numbers}.

This may seem like nonsense, since real numbers are the only
ones that are, well, real! Quantum mechanics can always be
related to the real world, however, because its structure is
such that the results of measurements always come out to be
real numbers. For example, we may describe an electron as
having non-real momentum in classically forbidden regions,
but its average momentum will always come out to be real
(the imaginary parts average out to zero), and it can never
transfer a non-real quantity of momentum to another particle.

A complete investigation of these issues is beyond the scope
of this book, and this is why we have normally limited
ourselves to standing waves, which can be described with
real-valued wavefunctions. Figure \figref{complex-wavefunction}
gives a visual depiction of the difference between real and
complex wavefunctions. The following remarks may also be helpful.

Neither of the graphs in \subfigref{complex-wavefunction}{2} should be
interpreted as a path traveled by something. This isn't anything mystical about
quantum physics. It's just an ordinary fact about waves, which we first encountered
in subsection \ref{subsec:wave-motion}, p.~\pageref{subsec:wave-motion},
where we saw the distinction between the motion
of a wave and the motion of a wave pattern. In \emph{both} examples in \subfigref{complex-wavefunction}{2},
the wave pattern is moving in a straight line to the right.

The helical graph in \subfigref{complex-wavefunction}{2}
shows a complex wavefunction whose value rotates around a circle in the
complex plane with a frequency $f$ related to its energy by $E=hf$. As it does
so, its squared magnitude $|\Psi|^2$ stays the same, so the corresponding probability stays
constant. Which direction does it rotate? This direction is purely a matter of convention,
since the distinction between the symbols $i$ and $-i$ is arbitrary --- both are equally valid
as square roots of $-1$. We can, for example, arbitrarily say that electrons with positive energies
have wavefunctions whose phases rotate counterclockwise, and as long as we follow that rule
consistently within a given calculation, everything will work. Note that it is not possible
to define anything like a right-hand rule here, because the complex plane shown in
the right-hand side of \subfigref{complex-wavefunction}{2} doesn't represent two dimensions of
physical space; unlike a screw going into a piece of wood, an electron doesn't have a direction
of rotation that depends on its direction of travel.

\begin{eg}{Superposition of complex wavefunctions}
\egquestion
The right side of figure \subfigref{complex-wavefunction}{3} is a cartoonish representation of
double-slit interference; it depicts the situation at the center, where symmetry guarantees that
the interference is constuctive. Suppose that at some off-center point, the two wavefunctions
being superposed are $\Psi_1=b$ and $\Psi_2=bi$, where $b$ is a real number with units.
Compare the probability of finding the electron at this position with what it would have been
if the superposition had been purely constructive, $b+b=2b$.

\eganswer
The probability per unit volume is proportional to the square of the magnitude of the total
wavefunction, so we have
\begin{equation*}
  \frac{P_{\text{off center}}}{P_{\text{center}}} 
         = \frac{|b+bi|^2}{|b+b|^2} = \frac{1^2+1^2}{2^2+0^2} = \frac{1}{2}\eqquad.
\end{equation*}
\end{eg}

Figure \figref{rainbow} shows a method for visualizing complex wavefunctions. The idea
is to use colors to represent complex numbers, according to the arbitrary convention
defined in figure \subfigref{rainbow}{1}. Brightness indicates magnitude, and the rainbow
hue shows the argument. Because this representation can't be understood in a black and white
printed book, the figure is also reproduced on the back cover of printed copies.
To avoid any confusion, note that the use of rainbow colors does
not mean that we are representing actual visible light. In fact, we will be using these
visual conventions to represent the wavefunctions of a material particle such as an electron.
It is arbitrary that we use red for positive real numbers and blue-green for negative numbers,
and that we pick a handedness for the diagram such that going from red toward yellow means
going counterclockwise. Although physically the rainbow is a linear spectrum, we are not
representing physical colors here, and we are exploiting the fact that the human brain
tends to perceive color as a circle rather than a line, with violet and red being perceptually
similar. One of the limitations of this representation is that brightness is limited, so we
can't represent complex numbers with arbitrarily large magnitudes.
%
<%
  fig(
    'rainbow',
    %q{%
      1.~A representation of complex numbers using color and brightness.
      2.~A wave traveling toward the right.
      3.~A wave traveling toward the left.
      4.~A standing wave formed by superposition of waves 2 and 3.
      5.~A two-dimensional standing wave.
      6.~A double-slit diffraction pattern.
    },
    {
      'width'=>'wide',
      'sidecaption'=>true
    }
  )
%>

Figure \subfigref{rainbow}{2} shows a traveling wave as it propagates to the right.
The standard convention in physics is that for a wave moving in a certain direction,
the phase in the forward direction is farther counterclockwise in the complex plane, and
you can verify for yourself that this is the case by comparing with the convention
defined by \subfigref{rainbow}{1}. The function being plotted here is $\Psi=e^{ikx}$,
where $k=2\pi/\lambda$ is the spatial analog of frequency, with an extra factor of $2\pi$ for
convenience. For the use of the complex exponential, see sec.~\ref{subsec:euler-formula},
p~.\pageref{subsec:euler-formula}; it simply represents a point on the unit circle in the
complex plane. The wavelength $\lambda$
is a constant and can be measured, for example, from one yellow point to the next. The wavelength
is \emph{not} different at different points on the figure, because we are using the colors merely
as a visual encoding of the complex numbers --- so, for example, a red point on the figure is
not a point where the wave has a longer wavelength than it does at a blue point.

Figure \subfigref{rainbow}{3} represents a wave travaling to the left.

Figure \subfigref{rainbow}{4} shows a standing wave created by superimposing the traveling
waves from \subfigref{rainbow}{2} and \subfigref{rainbow}{3}, $\Psi_4=(\Psi_2+\Psi_3)/2$. (The reason
for the factor of 2 is simply that otherwise some portions of $\Psi_4$ would have magnitudes too great to be
represented using the available range of brightness.) All points on this wave have real values,
represented by red and blue-green. We made the superposition real by an appropriate choice of
the phases of $\Psi_2$ and $\Psi_3$. This is always possible to do when we have a standing wave,
but it is \emph{only} possible for a standing wave, and this is the reason for all of the disclaimers
in the captions of previous figures in which I took the liberty of representing a traveling wave as
a sine-wave graph.

Figure \subfigref{rainbow}{5} shows a two-dimensional standing wave of a particle in a box,
and \subfigref{rainbow}{6} shows a double-slit interference pattern. (In the latter, I've cheated
by making the amplitude of the wave on the right-hand half of the picture much greater than it
would actually be.)

\begin{eg}{A paradox resolved}
Consider the following paradox. Suppose we have an electron that is traveling wave, and
its wavefunction looks like a wave-train consisting of 5 cycles of a sine wave. Call the distance
between the leading and trailing edges of the wave-train $L$, so that $\lambda=L/5$.
By sketching the wave, you can easily check that
there are 11 points where its value equals zero. Therefore at a particular moment in time, there are 11 points
where a detector has zero probability of detecting the electron.

But now consider how this would look in a frame of reference where the electron is moving
more slowly, at one fifth of the speed we saw in the original frame. In this frame, $L$ is
the same, but $\lambda$ is five times greater, because $\lambda=h/p$. Therefore in this frame
we see only one cycle in the wave-train. Now there are only 3 points where the probability
of detection is zero. But how can this be? All observers, regardless of their frames of reference, should agree
on whether a particular detector detects the electron.

The resolution to this paradox is that it starts from the assumption that we can depict
a traveling wave as a real-valued sine wave, which is zero in certain places. Actually, we can't.
It has to be a complex number with a rotating phase angle in the complex plane, as in figure
\subfigref{rainbow}{2}, and a
\emph{constant} magnitude.
\end{eg}


<% end_sec('complex-wavefunction') %>
<% begin_sec("Linearity of the Schr\\\"odinger equation",nil,'linearity-of-schrodinger') %>
Some mathematical relationships and operations are \emph{linear}, and some are not.
For example, $2\times(3+2)$ is the same as $2\times3+2\times2$, but $\sqrt{1+1}\ne\sqrt{1}+\sqrt{1}$.
Differentiation is a linear operation, $(f+g)'=f'+g'$. The Schr\"odinger equation is
built out of derivatives, so it is linear as well. That is, if $\Psi_1$ and $\Psi_2$ are
both solutions of the Schr\"odinger equation, then so is $\Psi_1+\Psi_2$. Linearity
normally implies linearity with respect both to addition and to multiplication by a scalar.
For example, if $\Psi$ is a solution, then so is $\Psi+\Psi+\Psi$, which is the same as $3\Psi$.

Linearity guarantees that the phase of a wavefunction makes no difference as to its validity
as a solution to the Schr\"odinger equation. If $\sin kx$ is a solution, then so is the sine wave
$-\sin kx$ with the opposite phase. This fact is logically interdependent with the fact that,
as discussed on p.~\pageref{phase-unobservable-basic}, the phase
of a wavefunction is unobservable. For measuring devices and humans are material objects
that can be described by wavefunctions. So suppose, for example, that we flip the phase of
all the particles inside the entire laboratory. By linearity,
the evolution of this measurement process is still a valid  solution of the Schr\"odinger equation.

The Schr\"odinger equation is a wave equation, and its
linearity implies that the waves obey the principle of superposition.
In most cases in nature, we find that the principle of superposition for waves is at best an
approximation. For example, if the amplitude of
a tsunami is so huge that the trough of the wave reaches all the way down to the ocean floor,
exposing the rocks and sand as it passes overhead, then clearly there is no way to double the
amplitude of the wave and still get something that obeys the laws of physics. Even at less extreme
amplitudes, superposition is only an approximation for water waves, and so for example it is only
approximately true that when two sets of ripples intersect on the surface of a pond, they pass
through without ``seeing'' each other.

It is therefore natural to ask whether the apparent linearity of the Schr\"odinger equation is only
an approximation to some more precise, nonlinear theory. This is not currently believed to be the
case. If we are to make sense of Schr\"odinger's cat
(p.~\pageref{subsubsec:qm-measurement}), then the experimenter who sees a live cat and the one
who sees a dead cat must remain oblivious to their other selves, like the ripples on the pond
that intersect without ``seeing'' each other. Attempts to create slightly nonlinear versions
of standard quantum mechanics have been shown to have implausible physical properties, such as
allowing the propagation of signals faster than $c$.

If you have had a course in linear algebra, then it is worth noting that the
linearity of the Schr\"odinger equation allows us to talk about its solutions as vectors in a vector
space. For example, if $\Psi_1$ represents an unstable nucleus that has not yet gamma decayed, and
$\Psi_2$ is its state after the decay, then any superposition $\alpha\Psi_1+\beta\Psi_2$, with real or complex
coefficients $\alpha$ and $\beta$, is
a possible wavefunction, and we can notate this as a vector, $\langle\alpha,\beta\rangle$, in a two-dimensional
vector space.
<% end_sec('linearity-of-schrodinger') %>

\startdqs

\begin{dq}
The zero level of interaction energy $U$ is arbitrary, e.g., it's equally valid to pick the zero of
gravitational energy to be on the floor of your lab or at the ceiling. Suppose we're doing the double-slit
experiment, \subfigref{complex-wavefunction}{3}, with electrons. We define the zero-level of $U$ so that
the total energy $E=U+K$ of each electron is positive. and we observe a certain interference
pattern like the one in figure \figref{ccd-diffraction} on p.~\pageref{fig:ccd-diffraction}. What happens
if we then redefine the zero-level of $U$ so that the electrons have $E<0$?
\end{dq}

\begin{dq}
The figure shows a series of snapshots in the motion of two pulses on a coil spring, one negative and one positive,
as they move toward one another and superpose. The final image is very close to the moment at which
the two pulses cancel completely. 
The following discussion is simpler if we consider infinite sine waves rather than pulses.
How can the cancellation of two such mechanical waves be reconciled with conservation of energy?
What about the case of colliding electromagnetic waves? 

Quantum-mechanically, the issue isn't conservation of energy, it's conservation of probability,
i.e., if there's initially a 100\%
probability that a particle exists somewhere, we don't want the probability to be more than or
less than 100\%
at some later time. What happens when the colliding waves have
real-valued wavefunctions $\Psi$? Complex ones?
What happens with standing waves?
\end{dq}
<% marg(30) %>
<%
  fig(
    'superposition-cancellation',
    ''
  )
%>
<% end_marg %>

\begin{dq}
The figure shows a skateboarder tipping over into a swimming pool with zero
initial kinetic energy. There is no friction, the corners are smooth enough to allow the
skater to pass over the smoothly, and the vertical distances are small
enough so that negligible time is required for the vertical parts of the motion.
The pool is divided into a deep end and a shallow end. Their widths are equal.
The deep end is four times deeper. (1) Classically, compare the skater's velocity in
the left and right regions, and infer the probability of finding the skater in either of the
two halves if an observer peeks at a random moment.
(2) Quantum-mechanically, this could be a one-dimensional model of an electron shared between two atoms
in a diatomic molecule. Compare the electron's kinetic energies, momenta, and wavelengths in the
two sides. For simplicity, let's assume that there is no tunneling into the classically forbidden
regions. What is the simplest standing-wave pattern that you can draw, and what are the probabilities
of finding the electron in one side or the other? Does this obey the correspondence principle?
\end{dq}

<%
  fig(
    'quantum-pool-skater',
    '',
    {                                 
      'width'=>'fullpage'
    }
  )
%>

 % --------------------------------------------------------------------- 
 % --------------------------------------------------------------------- 
 % --------------------------------------------------------------------- 
 % \fullpagewidthfignocaption{hwavefnlowres} 
\vfill\pagebreak[4]
<%
  fig(
    'hwavefnlowres',
    '',
    {
      'width'=>'wide',
      'anonymous'=>true,
      'float'=>false
    }
  )
%>
<% end_sec() %>
<% end_sec %>
<% begin_sec("The Atom",0,'atom') %>
You can learn a lot by taking a car engine apart, but you
will have learned a lot more if you can put it all back
together again and make it run. Half the job of reductionism
is to break nature down into its smallest parts and
understand the rules those parts obey. The second half is to
show how those parts go together, and that is our goal in
this chapter. We have seen how certain features of all atoms
can be explained on a generic basis in terms of the
properties of bound states, but this kind of argument
clearly cannot tell us any details of the behavior of an
atom or explain why one atom acts differently from another.

The biggest embarrassment for reductionists is that the job
of putting things back together job is usually much harder
than the taking them apart. Seventy years after the
fundamentals of atomic physics were solved, it is only
beginning to be possible to calculate accurately the
properties of atoms that have many electrons. Systems
consisting of many atoms are even harder. Supercomputer
manufacturers point to the folding of large \index{protein
molecules}protein molecules as a process whose calculation
is just barely feasible with their fastest machines. The
goal of this chapter is to give a gentle and visually
oriented guide to some of the simpler results about atoms.

<% begin_sec("Classifying states") %>
We'll focus our attention first on the simplest atom,
\index{hydrogen atom!classification of states}hydrogen, with
one proton and one electron. We know in advance a little of
what we should expect for the structure of this atom. Since
the electron is bound to the proton by electrical forces, it
should display a set of discrete energy states, each
corresponding to a certain standing wave pattern. We need to
understand what states there are and what their properties are.

What properties should we use to classify the states? The
most sensible approach is to used conserved quantities.
\index{hydrogen atom!energy in}Energy is one conserved
quantity, and we already know to expect each state to have a
specific energy. It turns out, however, that energy alone is
not sufficient. Different standing wave patterns of the atom
can have the same energy.

\index{hydrogen atom!momentum in}Momentum is also a
conserved quantity, but it is not particularly appropriate
for classifying the states of the electron in a hydrogen
atom. The reason is that the force between the electron and
the proton results in the continual exchange of momentum
between them. (Why wasn't this a problem for energy as well?
Kinetic energy and momentum are related by $K=p^2/2m$,
so the much more massive proton never has very much kinetic
energy. We are making an approximation by assuming all the
kinetic energy is in the electron, but it is quite a
good approximation.)

<% marg(0) %>
<%
  fig(
    'moat-rainbow',
    %q{1.~Eight wavelengths fit around this circle ($\ell=8$). This is a standing wave.
       2.~A traveling wave with $\ell=8$, depicted according to the color conventions
          defined in figure \figref{rainbow}, p.~\pageref{fig:rainbow}.
      }
  )
%>
<% end_marg %>
\index{hydrogen atom!angular momentum in}Angular momentum
does help with classification. There is no transfer of
angular momentum between the proton and the electron, since
the force between them is a center-to-center force,
producing no torque.

Like energy, angular momentum is quantized in quantum
physics. As an example, consider a quantum wave-particle
confined to a circle, like a wave in a circular moat
surrounding a castle. A sine wave in such a ``\index{angular
momentum!quantization of}\index{quantum moat}quantum moat''
cannot have any old wavelength, because an integer number of
wavelengths must fit around the circumference, $C$, of the
moat. The larger this integer is, the shorter the wavelength,
and a shorter wavelength relates to greater momentum and
angular momentum. Since this integer is related to angular
momentum, we use the symbol $\ell$ for it:
\begin{equation*}
                \lambda          =    C /   \ell
\end{equation*}
The angular momentum is 
\begin{equation*}
                L         =    rp\eqquad.  
\end{equation*}
Here, $r=C/2\pi $, and $p=h/\lambda=h\ell/C$, so
\begin{align*}
        L         &=    \frac{C}{2\pi}\cdot\frac{h\ell}{C}  \\
                         &=     \frac{h}{2\pi}\ell 
\end{align*}
In the example of the quantum moat, angular momentum is
quantized in units of $h/2\pi $. This makes $h/2\pi $ a
pretty important number, so we define the abbreviation
$\hbar=h/2\pi $. This symbol is read ``h-bar.''

In fact, this is a completely general fact in quantum
physics, not just a fact about the quantum moat:

\begin{important}[Quantization of angular momentum]
The angular momentum of a particle due to its motion through
space is quantized in units of $\hbar$.
\end{important}

<% self_check('angmompicture',<<-'SELF_CHECK'

What is the angular momentum of the wavefunction shown at
the beginning of the section?
  SELF_CHECK
  ) %>

<% end_sec() %>
<% begin_sec("Three dimensions") %>
\index{angular momentum!and the uncertainty principle}\index{angular
momentum!in three dimensions}

Our discussion of quantum-mechanical angular momentum has so far been limited to
rotation in a plane, for which we can simply
use positive and negative signs to indicate clockwise and
counterclockwise directions of rotation. A hydrogen atom,
however, is unavoidably three-dimensional.
The classical treatment of angular momentum in three-dimensions
has been presented in section
\ref{sec:amthreed}; in general, the angular momentum of a particle
is defined as the vector cross product $\vc{r}\times\vc{p}$.

There
is a basic problem here: the angular momentum of the electron in
a hydrogen atom depends on both its distance $\vc{r}$ from the proton
and its momentum $\vc{p}$, so in order to know its angular momentum
precisely it would seem we would need to know both its
position and its momentum simultaneously with good accuracy.
This, however,  seems forbidden by the Heisenberg
uncertainty principle.

Actually the uncertainty principle does place limits on what
can be known about a particle's angular momentum vector, but
it does not prevent us from knowing its magnitude as an
exact integer multiple of $\hbar$. The reason is that in three
dimensions, there are really three separate \index{Heisenberg
uncertainty principle!in three dimensions}\index{uncertainty
principle!in three dimensions}uncertainty principles:
\begin{align*}
        \Delta p_x \Delta x &\gtrsim h \\
        \Delta p_y \Delta y &\gtrsim h \\
        \Delta p_z \Delta z &\gtrsim h
\end{align*}
Now consider a particle, \figref{particle-a-m-examples}/1, that is moving along the $x$
axis at position $x$ and with momentum $p_x$. We may not be
able to know both $x$ and $p_x$ with unlimited accurately,
but we can still know the particle's angular momentum about
the origin exactly: it is zero, because the particle is
moving directly away from the origin.

<% marg(100) %>
<%
  fig(
    'particle-a-m-examples',
    %q{%
      Reconciling the uncertainty principle
      with the definition of angular momentum.
    }
  )
%>
<% end_marg %>
Suppose, on the other hand, a particle finds itself, \figref{particle-a-m-examples}/2, at
a position $x$ along the $x$ axis, and it is moving parallel
to the $y$ axis with momentum $p_y$. It has angular momentum
$xp_y$ about the $z$ axis, and again we can know its angular
momentum with unlimited accuracy, because the uncertainty
principle only relates $x$ to $p_x$ and $y$ to $p_y$. It does
not relate $x$ to $p_y$.

As shown by these examples, the uncertainty principle does
not restrict the accuracy of our knowledge of angular
momenta as severely as might be imagined. However, it does
prevent us from knowing all three components of an angular
momentum vector simultaneously. The most general statement
about this is the following theorem:

\begin{important}[The angular momentum vector in quantum physics]
The most that can be known about a (nonzero) orbital angular momentum vector
is its magnitude and one of its three vector components.
Both are quantized in units of $\hbar$.
\end{important}

<% marg() %>
<%
  fig(
    'spherical-harmonic',
    %q{%
      A wavefunction on the sphere with $|\vc{L}|=11\hbar$ and $L_z=8\hbar$, shown using the color conventions
          defined in figure \figref{rainbow}, p.~\pageref{fig:rainbow}.
    }
  )
%>
<% end_marg %>

To see why this is true, consider the example wavefunction shown in figure \figref{spherical-harmonic}.
This is the like the quantum moat of figure \figref{moat-rainbow}, p.~\pageref{fig:moat-rainbow},
but extended to one more dimension.
If we slice the sphere in any plane perpendicular to the $z$ axis, we get an 8-cycle circular rainbow exactly
like figure \figref{moat-rainbow}. This is required because $L_z=8\hbar$. But if we take a slice perpendicular
to some other axis, such as the $y$ axis, we don't get a circular rainbow as we would for a state with a
definite value of $L_y$. It is obviously not possible to get circular rainbows for slices perpendicular to more
than one axis.

For those with a taste for rigor, here is a
complete argument:

\noindent Theorem: On the sphere, if a wavefunction has
definite values of both $L_z$ and $L_x$, then it is a wavefunction
that is constant everywhere, so $\vc{L}=0$.

\noindent Lemma 1: If component of $\ell_A$ 
along a certain axis A has a definite value and is nonzero, then (a)
$\Psi=0$ at the poles, and (b) $\Psi$ is of the form $Ae^{i\ell_A\phi}$ on any circle in
a plane perpendicular to the axis. Part a holds because $\vc{L}=0$ if $r_\perp=0$.
For b, see p.~\pageref{fig:moat-rainbow}.

\noindent Lemma 2: If the component of $\vc{L}$ along a certain axis has a
definite value and is zero, then $\Psi$ is constant in any plane perpendicular to that axis.
This follows from lemma 1 in the case where $\ell_A=0$.

\noindent \emph{Case I: $\ell_z$ and $\ell_x$ are both nonzero.}
We have $\Psi=0$ at the poles along both the $x$ axis and
the $z$ axis. The $z$-axis pole is a point on the great circle
perpendicular to the $x$ axis, and vice versa, so applying 1b, $A=0$ and $\Psi$
vanishes on both of these great circles. But now if we apply 1b along
any slice perpendicular to either axis, we get $\Psi=0$ everywhere on
that slice, so $\Psi=0$ everywhere.

\noindent \emph{Case II: $\ell_z$ and $\ell_x$ are both zero.}
By lemma 2, $\Psi$ is a constant everywhere.

\noindent \emph{Case III: One component is zero and the other nonzero.}
Let $\ell_z$ be the one that is zero.
By 1a, $\Psi=0$ at the $x$-axis pole, so by 2, $\Psi=0$ on the great circle perpendicular to $z$.
But then 1b tells us that $\Psi=0$ everywhere.

<%
  fig(
    'completeness-ylm',
    %q{%
      Example \ref{eg:completeness-ylm}.
    },
    {
      'width'=>'wide',
      'sidecaption'=>true
    }
  )
%>

\begin{eg}{Completeness}\label{eg:completeness-ylm}
For a given value of $\ell$, consider the set of angular momentum states with $|\vc{L}|=\hbar\ell$
and the various possible values of the angular momentum's component along some fixed axis.
This set of states is \emph{complete}, meaning that they encompass all the possible states with this
$\ell$.

For example, figure \figref{completeness-ylm} shows wavefunctions with $\ell=1$ that are solutions of
the Schr\"odinger equation for a particle that is confined to the surface of a sphere. Although the
formulae for these wavefunctions are not particularly complicated,\footnote{They are 
$\Psi_{1,-1}=\sin\theta e^{-i\phi}$, $\Psi_{10}=\sqrt{2}\cos\theta$,
and $\Psi_{11}=\sin\theta e^{i\phi}$, wher $\theta$ is the angle measured down from the $z$
axis, and $\phi$ is the angle running counterclockwise around the $z$ axis. These functions are
called spherical harmonics.\index{spherical harmonics}} they are not our main
focus here, so to help with getting a feel for the idea of completeness, I have simply
selected three points on the sphere at which to give numerical samples of the value of the wavefunction.
These are the top (where the sphere is intersected by the positive $z$ axis),
left ($x$), and front ($y$). (Although the wavefunctions are shown using the
color conventions defined in figure \figref{rainbow}, p.~\pageref{fig:rainbow},
these numerical samples should make the example understandable if you're looking at a black and white
copy of the book.)

Suppose we arbitrarily choose the $z$ axis as the one along which to quantize the component
of the angular momentum. With this choice, we have three possible values for $\ell_z$: $-1$, $0$,
and $1$. These three states are shown in the three boxes surrounded by the black rectangle.
This set of three states is complete. 

Consider, for example, the fourth state, shown on the
right outside the box. This state is clearly identifiable as a copy of the $\ell_z=0$ state,
rotated by 90 degrees counterclockwise, so it is the $\ell_x=0$ state. We might imagine that
this would be an entirely new prize to be added to our stamp collection. But it is actually
not a state that we didn't possess before. We can obtain it as the sum of the
$\ell_z=-1$ and $\ell_z=1$ states, divided by an appropriate normalization factor. Although
I'm avoiding making this example an exercise in manipulating formulae, it is easy to check
that the sum does work out properly at the three sample points.
\end{eg}

\vfill

\pagebreak

 % Used this figure at beginning of section, without caption 
 % or label. 
<%
  fig(
    'hwavefnlowres',
    %q{A cross-section of a hydrogen wavefunction.},
    {
      'width'=>'wide'
    }
  )
%>
<% end_sec() %>

<% begin_sec("The hydrogen atom") %>
\index{hydrogen atom}\index{hydrogen atom!quantum numbers}
Deriving the wavefunctions of the states of the hydrogen
atom from first principles would be mathematically too
complex for this book, but it's not hard to understand the
logic behind such a wavefunction in visual terms. Consider
the wavefunction from the beginning of the section, which is
reproduced in figure \figref{hwavefnlowres}.
 Although the graph looks three-dimensional,
it is really only a representation of the part of the
wavefunction lying within a two-dimensional plane. The third
(up-down) dimension of the plot represents the value of the
wavefunction at a given point, not the third dimension of
space. The plane chosen for the graph is the one perpendicular
to the angular momentum vector.

Each ring of peaks and valleys has eight wavelengths going
around in a circle, so this state has $L=8\hbar$, i.e., we label
it $\ell=8$. The wavelength is shorter near the center, and this
makes sense because when the electron is close to the
nucleus it has a lower electrical energy, a higher kinetic energy,
and a higher momentum.

Between each ring of peaks in this wavefunction is a nodal
circle, i.e., a circle on which the wavefunction is zero. The
full three-dimensional wavefunction has nodal spheres: a
series of nested spherical surfaces on which it is zero. The
number of radii at which nodes occur, including $r=\infty$,
is called $n$, and $n$ turns out to be closely related to
energy. The ground state has $n=1$ (a single node only at
$r=\infty$), and higher-energy states have higher $n$
values. There is a simple equation relating $n$ to energy,
which we will discuss in subsection \ref{subsec:hydrogen-energies}.
<% marg(40) %>
<%
  fig(
    'hydrogen-energy-levels',
    %q{%
      The energy of a state in the hydrogen 
      atom depends only on its $n$ quantum number.
    }
  )
%>
<% end_marg %>

The numbers $n$ and $\ell$, which identify the state, are called
its \index{quantum numbers}quantum numbers.
 A state of a
given $n$ and $\ell$ can be oriented in a variety of directions in
space. We might try to indicate the orientation using the
three quantum numbers
$\ell_x=L_x/\hbar$,
$\ell_y=L_y/\hbar$, and
$\ell_z=L_z/\hbar$.
But we have already seen
that it is impossible to know all three of these simultaneously.
To give the most complete possible description of a state,
we choose an arbitrary axis, say the $z$ axis, and label the
state according to $n$, $\ell$, and $\ell_z$.\footnote{See page
\pageref{quantumnotation} for a note about the two different
systems of notations that are used for quantum numbers.}

Angular momentum requires motion, and motion implies kinetic
energy. Thus it is not possible to have a given amount of
angular momentum without having a certain amount of kinetic
energy as well. Since energy relates to the $n$ quantum
number, this means that for a given $n$ value there will be
a maximum possible . It turns out that this maximum
value of  equals $n-1$.

In general, we can list the possible combinations of
quantum numbers as follows:

\noindent\begin{tabular}{|l|}
\hline
  $n$ can equal 1, 2, 3, \ldots \\
  $\ell$ can range from 0 to $n-1$, in steps of 1\\
  $\ell_z$ can range from $-\ell$ to $\ell$, in steps of 1 \\
\hline
\end{tabular}

Applying these rules, we have the following list of states:

\noindent\begin{tabular}{|lll|l|}
\hline
$n=1$, & $\ell=0$, & $\ell_z=0$        & one state \\
$n=2$, & $\ell=0$, & $\ell_z=0$        & one state \\
$n=2$, & $\ell=1$, & $\ell_z=-1$, 0, or 1 &        three states\\
\ldots & & & \\
\hline
\end{tabular}

<% self_check('listingatomstates',<<-'SELF_CHECK'
Continue the list for $n=3$.
  SELF_CHECK
  ) %>

Figure \figref{hydrogen-three-states} on page \pageref{fig:hydrogen-three-states}
shows the lowest-energy states
of the hydrogen atom. The left-hand column of graphs
displays the wavefunctions in the $x-y$ plane, and the
right-hand column shows the probability distribution in a
three-dimensional representation.

<%
  fig(
    'hydrogen-three-states',
    %q{%
      The three states of the hydrogen atom having
      the lowest energies.
    },
    {
      'width'=>'wide'
    }
  )
%>

\startdqs

\begin{dq}
The quantum number $n$ is defined as the number of radii
at which the wavefunction is zero, including $r=\infty$.
Relate this to the features of figure \figref{hydrogen-three-states}.
\end{dq}

\begin{dq}
Based on the definition of $n$, why can't there be any
such thing as an $n=0$ state?
\end{dq}

\begin{dq}
Relate the features of the wavefunction plots in figure \figref{hydrogen-three-states}
to the corresponding features of the probability distribution pictures.
\end{dq}

\begin{dq}
How can you tell from the wavefunction plots in figure \figref{hydrogen-three-states}
which ones have which angular momenta?
\end{dq}

\begin{dq}
Criticize the following incorrect statement: ``The $\ell=8$
wavefunction in figure \figref{hwavefnlowres} has a shorter wavelength
in the center because in the center the electron is in a
higher energy level.''
\end{dq}

\begin{dq}
Discuss the implications of the fact that the probability
cloud in of the $n=2$, $\ell=1$ state is split into two parts.
\end{dq}

<% end_sec() %>
<% begin_sec("Energies of states in hydrogen",nil,'hydrogen-energies') %>
__incl(text/h-energy)

<% end_sec() %>
<% begin_sec("Electron spin") %>\index{spin}

It's disconcerting to the novice ping-pong player to
encounter for the first time a more skilled player who can
put spin on the ball. Even though you can't see that the
ball is spinning, you can tell something is going on by the
way it interacts with other objects in its environment. In
the same way, we can tell from the way electrons interact
with other things that they have an intrinsic spin of their
own. Experiments show that even when an \index{electron!spin
of}\index{spin!of electron}electron is not moving through
space, it still has angular momentum amounting to $\hbar/2$.
An important historical experiment of this type, the Stern-Gerlach
experiment, is described in detail in section \ref{subsec:stern-gerlach}.

<% marg(m4_ifelse(__sn,1,[:30:],[:0:])) %>
<%
  fig(
    'spin-vs-orbital',
    %q{%
      The top has angular momentum both
       because of the motion of its center of mass
       through space and due to its internal rotation. Electron
       spin is roughly analogous to the intrinsic spin of the top.
    }
  )
%>
<% end_marg %>
This may seem paradoxical because the quantum moat, for
instance, gave only angular momenta that were integer
multiples of $\hbar$, not half-units, and I claimed that angular
momentum was always quantized in units of $\hbar$, not just in the
case of the quantum moat. That whole discussion, however,
assumed that the angular momentum would come from the motion
of a particle through space. The  $\hbar/2$ angular momentum of the
electron is simply a property of the particle, like its
charge or its mass. It has nothing to do with whether the
electron is moving or not, and it does not come from any
internal motion within the electron. Nobody has ever
succeeded in finding any internal structure inside the
electron, and even if there was internal structure, it would
be mathematically impossible for it to result in a half-unit
of angular momentum.

We simply have to accept this $\hbar/2$ angular momentum, called the
``spin'' of the electron --- Mother Nature rubs our noses in it as
an observed fact.
\index{spin!proton's}\index{proton!spin of}
\index{spin!neutron's}\index{neutron!spin of}
\index{spin!photon's}\index{photon!spin of}
Protons and neutrons have
the same $\hbar/2$ spin, while photons have an intrinsic spin of $\hbar$.
In general, half-integer spins are typical of material particles.
Integral values are found for the particles that carry forces: photons,
which embody the electric and magnetic fields of force, as well as the
more exotic messengers of the nuclear and gravitational forces.

As was the case with ordinary angular momentum, we can
describe spin angular momentum in terms of its magnitude,
and its component along a given axis.
We write $s$ and $s_z$ for these quantities, expressed in
units of $\hbar$, so an
electron has $s=1/2$ and $s_z=+1/2$ or $-1/2$.

Taking electron spin into account, we need a total of four
quantum numbers to label a state of an electron in the
hydrogen atom: $n$, $\ell$, $\ell_z$, and $s_z$. (We omit $s$ because it
always has the same value.) The symbols  and  include only
the angular momentum the electron has because it is moving
through space, not its spin angular momentum. The availability
of two possible spin states of the electron leads to a
doubling of the numbers of states:

\noindent\begin{tabular}{|llll|l|}
\hline
  $n=1$, & $\ell=0$, & $\ell_z=0$, & $s_z=+1/2$ or $-1/2$ &        two states \\
  $n=2$, & $\ell=0$, & $\ell_z=0$, & $s_z=+1/2$ or $-1/2$ &        two states\\
  $n=2$, & $\ell=1$, & $\ell_z=-1$, 0, or 1, & $s_z=+1/2$ or $-1/2$ &        six states\\
        \ldots & & & &\\
\hline
\end{tabular}

<% begin_sec("A note about notation") %>\label{quantumnotation}
There are unfortunately two inconsistent systems of notation
for the quantum numbers we've been discussing. The notation
I've been using is the one that is used in nuclear
physics, but there is a different one that is used in
atomic physics.

\begin{tabular}{|l|l|}
  \hline
  nuclear physics        & atomic physics \\
  \hline
  $n$                        & same \\
  $\ell$                & same \\
  $\ell_x$                & no notation \\
  $\ell_y$                & no notation \\
  $\ell_z$                & $m$ \\
  $s=1/2$                & no notation (sometimes $\sigma$)\\
  $s_x$                        & no notation \\
  $s_y$                        & no notation \\
  $s_z$                        & $s$ \\
  \hline
\end{tabular}

\noindent{}The nuclear physics notation is more logical (not giving
special status to the $z$ axis) and more memorable ($\ell_z$
rather than the obscure $m$), which is why I use it consistently
in this book, even though nearly all the applications we'll
consider are atomic ones.

We are further encumbered with
the following historically derived
letter labels, which deserve to be
eliminated in favor of the simpler numerical ones:

\begin{tabular}{|cccc|}
\hline
$\ell=0$        & $\ell=1$ & $\ell=2$ & $\ell=3$\\
s               & p        & d        & f \\
\hline
\end{tabular}

\begin{tabular}{|ccccccc|}
\hline
$n=1$   & $n=2$ & $n=3$ & $n=4$ & $n=5$ & $n=6$ & $n=7$ \\
K & L & M & N & O & P & Q\\
\hline
\end{tabular}

\noindent{}The spdf labels are used in both nuclear\footnote{After f,
the series continues in alphabetical order. In nuclei
that are spinning rapidly enough that they are almost breaking apart,
individual protons and neutrons can
be stirred up to $\ell$ values as high as 7, which is j.}
 and atomic physics, while
the KLMNOPQ letters are used only to refer to states of electrons.

And finally, there is a piece of notation that is good and useful,
but which I simply haven't mentioned yet. The vector $\vc{j}=\vc{\ell}+\vc{s}$
stands for the total angular momentum of a particle in units of $\hbar$, including
both orbital and spin parts. This quantum number turns out to be
very useful in nuclear physics, because nuclear forces tend to
exchange orbital and spin angular momentum, so a given energy level
often contains a mixture of $\ell$ and $s$ values, while remaining
fairly pure in terms of $j$.

<% end_sec() %>
<% end_sec() %>
<% begin_sec("Atoms with more than one electron") %>
\index{atoms!with many electrons}
What about other atoms besides hydrogen? It would seem that
things would get much more complex with the addition of a
second electron. A hydrogen atom only has one particle that
moves around much, since the nucleus is so heavy and nearly
immobile. \index{atoms!helium}\index{helium}Helium, with
two, would be a mess. Instead of a wavefunction whose square
tells us the probability of finding a single electron at any
given location in space, a helium atom would need to have a
wavefunction whose square would tell us the probability of
finding two electrons at any given combination of points.
Ouch! In addition, we would have the extra complication of
the electrical interaction between the two electrons, rather
than being able to imagine everything in terms of an
electron moving in a static field of force created
by the nucleus alone.

Despite all this, it turns out that we can get a surprisingly
good description of many-electron atoms simply by assuming
the electrons can occupy the same standing-wave patterns
that exist in a hydrogen atom. The ground state of helium,
for example, would have both electrons in states that are
very similar to the $n=1$ states of hydrogen.  The
second-lowest-energy state of helium would have one electron
in an $n=1$ state, and the other in an $n=2$ states. The
relatively complex spectra of elements heavier than hydrogen
can be understood as arising from the great number of
possible combinations of states for the electrons.

A surprising thing happens, however, with \index{atoms!lithium}lithium,
the three-electron atom. We would expect the ground state of
this atom to be one in which all three electrons settle down
into $n=1$ states. What really happens is that two electrons
go into $n=1$ states, but the third stays up in an $n=2$
state. This is a consequence of a new principle of physics:
\begin{important}[The Pauli Exclusion Principle]
\index{exclusion principle}\index{Pauli exclusion principle}
Only one electron can ever occupy a given state.
\end{important}

There are two $n=1$ states, one with $s_z=+1/2$ and one with
$s_z=-1/2$, but there is no third $n=1$ state for lithium's
third electron to occupy, so it is forced to go into an $n=2$ state.

It can be proved mathematically that the Pauli exclusion
principle applies to any type of particle that has
half-integer spin. Thus two neutrons can never occupy the
same state, and likewise for two protons. Photons, however,
are immune to the exclusion principle because their spin is an integer.

<% begin_sec("Deriving the periodic table") %>\index{periodic table}
We can now account for the structure of the periodic table,
which seemed so mysterious even to its inventor Mendeleev.
The first row consists of atoms with electrons only
in the $n=1$ states:

\noindent\begin{tabular}{rp{98mm}}
                H &        1 electron in an $n = 1$ state  \\
                He &        2 electrons in the two $n = 1$ states  
\end{tabular}

The next row is built by filling the $n=2$ energy levels:

\noindent\begin{tabular}{rp{98mm}}
        Li &        2 electrons in $n=1$ states, 1 electron in an $n=2$ state\\
        Be &        2 electrons in $n=1$ states, 2 electrons in $n=2$ states\\
        \ldots\\
        O &        2 electrons in $n=1$ states, 6 electrons in $n=2$ states\\
        F &        2 electrons in $n=1$ states, 7 electrons in $n=2$ states\\
        Ne &        2 electrons in $n=1$ states, 8 electrons in $n=2$ states
\end{tabular}

In the third row we start in on the $n=3$ levels:

\noindent\begin{tabular}{rp{98mm}}
        Na        & 2 electrons in $n=1$ states, 8 electrons in $n=2$ 
                        states, 1 electron in an $n=3$ state \\
        ...
\end{tabular}

<% marg(0) %>
<%
  fig(
    'small-periodic-table',
    %q{The beginning of the periodic table.}
  )
%>
\spacebetweenfigs
<%
  fig(
    'hindenburg',
    %q{Hydrogen is highly reactive.}
  )
%>
<% end_marg %>
We can now see a logical link between the filling of the
energy levels and the structure of the periodic table.
Column 0, for example, consists of atoms with the right
number of electrons to fill all the available states up to a
certain value of $n$. Column I contains atoms like lithium
that have just one electron more than that.

\label{ionicbonds}
This shows that the columns relate to the filling of energy
levels, but why does that have anything to do with
chemistry? Why, for example, are the elements in columns I
and VII dangerously reactive? Consider, for example, the
element \index{atoms!sodium}\index{sodium}sodium (Na), which
is so reactive that it may burst into flames when exposed to
air. The electron in the $n=3$ state has an unusually high
energy. If we let a sodium atom come in contact with an
oxygen atom, energy can be released by transferring the
$n=3$ electron from the sodium to one of the vacant
lower-energy $n=2$ states in the oxygen. This energy is
transformed into heat. Any atom in column I is highly
reactive for the same reason: it can release energy by
giving away the electron that has an unusually high energy.

Column VII is spectacularly reactive for the opposite
reason: these atoms have a single vacancy in a low-energy
state, so energy is released when these atoms steal an
electron from another atom.

It might seem as though these arguments would only explain
reactions of atoms that are in different rows of the
periodic table, because only in these reactions can a
transferred electron move from a higher-$n$ state to a
lower-$n$ state. This is incorrect. An $n=2$ electron in
fluorine (F), for example, would have a different energy
than an $n=2$ electron in lithium (Li), due to the different
number of protons and electrons with which it is interacting.
Roughly speaking, the $n=2$ electron in fluorine is more
tightly bound (lower in energy) because of the larger number
of protons attracting it. The effect of the increased number
of attracting protons is only partly counteracted by the
increase in the number of repelling electrons, because the
forces exerted on an electron by the other electrons are in
many different directions and cancel out partially.
        
 % ============================================================= 
 % ============================================================= 
 % ============================================================= 
 % ============================================================= 
 % ============================================================= 
<% end_sec() %>
<% end_sec() %>
<% end_sec() %>

\vfill

<% begin_sec("Some optional topics",nil,'stern-gerlach',{'optional'=>true}) %>

<% begin_sec("The Stern-Gerlach experiment",nil,'stern-gerlach') %>
In 1921, Otto Stern proposed an experiment about angular momentum, shown in figure \figref{stern-gerlach}
on p.~\pageref{fig:stern-gerlach}, that his
boss at the University of Frankfurt
and many of his colleagues were certain wouldn't work. At this time, quantization of
angular momentum had been proposed by Niels Bohr, but most physicists, if they had heard of
it at all, thought of the idea as a philosophical metaphor or a mathematical trick that
just happened to give correct results. World War I was over, hyperinflation was getting
under way in Germany (a paper mark was worth a few percent of its prewar value), and
the Nazi coup was still in the future, so that Stern, a Jew, had not yet been forced to flee to America.
Because of the difficult economic situation, Stern and his colleague Walther Gerlach scraped
up some of the funds to carry out the experiment from US banker Henry Goldman, cofounder of the
investment house Goldman-Sachs.


\pagebreak

<% marg(300) %>
<%
  fig(
    'stern-gerlach',
    %q{%
      Bottom: A schematic diagram of the Stern-Gerlach experiment. The $z$ direction is out of the page.
      The entire apparatus is about 10 cm long.
      Top: A portion of Gerlach's celebratory 1922 postcard to Niels Bohr, with a photo showing the results.
      A coordinate system is superimposed. The orientation is flipped downward by 90 degrees compared to the
      schematic. The photo was taken through a microscope, and Gerlach drew the 1.0 mm scale on after the 
      magnified photo had been printed.
    }
  )
%>
<% end_marg %>

The entire apparatus was sealed inside a vacuum chamber with the best vacuum obtainable
at the time. A sample of silver was heated to $1000\degcunit$, evaporating it.
The atoms leaving the oven encountered two narrow slits, so that what emerged was a beam with
a width of only $0.03\ \zu{mm}$, or about a third of the width of a human hair. The atoms then
encountered a magnetic field. Because the atoms were electrically neutral, we would normally
expect them to be unaffected by a magnetic field. But in the planetary model of the atom, we
imagine the electrons as orbiting in circles like little current loops, which would give
the atom a magnetic dipole moment $\vc{m}$. Even if we are sophisticated enough about quantum mechanics not to
believe in the circular orbits, it is reasonable to imagine that such a dipole moment would exist.
When a dipole encounters a \emph{nonuniform}
field, it experiences a force
(example \ref{eg:dipole-in-nonuniform-field-fancy}, p.~\pageref{eg:dipole-in-nonuniform-field-fancy}).
In this example, the forces in the $x$ and $z$ directions would be
$F_x=\vc{m}\cdot(\partial\vc{B}/\partial x)$
and $F_z=\vc{m}\cdot(\partial\vc{B}/\partial z)$. (Because of Gauss's law for magnetism,
these two derivatives are not independent --- 
we have $\partial B_x/\partial x+\partial B_z/\partial z=0$.) 
The rapidly varying magnetic field for this experiment
was provided by a pair of specially shaped magnet poles, described in
example \ref{eg:sharp-magnet-poles}, p.~\pageref{eg:sharp-magnet-poles}.

Because electrons have charge, we expect the motion of an electron to give it a magnetic dipole moment $\vc{m}$.
But they also have mass, so for exactly the same reasons, we expect there to be some angular momentum $\vc{L}$ as well.
The analogy is in fact mathematically exact ---
charge is to mass as $\vc{m}$ is to $\vc{L}$ ---
so that classically we expect $\vc{m}$ and $\vc{L}$ to be exactly the same vector, except for a rescaling
by the charge-to-mass ratio $-e/m$. Therefore this experiment with dipoles and magnetic fields is actually
a probe of the behavior of angular momentum at the atomic level.

Luckily for Stern and Gerlach, who had no modern knowledge of atomic structure, the silver atoms that
they chose to use do happen
to have nonzero total $\vc{L}$, and therefore nonzero $\vc{m}$.
The atoms come out of the oven with random orientations. 

Classically, we would expect the following.
Each atom has an energy $\vc{m}\cdot\vc{B}$ due to its interaction with the magnetic field, and
this energy is conserved, so that the component $m_x$ stays constant. However, there is a
torque $\vc{m}\times\vc{B}$, and this causes the direction of the atom's angular momentum to
precess, i.e., wobble like a top, with its angular momentum forming a cone centered on the $x$ axis
(example \ref{eg:precession}, p.~\pageref{eg:precession}). This precession is extremely fast, carrying
out about $10^{10}$ wobbles per second, so that the atom precesses about $10^6$ times while traveling the
3.5 cm length of the spectrometer. So even though the forces $F_x$ and $F_z$ are typically about
the same size, the rapid precession causes $F_z$ to average out to nearly zero, and only a deflection
in the $x$ direction is expected. Because the orientations of the atoms are random as they enter the
magnetic field, they will have every possible value of $L_x$ ranging from $-|\vc{L}|$
to $+|\vc{L}|$, and therefore we expect that when the magnetic field is turned on, the effect should
be to smear out the image on the glass plate from a vertical line to a somewhat wider oval. The atoms
are dispersed from left to right along a certain scale of measurement according to their random
value of $L_x$. The spectrometer is a device for determining $L_x$,
a continuously varying number.

But that's all the classical theory. Quantum mechanically, $L_x$ is
quantized, so that only certain very specific values of $F_x$ can
occur. Although the discussion of precession above is really classical
rather than quantum-mechanical, the result of $F_z$ averaging out to
zero turns out to be approximately right if the field is strong.
Therefore we expect to see well separated vertical bands on the glass
plate corresponding to the quantized values of $L_x$.  This is
approximately what is seen in figure \figref{stern-gerlach},
although the field rapidly weakens outside the $x$-$y$ plane, so we get the slightly
more complicated pattern like a sideways lipstick kiss. Since we
observe two values of $L_x$ (the two ``lips''), we conclude from these
results that a silver atom has spin $1/2$, so that $L_x$ takes on the
values $-\hbar/2$ and $+\hbar/2$. Although it took about five years for
the experiment to be interpreted completely correctly, we now understand
the Stern-Gerlach experiment to be not just a confirmation of the quantization
of angular momentum along any given axis but also the first experimental
evidence that the electron has an intrinsic spin of 1/2.

\startdqs

\begin{dq}
Could the Stern-Gerlach experiment be carried out with a beam of electrons?
\end{dq}

\begin{dq}
A few weeks after the Stern-Gerlach experiment's results became
public, Einstein and Ehrenfest carried out the following reasoning,
which seemed to them to make the results inexplicable.
Before a particular silver atom enters the magnetic
field, its magnetic moment $\vc{m}$ is randomly oriented. Once it
enters the magnetic field, it has an energy $\vc{m}\cdot\vc{B}$.
Unless there is a mechanism for the transfer of energy in or out of
the atom, this energy can't change, and therefore the magnetic moment
can only precess about the $\vc{B}$ vector, but the angle between
$\vc{m}$ and $\vc{B}$ must remain the same. Therefore the atom cannot
align itself with the field. (They considered various mechanisms of
energy loss, such as collisions and radiation, and concluded that all
of them were too slow by orders of magnitude to have an effect during
the atom's time of flight.) It seemed to them that as soon as the atom left
the oven, it was somehow required
to have anticipated the direction of the field and picked one of two orientations
with respect to it. How can this paradox be resolved?
\end{dq}

\begin{dq}
Suppose we send a beam of atoms with $L=\hbar$ through a Stern-Gerlach spectrometer,
throwing away the emerging parts with $\ell_x=-1$ and $+1$ to make a beam of the pure $\ell_x=0$ state.
Now we let this beam pass through a second spectrometer that is identical
but oriented along the $z$ axis. Can we produce a beam in which
every atom has both $\ell_x=0$ and $\ell_z=+1$? 
Hint: See example \ref{eg:completeness-ylm}, p.~\pageref{eg:completeness-ylm}.
\end{dq}

<% end_sec() %>

<% begin_sec("Rotation and vibration",4,'qm-rot-and-vib') %>

<% begin_sec("Types of excitations",nil,'types-of-excitations') %>
Figure \figref{n2-spectrum} shows the visible-light spectrum of the molecule $\zu{N}_2$.
Because this particular chemical bond is unusually strong, the molecule does not break apart, even at the high
temperature of a gas discharge tube, so we see the spectrum of the molecule, not of monoatomic nitrogen.
This spectrum is more complex than the spectrum of the hydrogen atom, and that's not surprising,
because the number of different states grows exponentially with the number of particles (here, 14 electrons
plus 2 nuclei).

<%
  fig(
    'n2-spectrum',
    %q{%
      Visible spectrum of $\zu{N}_2$. Violet is on the left, red on the right.
    },
    {
      'width'=>'wide',
      'sidecaption'=>true
    }
  )
%>

What is more surprising is that there are some clear, simple patterns in this spectrum --- patterns
simpler than any that we would see in the spectrum of a monoatomic gas with the same number of particles.
To start to understand this, we note that the $\zu{N}_2$ lacks the spherical symmetry of an individual
atom, but it does have an axis of symmetry, \subfigref{axial-symmetry}{1}. These properties are also possessed
by many nuclei, e.g., \subfigref{axial-symmetry}{2}. We now consider three different ways in which an excited
energy state could occur in $\zu{N}_2$:

\begin{itemize}
\item Individual \emph{particles} (electrons) can be raised to a higher energy level.
\item The molecule can \emph{vibrate} along its long axis, so that the nuclei (which have nearly all the inertia)
       move back and forth, elongating and compressing the system.
\item The molecule can \emph{rotate}.
\end{itemize}

<% marg(50) %>
<%
  fig(
    'axial-symmetry',
    %q{%
      1.~The molecule $\zu{N}_2$. 2.~The nucleus ${}^{178}\zu{Hf}$. 
    }
  )
%>
<% end_marg %>

<% end_sec('types-of-excitations') %>

<% begin_sec("Vibration",nil,'qm-vibration') %>
Particle excitations would produce the type of very complex, disorganized spectrum that we normally see in atoms
that have many electrons, so that isn't what we're seeing in figure \figref{n2-spectrum}. What about vibrations?
For a classical harmonic oscillator, we know that the frequency of vibration is independent of the amplitude.
If a classical oscillator contains electric charge, it will emit electromagnetic radiation at this frequency,
smoothly and continuously draining itself of energy. As the energy is lost, the frequency stays the same. 
By the correspondence principle, we expect that when the quantum mechanical version of such a system is highly
excited, it should emit a large number of photons of this frequency $f$, so that the discrete quantum jumps
are undetectable and the radiation appears as a classical wave. We can thus infer that for a quantum vibrator,
the excited states should show an \emph{evenly spaced} ladder of energy levels.

Figure \figref{n2-level-scheme} shows how the series of red lines in figure \figref{n2-spectrum} arises.
For an excitation consisting only of vibrational motion, we
expect based on the correspondence principle to see the evenly spaced ladder of states shown in a stack
built above the ground state, with all of the photons having the same energy. These states and transitions
do exist, but the light lies in the infrared spectrum and so is not seen in figure \figref{n2-spectrum}.
The red visible-light lines arise as shown in the second box, from states that involve both a certain
particle excitation and some vibration. Because the spacing of the two ladders is slightly unequal, the
red lines all have slightly different wavelengths.

<% marg(300) %>
<%
  fig(
    'n2-level-scheme',
    %q{%
      Energy levels of the $\zu{N}_2$ molecule. 
    }
  )
%>
<% end_marg %>

<% end_sec('qm-vibration') %>

<% begin_sec("Rotation",nil,'qm-rotation') %>
What about rotation? An interesting thing happens here due to the structure of quantum mechanics. Quantum
mechanics can describe motion only as a wave, with the value of the wave oscillating from one place to another.
But this implies that according to quantum mechanics, no object can rotate about one
of its axes of symmetry, for the rotated version of a state would then be the same state.
This is why rotational excitations are never seen in individual atoms, or in nuclei
that have spherical shapes. In examples like the ones in figure \figref{axial-symmetry}, which have a single
axis of symmetry, we can therefore have end-over-end rotation, but never rotation about the symmetry axis.
Such end-over-end rotational states are observed in $\zu{N}_2$, for example, but because this involves
large motions by the high-mass nuclei, the moment of inertia $I$ is quite large, and therefore the rotational
energies --- classically, $K=L^2/2I$ --- are very small, and infrared rather than visible photons are emitted.
If rotation about the symmetry axis were possible, then the moment of inertia would be thousands of times smaller,
because in such a rotation the nuclei would not move. The energies involved would be thousands of times higher,
and the photons would lie approximately in the visible region of the spectrum. No such visible lines are actually observed.

<% marg(-300) %>
<%
  fig(
    'hf-rotation',
    %q{%
      Excited states of the nucleus ${}^{178}\zu{Hf}$. Black squares represent states that are interpreted
      as end-over-end rotation, while white diamonds show particle excitations. For each angular momentum,
      the graph shows the lowest-energy state of each type, where known. 
    }
  )
%>
<% end_marg %>

Perhaps more vivid evidence for the nonexistence of rotation about a symmetry axis is shown in figure
\figref{hf-rotation}. The states involving end-over-end rotation of the nucleus as a whole (``collective'' rotation)
are approximately a parabola on this graph, which is reasonable given the classical relation $K=L^2/2I$.
But angular momentum cannot be generated along the symmetry axis through collective rotation. Instead,
we see an irregular set of energy levels in which first one particle (for $L\le8\hbar$) and then two
(14 and 16 $\hbar$) are excited.

Note that only even multiples of $\hbar$ are observed in collective rotation in figure \figref{hf-rotation}.
This is because the nucleus's shape has an additional mirror symmetry, so that it is unaffected by
a 180-degree rotation. This means that the wavefunction describing the collective rotation
must oscillate twice as we pass through a full rotation.
<% end_sec('qm-rotation') %>

<% begin_sec("Corrections to semiclassical energies",nil,'qm-corrections') %>

So far we've been using the correspondence principle to make educated guesses about quantum-mechanical
expressions for the energies of vibrators and rotors. This style of reasoning is called semiclassical,
because it combines ideas from classical and quantum physics. These expressions are guaranteed to be good approximations 
in the classical limit obtained when
the quantum numbers are large, but figure \figref{rot-and-vib-corrections} shows that the approximations
can be poorer when the quantum numbers are small.

<%
  fig(
    'rot-and-vib-corrections',
    %q{%
      Quantum-mechanical corrections to the semiclassical results for the energy of a vibrator and a rotor.
      The rotational levels are shown for the case of a rotor with mirror symmetry, so that only even values of
      $\ell$ occur.
    },
    {
      'width'=>'wide',
      'sidecaption'=>true
    }
  )
%>

In the case of the $n$th excited state of a vibrator, the energy is $(n+1/2)hf$, where the $+1/2$ term represents
a quantum correction to the semiclassical approximation. This shifts the entire ladder upward in energy by half
a step. In particular, the energy of the ground state is not zero but rather $(1/2)hf$. This can be verified
quantitatively by calculating the energy for the solution to the Schr\"odinger discussed using the guess-and-check method
in problem \ref{hw:quantumho}, p.~\pageref{hw:quantumho}. It is easy to see why the answer cannot be zero, for
if it were, then the particle in the ground state would have zero kinetic energy and zero potential energy.
To have zero kinetic energy, it would have to have a momentum of exactly zero, so $\Delta p=0$, but to have
zero potential energy it would also have to sit still at exactly the equilibrium position, so $\Delta x=0$. But
this would violate the Heisenberg uncertainty principle and so is impossible.

The inevitable motion that
is present even in the ground state is known as zero-point motion, and its energy is the zero-point energy.
Relativity tells us that $E=mc^2$, so the zero-point energy of particles is equivalent to a certain amount of
mass. In fact, nearly all the mass of ordinary matter arises from the zero-point energy of the quarks inside
the neutrons and protons. Another interesting application is to spontaneous nuclear fission, which is the basis
for nuclear energy, providing the kick-off for a chain reaction. Spontaneous fission requires that a
nucleus become more and more elongated until it breaks apart into two pieces. The very elongated shapes have
a high potential energy, so that spontaneous fission requires quantum-mechanical tunneling. If it were not for
the zero-point vibrational energy associated with this motion, the tunneling probability for uranium and
plutonium isotopes would be extremely small. These isotopes would decay only by alpha emission, and nuclear
reactors and bombs would not work.

<%
  fig(
    'j-jplus1-visual',
    %q{%
      Each panel of the figure shows a standing wave on a sphere, with the convention that gray is zero, white is a
      positive real number, and black is a negative real number. (These could instead have been drawn as traveling
      waves, but then we would have needed to represent complex numbers using color, as in figure \figref{spherical-harmonic}
      on p.~\pageref{fig:spherical-harmonic}.) 
      Only 2 is a solution of the Schr\"odinger equation.
    },
    {
      'width'=>'wide',
      'sidecaption'=>true
    }
  )
%>

Figure \figref{j-jplus1-visual} shows visually the reason for the correction of $\ell^2$ to $\ell(\ell+1)$.
Each of these standing waves has $|\ell_z|=16$, where $z$ is the vertical
axis. But only \subfigref{j-jplus1-visual}{2} is a solution of the Schr\"odinger equation or a state of definite
$\ell$.
To be a solution of the Schr\"odinger equation, such a wave must have the same kinetic energy everywhere. 
Each of these three has the same kinetic energy associated
with its wavelength in the ``east-west,'' or azimuthal, direction.
Wave
\subfigref{j-jplus1-visual}{1} is not a solution, because near the equator, it has an extremely short wavelength
in the ``north-south,'' or longitudinal, direction, and this gives it a greater kinetic energy near the equator than
elsewhere. The opposite problem occurs in \subfigref{j-jplus1-visual}{3}, where the wave is constant in the longitudinal
direction; at the poles, the wavefunction varies infinitely rapidly,
and therefore the kinetic energy blows up to infinity there. The only valid solution is
\subfigref{j-jplus1-visual}{2}, which has a Goldilocks-style just-right wavelength in the longitudinal direction,
The kinetic energy associated with this wavelength is the difference between the semiclassical
$\ell^2$ and the correct quantum mechanical $\ell(\ell+1)$. 

A different example that is particularly easy
to reason about is the wavefunction $\Psi_{10}$ shown in figure \figref{completeness-ylm}
on p.~\pageref{fig:completeness-ylm}, for $\ell=1$ and $\ell_z=0$. (The odd value of $\ell$ is possible for
a rotor that doesn't have mirror symmetry, e.g., the carbon monoxide molecule CO.) The ratio of the
correct quantum mechanical energy to the semiclassical one is $\ell(\ell+1)/\ell^2=2$, and the factor of
two  makes sense because at the poles, the wave has equal contributions to its kinetic energy due to
oscillations in the two perpendicular directions that occur in the Laplacian $\partial^2/\partial x^2+\partial^2/\partial y^2$.

<% end_sec('qm-corrections') %>

<% end_sec() %>

<% begin_sec("The underlying structure of quantum mechanics",nil,'qm-structure') %>

So far we have been building up the structure of quantum mechanics by casually laying
one brick on top of another, but at this point it will be advantageous to pause and
consider the broader blueprint.

<% begin_sec("The time-dependent Schr\\\"odinger equation",nil,'time-dependent-s-eqn') %>
For simplicity our discussion of the Schr\"odinger equation in section
\ref{subsec:schrodinger}, p.~\pageref{s-eqn-simplest-initial-statement}, was limited to standing waves, allowing us to avoid
explicitly discussing how the wavefunction changed with passing time. Let's consider
the generalization to the full time-dependent case.

Classically, suppose I show you a picture of a baseball next to a tree, and I ask you how
long it will take to hit the ground. You can't tell, because you also need information about the ball's initial
velocity. That is, the future time-evolution of the system $x(t)$ depends not just on the initial position $x(0)$
but also on its initial time derivative $x'(0)$.

But if I show you a uranium atom in its lowest energy state, you don't
need to know any other information to predict everything that can be
predicted about its future decay. Whereas the baseball could be thrown downward
in order to make it reach the ground more quickly, nobody knows of any way to
prepare the uranium nucleus in such a way that it is any more likely to decay
sooner. Knowing the initial wavefunction $\Psi(0)$ to be that of the ground state
lets us say as much as can be said about the future time-evolution $\Psi(t)$, and it's
neither necessary nor helpful to know the time derivative $\Psi'(t)$.

This is an example of a more general idea about the interpretation of quantum mechanics,
which is that the wavefunction is a complete description of any
system. There isn't more information that can be known about the system.
(The phase and normalization of the wavefunction are not considered to give
any information, since the phase is unobservable, and the normalization can be standardized
so that the total probability is 1.) This principle seems to be widely agreed upon by
physicists, but doesn't seem to have a standard name.

\begin{important}[Wavefunction fundamentalism]
All knowable information about a system is encoded in its wavefunction (ignoring phase
and normalization).
\end{important}

\noindent The pilot wave hypothesis, discussed on p.~\pageref{subsubsec:pilot-wave},
is an example of an idea that would violate this principle.

Another example that shows the contrast with the classical description is that
if I show you a snapshot of a wave on a string, you can't tell which
direction it's going --- as with the baseball, you need to know its
initial velocity in addition. But
if I show you a snapshot of a quantum-mechanical traveling wave, you \emph{can} tell which
direction it's going, because of the complex phase, as shown in figures
\subfigref{rainbow}{2} and \subfigref{rainbow}{3} on page \pageref{fig:rainbow}.
Note that this mechanism wouldn't work if wavefunctions were always real numbers,
so wavefunction fundamentalism implies complex wavefunctions.

Given the wavefunction at some initial time, we can predict its evolution into the future
by making use of the principle that $E=hf$. Suppose for example that we have a sinusoidal plane wave
traveling to the right. Then we expect the value of the wavefunction at a particular point in space
to rotate clockwise about the origin in the complex plane at the appropriate freqency $f$, showing
a time dependence $e^{-i\omega t}$ (where, as usual, $\omega=2\pi f$). Thus the time derivative
of the wavefunction is $\Psi'=-i\omega\Psi=-i(E/\hbar)\Psi$, so that $E\Psi=i\hbar\Psi'$. Then to
generalize the time-independent Schr\"odinger equation to its time-dependent version, the most
obvious thing to try is simply to substitute $i\hbar\partial\Psi/\partial t$ for $E\Psi$, which gives
\begin{equation*}\index{Schr\"odinger equation!time-dependent}
  i\hbar\frac{\partial\Psi}{\partial t} = 
              -\frac{\hbar^2}{2m}\nabla^2 \Psi  +   U \Psi   .
\end{equation*}
Unlike Newton's laws of motion, which refer to a second derivative with respect to time, the Schr\"odinger equation
involves only a first time derivative. This is why we don't need initial data on $\partial\Psi/\partial t$, but only
$\Psi$: if we know $\Psi$, then the right-hand side of the Schr\"odinger equation is what \emph{gives}
us $\partial\Psi/\partial t$. But the Schr\"odinger equation has some other properties
that match up with those of Newton's laws. It is completely deterministic, so that if we know
$\Psi$ initially, we can always predict it in the future. We can also ``predict'' backward in time,
so that the system's history can always be recovered
from knowledge of its present state. Thus there is never any loss of information over time. Furthermore,
it can be shown that probability is always conserved, in the sense that if the wavefunction is initially
normalized, it will also be normalized at all later times. 

\begin{important}[Unitary evolution of the wavefunction]\index{unitary evolution of the wavefunction}
The wavefunction evolves over time, according to the Schr\"odinger equation, in a deterministic
and \emph{unitary} manner,
meaning that probability is conserved and information is never lost.
\end{important}

Since we think of quantum mechanics as being all about randomness, this determinism may seem surprising.
But determinism in the time-evolution of the wavefunction isn't the same as determinism in the results
of experiments as perceived and recorded by a human brain. Suppose that you prepare a uranium atom in
its ground state, then wait one half-life and observe whether or not it has decayed, as in the thought
experiment of Schr\"odinger's cat. There is no uncertainty or randomness about the wavefunction of the
whole system (atom plus you) at the end. We know for sure what it looks like.
It consists of an equal superposition of two states,
one in which the atom has decayed and your brain has observed that fact, and one in which the atom has
not yet decayed and that fact is instead recorded in your brain. 

To get more of a feeling for what is meant by unitarity, it may be helpful to consider some examples of
how it could be violated. In the Copenhagen interpretation of quantum mechanics
(p.~\pageref{copenhagen}), measurement causes the wavefunction to collapse to one randomly chosen
possibility. The other possibilities disappear, and there is no way to tell that they ever existed.
Along with the many other problems with the Copenhagen interpretation, this shows that it requires
nonunitarity.\index{Copenhagen interpretation!nonunitary}

Another interesting example is the disappearance of matter into a black hole.
If I throw my secret teenage diary into a black hole, then it contributes a little bit to the black
hole's mass, but the embarrassing information on the pages is lost forever. This loss of information
seems to imply nonunitarity. This is one of several arguments suggesting that quantum mechanics cannot
fully handle the gravitational force. Thus although physicists currently seem to possess a completely successful
theory of gravity (Einstein's theory of general relativity) and a completely successful theory of
the microscopic world (quantum mechanics), the two theories are irreconcilable, and
we can only make educated guesses, for example, about the behavior of a hypothetical microscopic black hole.

\begin{eg}{A plane wave}\label{eg:t-dep-s-eqn-plane-wave}
Consider a free particle of mass $m$ in one dimension, with the wavefunction $\Psi=e^{ikx-i\omega t}$.
Then application of the Schr\"odinger equation, $i\hbar\partial\Psi/\partial t=-(\hbar^2/2m)\partial^2\Psi/\partial x^2$,
gives
\begin{equation*}
  \hbar\omega e^{(\ldots)} = \frac{\hbar^2k^2}{2m} e^{(\ldots)},
\end{equation*}
and if this is to hold true for all values of $x$ and $t$, then we must have
\begin{equation*}
  \hbar\omega = \frac{\hbar^2k^2}{2m},
\end{equation*}
which is simply an expression of the Newtonian relation $K=p^2/2m$, since $k\lambda=2\pi$ and $p=h/\lambda$.
\end{eg}

\begin{eg}{Dispersion of a wave packet}
An annoying feature of example \ref{eg:t-dep-s-eqn-plane-wave} is that the wavefunction cannot be normalized
because it extends in all directions to infinity. This type of infinite plane wave
is at best an idealization of the wavefunction for a realistic example such an electron launched by a cathode
ray tube, or an alpha particle emitted by a nucleus. As a more realistic example, we might try something
like a wave packet, such as a pulse with a certain shape, traveling in a certain direction.
This works for waves on a string or for electromagnetic waves: the pulse or packet simply glides along
while rigidly maintaining its shape. To investigate this idea using the time-dependent Schr\"odinger equation,
it will be convenient to adopt the frame of reference in which the particle is at rest. In this frame,
we would expect the wave to look frozen, just as an ocean wave looks frozen in place to a surfer who is
riding it. It must therefore be of the form
\begin{equation*}
  \Psi = e^{-i\omega t}f(x),
\end{equation*}
where $f$ is some function specifying the shape of the wave packet.
But this $\Psi$ is not a solution to the Schr\"odinger equation. On the left-hand side
of the Schr\"odinger equation, evaluating the time derivative gives
$\hbar\omega\Psi$, which is just the original wavefunction multiplied by a constant.
If we are to satisfy the Schr\"odinger equation, then the right-hand side, which is the second derivative
with respect to $x$, must also
be equal to the original wavefunction multiplied by a constant. But the only functions for which
$(\der^2/\der x^2)(\ldots)=(\text{constant})(\ldots)$ are exponentials and sine waves. An exponential shape
can't be a physical realization of a wave packet, because it can't even be a normalized. A sine wave
works, but it just describes an infinite plane wave like the one in example \ref{eg:t-dep-s-eqn-plane-wave}, not
a wave packet that can be localized and normalized.

The underlying reason for this result is that the Schr\"odinger equation is dispersive: waves with different wavelengths
travel at different speeds (because they correspond to different momenta). Suppose a pulse has the
shape $f(x)$ at $t=0$. Since a pulse is not a sine wave, it doesn't have a single well-defined wavelength, and therefore
it doesn't have a definite momentum or velocity. In fact, the spread in momentum must be at least a certain
size due to the Heisenberg uncertainty principle $\Delta p\Delta x\gtrsim h$. This causes the pulse to spread out over time.

This leads to a strange thought experiment. Suppose that a uranium atom in the Andromeda galaxy emits an alpha particle,
which then travels thousands of light years and eventually flies past the earth. Its wave packet may initially have
been as narrow as the diameter of an atomic nucleus, $\sim 10^{-15}\ \munit$, but by the time it arrives perhaps it
is the size of an aircraft carrier. Will an observer see a gigantic alpha particle flying by? No, because observing
it constitutes a measurement of its position, and by the probability interpretation of the wavefunction this
measurement simply has a certain probability of giving a result that is anywhere within some region the size of
an aircraft carrier.
\end{eg}

<% end_sec('time-dependent-s-eqn') %>

<% begin_sec("Observables",nil,'qm-observables') %>
By the time my first-year mechanics students have been in class for a week, they know how to answer
when I ask them the velocity of the tape dispenser at the front of the classroom: ``We don't know, it
depends on your frame of reference.'' The \emph{absolute} velocity of an object is a meaningless concept,
part of the mythical dungeons-and-dragons cosmology of Aristotelian physics. Quantum mechanics is as great
a break from Newton as Newton was from Aristotle, and similar care is required in redefining what concepts
are \emph{observables} --- meaningful things to talk about measuring.

\newcommand{\eqnimage}[1]{\raisebox{-.2\height}{\includegraphics[resolution=300]{../share/quantum/figs/#1}}}

Classically, we describe the state of the system as a point in phase
space (sec.~\ref{subsec:phase-space}, p.~\pageref{subsec:phase-space}) --- which is just a fancy way of
saying that we specify all the positions and momenta of its particles ---
and an observable is defined as a function that takes that
point as an input and produces a real number as an output. (By the way, the word ``phase'' in ``phase space''
doesn't refer directly to the phase of a wave, which we'll also be discussing below.)
For example, kinetic energy is a classical observable, and 
$K(\eqnimage{eqn-tennis-ball})=0$, where the picture represents
a tennis ball at rest. For a moving tennis ball with one unit of energy, 
$K(\eqnimage{eqn-tennis-ball-moving})=1$.
For a vibrating violin string, we could have 
$U(\eqnimage{eqn-wave2-small-amp})=1$, and $U(\eqnimage{eqn-wave2})=4$ (where doubling the amplitude gives
four times the energy).

Quantum-mechanically, the Heisenberg uncertainty principle tells us that we can't
independently dial in the desired values of a particle's position and momentum. They aren't
two variables that are independent of one another. Therefore we don't have a phase space, so an observable
has to be represented by a function whose input is a wavefunction.
Furthermore, we expect that:
\begin{itemize}\label{operator-criteria}
\item The output shouldn't depend on the phase\footnote{``Phase'' as in the phase of a wave, not as in ``phase space.''} of the wavefunction.
\item The output shouldn't depend on amplitude (because a different amplitude might just mean an incorrectly normalized state).
\item The output should be well defined when we superpose any two states.
\end{itemize}
\noindent These requirements are hard to reconcile with the idea that
the output of the observable is just a real number representing the
result of the measurement. We could decree that the input wavefunction is just
required to be have the standard normalization, but there's no obvious
way to define a standardization of phase. And suppose we have a
particle in a one-dimensional box, with the two lowest energies being
$E(\eqnimage{eqn-wave1})=1$ and $E(\eqnimage{eqn-wave2})=4$.
Then what should we define
for the superposition $E(\eqnimage{eqn-wave1}+\eqnimage{eqn-wave2})$?
We could define it to be the average, 2.5, but
that isn't even a possible value of the measurement; in reality, the
result of the measurement would be either 1 or 4, with equal
probability.

For a clue as to a better way to proceed, note the structure of the
time-independent Schr\"odinger equation for a free particle, omitting all constant
factors like $m$, 2, and $\hbar$. It isn't $(\der^2/\der x^2)\Psi=E$, it's
$(\der^2/\der x^2)\Psi=E\Psi$. This fixes all the problems. For example,
if we change the phase of the wavefunction by flipping its sign, the equation
still holds with the same value of $E$. This equation is a specific example
of a more general type of equation that looks like
\begin{equation*}
  \text{operator}(\text{input}) = \text{number}\times\text{input}.
\end{equation*}
Another, simpler example is $(\der/\der x)f=3f$, which is satisfied if $f=Ae^{3x}$, where
$A$ is any constant.
Such an equation says that applying the operator to the input just gives back the
\emph{input itself}, multiplied by some constant. For this reason, this type of
equation is called an \emph{eigenvalue equation}, because ``\emph{eigen}'' is the
German word for ``self.'' We say that 3 is the eigenvalue of 
the eigenvalue equation $(\der/\der x)f=3f$.
In the time-independent Schr\"odinger equation, the eigenvalue is the energy,
and a solution $\Psi$ is called a state of definite energy (or
``eigenstate''). 

All observables in quantum mechanics are described by operators such as derivatives.
The second derivative (with the appropriate factor
of $-h^2/2m$) is the kinetic energy operator in quantum mechanics.
Given an operator $\mathcal{O}$ that describes a certain observable,
a state $\Psi$ with a definite value $c$ of that observable is one for which
$\mathcal{O}(\Psi)=c\Psi$. Although it's common to use parentheses when notating functions,
as in $\cos(\pi)=-1$, they are optional, and we can write $\cos\pi=-1$, so we will often
use notationa like $\mathcal{O}\Psi$ instead of $\mathcal{O}(\Psi)$, but keep in mind
that this not multiplication, just as $\cos\pi$ doesn't mean multiplying $\cos$ by $\pi$.

When we carried over the classical kinetic energy observable to quantum mechanics, we weren't
going blind. For example, the factor of $-h^2/2m$ in front is tightly
constrained by requirements like units and the need for a traveling
sine wave to have positive energy. But for the superposition of two
states, classical mechanics will never give us any guidance. For
example, what is the body temperature of Schr\"odinger's cat? For the
energy operators appearing in the Schr\"odinger equation, we used linear operators.
The
result was that our law of physics was perfectly linear, and this is a
hard requirement, for the reasons described on p.~\pageref{subsubsec:linearity-of-schrodinger}.
It therefore seems natural to require that \emph{all} observables be represented by
linear operators,
\begin{equation*}
  \mathcal{O}(\Psi_1+\Psi_2)=\mathcal{O}\Psi_1+\mathcal{O}\Psi_2.
\end{equation*}
Indeed, if they were not linear, then quantum mechanics would lack
self-consistency, for the act of measurement can be described by applying the Schr\"odinger equation
to a big system consisting of the
system being observed interacting with the measuring device.

Finally, we have one more requirement, which is that the linear
operator representing an observable should have eigenvalues that are
real. This isn't because the results of a measurement must logically
be real --- e.g., we can measure complex impedances. But in any
real-world application of the complex number system, we must always
choose some arbitrary phase conventions, such as that an inductor has a
positive imaginary impedance to represent the fact that the voltage
leads the current by 90 degrees. (Such phase conventions are always
arbitrary because we
define $i$ as $\sqrt{-1}$, but this doesn't distinguish $i$ from $-i$.)
These phase conventions are all independent of one another, and the
classical ones are independent of the convention used for
wavefunctions in quantum mechanics, which is that a state with positive
energy spins clockwise in the complex plane.

\begin{important}[Observables]
In quantum mechanics, any observable is represented by a linear operator that takes
a wavefunction as an input and has real eigenvalues.
\end{important}

\begin{eg}{The momentum operator}\label{eg:momentum-operator}
Quantum mechanics represents motion as a dependence of the wavefunction on
position, so that a constant wavefunction has no motion.
This suggests defining
the momentum operator as the derivative with respect to position.
This almost works, but needs to be tweaked a little.
We expect that a state of
definite momentum is a sine wave of the form $\Psi=e^{ikx}$. We have $k\lambda=2\pi$
and $p=h/\lambda=\hbar k$, and the sign is a matter of convention. Taking the
derivative of $\Psi$ gives an eigenvalue $ik$, which has the wrong units
(easily fixed by tacking on a factor of $\hbar$), but more
importantly is not real. This suggests defining the momentum operator
as 
\begin{equation*}
  \mathcal{O}_p=-i\hbar\frac{\der}{\der x}.
\end{equation*}
\end{eg}

\begin{eg}{A nonexample}\label{eg:not-an-observable}
Consider the one-dimensional particle in a box, and restrict our attention to
the two lowest-energy states and their superpositions.
Define an operator $\mathcal{O}$ by the rule
\begin{align*}
  \mathcal{O}(\eqnimage{eqn-wave1}) &= \eqnimage{eqn-wave2} \\
  \mathcal{O}(\eqnimage{eqn-wave2}) &= -(\eqnimage{eqn-wave1}) .
\end{align*}
Since $\mathcal{O}$ is linear, defining its action on $\eqnimage{eqn-wave1}$ and
$\eqnimage{eqn-wave2}$ suffices to define its action on the superpositions of these
states as well.
This operator
has eigenvalues, one of which is $i$, corresponding to the state
$\eqnimage{eqn-wave1}-i\eqnimage{eqn-wave2}$.
(It also has a second eigenvalue, which is imaginary as well.)
Because this operator doesn't have real eigenvalues, it is not a valid
observable.
\end{eg}

Note that in examples \ref{eg:momentum-operator} and
\ref{eg:not-an-observable}, it doesn't matter whether the operator is
\emph{defined} using complex numbers. Our definition of the momentum
operator was stated using an equation that had an $i$ in it, but its
eigenvalues are real, so that's OK. The operator $\mathcal{O}$ in
example \ref{eg:not-an-observable} was defined using only real
numbers, but its eigenvalues are not real. 

\begin{eg}{Position is an observable}\label{eg:x-is-observable}
If we have a wavefunction $\Psi(x)$ expressed as a function of position $x$, then
we simply take the operator for position $\mathcal{O}_x$ to be multiplication by the number $x$,
\begin{equation*}
  \mathcal{O}_x(\Psi)=x\Psi .
\end{equation*}
For example, if $\Psi=e^{ix}$ (ignoring units), then \mathcal{O}_x(\Psi)=xe^{ix}$.
This operator is definitely linear, because multiplication by a number is linear, e.g.,
$7(a+b)=7a+7b$. The only question is whether it has eigenvalues, and whether those are
real. A state of definite $x$, say a state with $x=0$, would have to be represented by a wavefunction $\Psi(x)$
for which there was zero probability of having $x\ne0$, and this requires us to have
$\Psi(x)=0$ for nonzero $x$.
But what would be the value of $\Psi(0)$? It has to be \emph{infinite} if $\Psi$ is to be properly
normalized. With this motivation, the physicist P.A.M.~Dirac defined
Dirac delta function,
\begin{equation*}
  \delta(x)=\begin{cases}
    0 & \text{for $x\ne0$} \\
    +\infty & \text{for $x=0$}
  \end{cases}
\end{equation*}
Its graph is an infinitely narrow, infinitely tall spike at $x=0$, and it has
$\int_{-\infty}^{+\infty}\delta(x)\der x=1$.
Mathematicians will shake their heads and say that this is not a definition of a function, but
it's very useful to pretend that it is, and the delta ``function'' is widely used in a variety of
fields such as electrical engineering. Because it was useful, mathematicians felt obliged to
define a theory in which functions are generalized to things called distributions or generalized functions.
\end{eg}

Becase we represent an observable as an operator that changes a wavefunction into a new wavefunction,
a common misconception is that this change represents the effect of measurement on the system.
Although it is often true that microscopic systems are delicate, so that the act of measurement may have a
significant effect on them, that action of the operator on the wavefunction does not represent
that effect. For example, the position operator $\mathcal{O}_x$ from example \ref{eg:x-is-observable} consists simply
of multiplication of the wavefunction by $x$. Suppose we have a particle in a box with a wavefunction
given by $\Psi=\sin x$, where we ignore units and normalization, and the box is defined by
$0\le x\le \pi$. Then $\mathcal{O}_x\Psi$ eats the
input wavefunction $\sin x$ and poops out the new function $x\sin x$. But the act of measuring the
particle's position clearly can't do anything like this --- for one thing, the function $x\sin x$ 
has larger values on the right side of the box than on the left, but there is nothing to create
such an asymmetry in either the original state or the measuring process. The real-world effect of
the measurement would probably be to knock the particle out of the box completely, since a high-resolution
measurement will have a small uncertainty $\Delta x$, which by the Heisenberg uncertainty principle
means creating a large $\Delta p$.

Nor is it always true that measuring a system disturbs it. For example, suppose that we prepare a
beam of silver atoms, as in the Stern-Gerlach experiment, in such a way that every atom is guaranteed
to be in either a state of definite $L_x=+1/2$ or $L_x=-1/2$. That is, the beam may be a mixture of
both of these possibilities, but each atom is guaranteed have its spin either exactly aligned with
the magnetic field or exactly antiparallel to it. Then the effect of the magnetic field is simply
to sort out the two types of atoms according to spin, without having the slightest effect on those spins.

\begin{eg}{Phase is not an observable}\label{eg:phase-not-observable}
On p.~\pageref{operator-criteria} we listed three criteria for implementing the concept
of an observable in quantum mechanics, and one of these was that since wavefunctions that differ
only by a phase describe the same state, the result of an observation should not depend on phase.
For this reason, it should not be a surprise that the mathematical definition of an observable
that we came up with does not allow for the creation of an observable to describe measurement of
a phase.

By way of rigorous proof, suppose to the contrary that we did have such an observable
$\mathcal{O}_\zu{ph}$. By our definition of an observable, it would have to have some set of
eigenvalues that were real numbers. Consider such an eigenvalue $\varphi$, which might perhaps be
the argument of the wavefunction in the complex plane, although we will not need to assume that.
Let $\Psi$ be the state of definite phase having the phase $\varphi$, so that 
\begin{equation}\label{eqn:phase-not-observable-1}
  \mathcal{O}_\zu{ph}\Psi = \varphi\Psi.
\end{equation}
We can change the phase of $\Psi$ to create a new wavefunction. Let's retard its
phase by 90 degrees, creating $i\Psi$. Since $\Psi$ was a state of definite phase, clearly
$i\Psi$ is as well, and and it must have some different eigenvalue $\varphi'$.
Perhaps $\varphi'=\varphi+\pi/2$, but in any case we must have $\varphi'\ne\varphi$. Then
\begin{equation}\label{eqn:phase-not-observable-2}
  \mathcal{O}_\zu{ph}(i\Psi) = \varphi'(i\Psi).
\end{equation}
But by linearity equation \eqref{eqn:phase-not-observable-2} is equivalent to $i\mathcal{O}_\zu{ph}\Psi=i\varphi'\Psi$, or
$\mathcal{O}_\zu{ph}\Psi=\varphi'\Psi$, and therefore by comparison with
equation \eqref{eqn:phase-not-observable-1}, $\varphi=\varphi'$, which is a contradiction, so
we conclude that there cannot be an observable representing phase.
\end{eg}

The result of example \ref{eg:phase-not-observable} was a bit of a
foregone conclusion, since we specifically designed our notion of an
observable to be insensitive to phase. Therefore this argument is
subject to the objection that perhaps there is some way to measure a
quantum-mechanical phase, but our definition of an observable is just
too restrictive to describe it.  However, we will see later that there
are more concrete reasons why phase cannot be measured. 

\begin{eg}{Time is not an observable}\label{eg:t-not-observable}
We do not expect to have a time operator in quantum mechanics. This
follows simply because an operator is supposed to be a function that takes a
wavefunction as an input, but we typically can't tell what time it is by looking at the
wavefunction. For example, if the electron in a hydrogen atom is in its ground state, then we
could say its energy is zero, so its frequency is zero, the period is infinite,
and the wavefunction doesn't vary at all with time. (We can choose our
reference level for the electrical energy $U_\zu{elec}$ to be anything we like.
Even if we choose it such that the energy of the ground state
is nonzero, the only change in the electron's wavefunction over time will be
a phase rotation, which by example \ref{eg:phase-not-observable} is not observable.)

Of course this doesn't mean that quantum mechanics forbids us from building clocks.
It just tells us that many
quantum mechanical systems are too simple to function as clocks. In particular,
we would be misled if we pictured a hydrogen atom classically in terms of an
electron traveling in a circular orbit around a proton, in which case it really
could act like the hand on a tiny clock.
\end{eg}

Since you've already studied relativity, you've been carefully inculcated with the
idea that space and time are to be treated symmetrically, as parts of a more general
thing called spacetime. The differing results of examples \ref{eg:x-is-observable} and \ref{eg:t-not-observable}
are clearly not consistent with relativity. This is to be expected because
the Schr\"odinger equation is nonrelativistic
(cf.~self-check \ref{sc:schrodingerassumptions}, p.~\pageref{sc:schrodingerassumptions}), and
the principles laid out in this section are the principles of \emph{nonrelativistic} quantum mechanics.

\begin{eg}{Parity}
In freshman calculus you will have encountered the notion of even and odd functions. In quantum
mechanics, we can have even and odd wavefunctions, and they can be distinguished from one another
using the parity operator $\mathcal{P}$.\index{parity!operator in quantum mechanics}
If $\Psi(x)$ is a wavefunction, then $\mathcal{P}\Psi$ is a new wavefunction, call it $\Psi'$,
such that $\Psi'(x)=\Psi(-x)$. In other words, the parity operator flips the wavefunction
across the origin. (In three dimensions, we negate all three coordinates.) States of definite
parity are represented by wavefunctions that are even (eigenvalue $+1$) or odd ($-1$).
\end{eg}

\begin{eg}{States of definite angular momentum}
In section \ref{subsubsec:qm-corrections}, p.~\pageref{subsubsec:qm-corrections}, we saw that the kinetic
energy of a quantum mechanical rotor is proportional not to $\ell^2$ but instead to 
$\ell(\ell+1)$. This was justified qualitatively in terms of the solutions of the 
Schr\"odinger equation for a particle on a sphere, but in fact there is a deeper reason, which is
that the eigenvalues of the orbital angular momentum operator turn out to be $\ell(\ell+1)$.
The parity of such a state is $(-1)^\ell$, which can be seen in figure \figref{hydrogen-three-states}
on p.~\pageref{fig:hydrogen-three-states}.
\end{eg}

<% end_sec('qm-observables') %>

<% begin_sec("The inner product",nil,'qm-inner-product') %>
We've defined the normalization of a wavefunction as the requirement $\int_{-\infty}^{+\infty}\Psi^*\Psi\der x=1$,
which means that the total probability that the particle is \emph{somewhere} equals 1.
(Another way of writing $\Psi^*\Psi$ would be $|\Psi|^2$.) This assumes that the wavefunction is written as
a function of the position $x$. But it is also possible to have a wavefunction that depends on some other
variable, such as spin or momentum, or on some combination of variables, e.g., both the spin $s$ and the position $x$
of an electron, $\Psi(x,s)$. We can also use a wavefunction to describe a correlation between
multiple particles, in which case the wavefunction might look like $\Psi(x_1,x_2)$. The variables that the wavefunction
depends on may be either continuous, like position and momentum, or discrete, like spin or angular momentum.
Given all of these possibilities, we need to figure out an appropriate generalization of the integral over $x$
that we originally used to define our normalization condition. To provide for flexibility and generality, we
will start by simply defining a new notation that looks like this:
\begin{equation*}
  \langle \Psi | \Psi \rangle = 1.
\end{equation*}
In the case where $\Psi$ is a function of $x$ alone, the angle brackets $\langle\ldots|\ldots\rangle$ basically
mean just an integral over $x$, and we think of the $\langle\ldots|$ part as automatically implying the complex
conjugation of the thing inside it. The operation $\langle\ldots|\ldots\rangle$ is called the
\emph{inner product}.\index{inner product!in quantum mechanics}

Because negative probabilities don't make sense, we require that the inner product of a wavefunction with itself
always be positive,
\begin{equation*}
  \langle u | u \rangle \ge 0.
\end{equation*}
This makes it similar to the dot product used with vectors in Euclidean geometry.

In the case of Euclidean geometry, the ability to add vectors and measure their lengths automatically
gives us a way to judge the similarity of two vectors. For example, if $|u|=1$, $|v|=1$, and $|u+v|=2$,
then we conclude that $u$ and $v$ are in the same direction. On the other hand, if
$|u|=1$, $|v|=1$, and $|u+v|=\sqrt{2}$, then we can tell that $u$ and $v$ are perpendicular, which makes
them as different as two unit-length vectors can be. More generally, $(u+v)\cdot(u+v)=|u|^2+|v|^2+2u\cdot v$,
because the dot product is linear, so we can see that the information about how similar $u$ and $v$ are
is all contained in their dot product $u\cdot v$. Making the analogy with quantum mechanics, we expect that
since we can define normalization of wavefunctions, we should automatically get, ``for free,'' a way of
measuring how similar two states are.

With this motivation, we assume that there is an inner product on wavefunctions that has properties
analogous to those of the dot product. We assume linearity, so that if $u$, $v$, and $w$ are wavefunctions,
then
\begin{align*}
  \langle u | \alpha v+\beta w \rangle = \alpha\langle u | v \rangle + \beta\langle u | w \rangle \\
\intertext{and}
  \langle \alpha u+\beta v | w \rangle = \alpha^*\langle u | w \rangle + \beta^*\langle v | w \rangle .
\end{align*}
In the second equation, we need to take the complex conjugates $\alpha^*$ and $\beta^*$, for if we omitted the
conjugation, then when $\langle u | u \rangle=1$ we would have $\langle iu | iu \rangle=-1$, describing a negative
probability.

\begin{important}[Inner product]
Wavefunctions come equipped with an inner product that has the properties described above.
\end{important}

If we're dealing with wavefunctions that are expressed as functions of position, then it's pretty clear
how to define an appropriate inner product: $\langle u | v \rangle = \int u^*v \der x$.
The inner product axiom stated above then requires that this (possibly improper) integral converge in all cases, which
means, for example, that we have to exclude infinite plane waves from consideration. However, because it's
so convenient sometimes to talk about plane waves, we may break this rule when nobody is looking.
Note the similarity between the expression $\int u^*v \der x$ and the expression
$u_xv_x+u_y+v_y+u_zv_z$ for a dot product; the integral is a continuous sum, and the dot product is
a discrete sum. A set of wavefunctions that obeys our inner product postulate is called a Hilbert space,
after mathematician David Hilbert.

Two wavefunctions have a zero inner product if and only if they are completely distinguishable from each other
by the measurement of some observable.
For example, $\langle \eqnimage{eqn-wave1} | \eqnimage{eqn-wave2} \rangle=0$, as can be verified
from the integral $\int_0^\pi \sin x \sin 2x \der x=0$. These states are also distinguishable by measuring
either their momentum or their energy. Let's consider more carefully the general justification for this
assertion that perfect distinguishability is logically equivalent to a zero inner product.

We have described valid observables in quantum mechanics as being represented by operators that have
real eigenvalues. An alternative description of such an operator $\mathcal{O}$,
called a Hermitian operator after Charles Hermite,
is that it is one such that $\langle \mathcal{O} u|v\rangle = \langle  u|\mathcal{O} v\rangle$ for any $u$ and $v$.
To see this, let $e$ be an eigenvalue, $\mathcal{O}u=eu$ for $u\ne0$. Then
$\langle \mathcal{O} u|u\rangle = \langle  u|\mathcal{O} u\rangle$, so
$\langle e u|u\rangle = \langle  u|e u\rangle$, and
$e^*\langle u|u\rangle = e\langle  u| u\rangle$,                   
so $e^*=e$, meaning that $e$ is real.

Using this fact about observables, we now consider states $u$ and $v$ that can be distinguished
empirically because they are states with different values of some observable, $\mathcal{O} u=e_1 u$
and $\mathcal{O} v=e_2 v$. Because $\mathcal{O}$ is Hermitian, we have
$\langle \mathcal{O} u|v\rangle = \langle u|\mathcal{O}v\rangle$, so
$e_1^*\langle u|v\rangle=e_2 \langle u|v\rangle$. But since $e_1$ and $e_2$ are real and unequal, we must have
$\langle u|v\rangle=0$.

Going in the opposite direction, suppose that $\langle u|v\rangle=0$. How can we then conclude that
there exists some observable $\mathcal{O}$ that can distinguish them? There is no straightforward
mathematical reason why
this must be true, but it would not make sense physically to talk about two states that were utterly distinct
and yet indistinguishable by any experiment. We therefore take this as a postulate.

\begin{important}[Dirac's requirement]
There exists a set of observables such that each of these observables can be simultaneously measured along
with all the others, and all states can be expressed as a superposition of the eigenstates of these observables.
\end{important}

<% end_sec('qm-inner-product') %>

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% example \ref{eg:t-not-observable}: state of def energy can't be a clock, need to compare phases; E-t uncertainty

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

<% end_sec('qm-structure') %>

<% end_sec() %>

<% begin_hw_sec %>

<% begin_hw('wholelife') %>__incl(hw/wholelife)<% end_hw() %>
 % 
<% begin_hw('snakeeyes') %>
What is the probability of rolling a pair of dice and
getting ``snake eyes,'' i.e., both dice come up with ones?
<% end_hw() %>
 % 
<% begin_hw('exponentiationapprox') %>
Problem \ref{hw:exponentiationapprox} has been deleted.
<% end_hw() %>
 % 
<% begin_hw('rateofdecayapprox') %>
Problem \ref{hw:rateofdecayapprox} has been deleted.
<% end_hw() %>

\enlargethispage{\baselineskip}

<% begin_hw('heightdistribution') %>__incl(hw/heightdistribution)<% end_hw() %>
 % 
<% marg(15) %>
<%
  fig(
    'hw-unknownisotopes',
    %q{Problem \ref{hw:unknownisotopes}.}
  )
%>
<% end_marg %>
<% begin_hw('unknownisotopes') %>__incl(hw/unknownisotopes)<% end_hw() %>

<% begin_hw('craps') %>__incl(hw/craps)<% end_hw() %>

\enlargethispage{\baselineskip}

<% begin_hw('blindfold-target') %>__incl(hw/blindfold-target)<% end_hw() %>
 % 
<% begin_hw('truncated-half-life') %>__incl(hw/truncated-half-life)<% end_hw() %>

<% begin_hw('maxwellian') %>__incl(hw/maxwellian)<% end_hw() %>

\pagebreak

<% begin_hw('lava') %>
All helium on earth is from the decay of naturally occurring heavy radioactive elements
 such as uranium. Each alpha particle that is emitted ends up claiming two electrons,
 which makes it a helium atom. If the original $^{238}\zu{U}$ atom is in solid
 rock (as opposed to the earth's molten regions), the He atoms are unable to
 diffuse out of the rock. This problem involves dating a rock using the known
 decay properties of uranium 238. Suppose a geologist finds a sample of hardened lava,
 melts it in a furnace, and finds that it contains 1230 mg of uranium
 and 2.3 mg of helium.  $^{238}\zu{U}$ decays by alpha emission, with a half-life
 of $4.5\times10^9$ years.  The subsequent chain of alpha and electron (beta) decays involves
 much shorter half-lives, and terminates in the 
stable nucleus $^{206}\zu{Pb}$.  Almost all natural
 uranium is $^{238}\zu{U}$, and the chemical composition
 of this rock indicates that there were
 no decay chains involved other than that of $^{238}\zu{U}$.\hwendpart
(a) How many alphas are emitted per decay chain? [Hint: Use conservation of mass.]\hwendpart
(b) How many electrons are emitted per decay chain?
  [Hint: Use conservation of charge.]\hwendpart
(c) How long has it been since the lava originally hardened?\answercheck
<% end_hw() %>
 % 
<% begin_hw('mirrorphotons') %>
When light is reflected from a mirror, perhaps only 80\%
of the energy comes back. One could try to explain this in
two different ways: (1) 80\% of the photons are reflected,
or (2) all the photons are reflected, but each loses 20\% of
its energy. Based on your everyday knowledge about mirrors,
how can you tell which interpretation is correct? [Based on
a problem from PSSC Physics.]
<% end_hw() %>
 % 
<% begin_hw('pelightsensor') %>__incl(hw/pelightsensor)<% end_hw() %>
 %  

\pagebreak[4]% otherwise table in next problem is split out of problem

<% begin_hw('cancer') %>
The photoelectric effect can occur not just for metal
cathodes but for any substance, including living tissue. 
Ionization of DNA molecules can cause cancer or birth
defects. If the energy required to ionize DNA is on the same
order of magnitude as the energy required to produce the
photoelectric effect in a metal, which of the following types of
electromagnetic waves might pose such a hazard? Explain.

\begin{tabular}{l}
                60 Hz waves from power lines\\
                100 MHz FM radio\\
                microwaves from a microwave oven\\
                visible light\\
                ultraviolet light\\
                x-rays
\end{tabular}

<% end_hw() %>

<% marg(0) %>
<%
  fig(
    'comparephotons',
    %q{Problem \ref{hw:comparephotons}.}
  )
%>
\spacebetweenfigs
<%
  fig(
    'wavespeedingup',
    %q{Problem \ref{hw:wavespeedingup}.}
  )
%>
<% end_marg %>
<% begin_hw('comparephotons') %>__incl(hw/comparephotons)<% end_hw() %>


<% begin_hw('wavespeedingup') %>__incl(hw/wavespeedingup)<% end_hw() %>
 % 
<% begin_hw('projector') %>
The beam of a 100-W overhead projector covers an area of
$1\ \zu{m}\times1\ \zu{m}$ when it hits the screen 3 m away. Estimate
the number of photons that are in flight at any given time.
(Since this is only an estimate, we can ignore the fact that
the beam is not parallel.)\answercheck
<% end_hw() %>
 % 
<% begin_hw('pe') %>
In the photoelectric effect, electrons are observed with
virtually no time delay ($\sim10$ ns), even when the light
source is very weak. (A weak light source does however only
produce a small number of ejected electrons.) The purpose of
this problem is to show that the lack of a significant time
delay contradicted the classical wave theory of light, so
throughout this problem you should put yourself in the shoes
of a classical physicist and pretend you don't know about
photons at all. At that time, it was thought that the
electron might have a radius on the order of $10^{-15}$ m. 
(Recent experiments have shown that if the electron has any
finite size at all, it is far smaller.)\hwendpart
(a) Estimate the power that would be soaked up by a single
electron in a beam of light with an intensity of 1 $\zu{mW}/\zu{m}^2$.\answercheck\hwendpart
(b) The energy, $E_s$, required for the electron to escape
through the surface of the cathode is on the order of 
$10^{-19}$ J. Find how long it would take the electron to
absorb this amount of energy, and explain why your result
constitutes strong evidence that there is something wrong
with the classical theory.\answercheck
<% end_hw() %>
 % 
<% begin_hw('tv') %>__incl(hw/tv)<% end_hw() %>
 % 
<% begin_hw('lead') %>
Use the Heisenberg uncertainty principle to estimate the
minimum velocity of a proton or neutron in a $^{208}\zu{Pb}$
nucleus, which has a diameter of about 13 fm (1 fm=$10^{-15}$
 m). Assume that the speed is nonrelativistic, and then
check at the end whether this assumption was warranted.\answercheck
<% end_hw() %>
 % 
<% begin_hw('particleinabox') %>__incl(hw/particleinabox)<% end_hw() %>
 % 
<% begin_hw('chip-qm') %>
A free electron that contributes to the current in an
ohmic material typically has a speed of $10^5$  m/s (much
greater than the drift velocity).\hwendpart
(a) Estimate its de Broglie wavelength, in nm.\answercheck\hwendpart
(b) If a computer memory chip contains $10^8$  electric
circuits in a 1 $\zu{cm}^2$ area, estimate the linear size, in
nm, of one such circuit.\answercheck\hwendpart
(c) Based on your answers from parts a and b, does an
electrical engineer designing such a chip need to worry
about wave effects such as diffraction?\hwendpart
(d) Estimate the maximum number of electric circuits that
can fit on a 1 $\zu{cm}^2$ computer chip before quantum-mechanical
effects become important.
<% end_hw() %>
 % 
<% begin_hw('quantumho') %>__incl(hw/quantumho)<% end_hw() %>
 % 
<% begin_hw('hydrogen-scale') %>__incl(hw/hydrogen-scale)<% end_hw() %>
 % 
<% marg(80) %>
<%
  fig(
    'hw-h-transitions',
    %q{Problem \ref{hw:hydrogenlevels}.}
  )
%>
<% end_marg %>
<% begin_hw('hydrogenlevels') %>__incl(hw/hydrogenlevels)<% end_hw() %>

<% begin_hw('hydrogenphoton') %>
Find an equation for the wavelength of the photon emitted
when the electron in a hydrogen atom makes a transition from
energy level $n_1$ to level $n_2$.\answercheck
<% end_hw() %>
 % 
<% begin_hw('basketball') %>__incl(hw/basketball)<% end_hw() %>
 % 
<% begin_hw('hydrogennonrelativistic') %>
Assume that the kinetic energy of an electron in the $n=1$
state of a hydrogen atom is on the same order of magnitude
as the absolute value of its total energy, and estimate a
typical speed at which it would be moving. (It cannot really
have a single, definite speed, because its kinetic and
interaction energy trade off at different distances from the
proton, but this is just a rough estimate of a typical
speed.) Based on this speed, were we justified in assuming
that the electron could be described nonrelativistically?
<% end_hw() %>


<% begin_hw('energysum') %>__incl(hw/energysum)<% end_hw() %>

<% begin_hw('electroninproton') %>
The wavefunction of the electron in the ground state
of a hydrogen atom is 
\begin{equation*}
  \Psi  =  \pi^{-1/2} a^{-3/2} e^{-r/a}\eqquad,
\end{equation*}
where $r$ is the distance from the proton, and $a=5.3\times10^{-11}$ m
is a constant that sets the size of the wave. \hwendpart
(a) Calculate symbolically, without plugging in numbers, the
probability that at any moment, the electron is inside the
proton.  Assume the proton is a sphere with a radius of
$b=0.5$ fm. Do not use calculus. [Hint: Does it matter if you plug in $r=0$ or
$r=b$ in the equation for the wavefunction?]\answercheck\hwendpart
(b) Calculate the probability numerically.\answercheck\hwendpart
(c) Based on the equation for the wavefunction, is it valid
to think of a hydrogen atom as having a finite size? Can $a$
be interpreted as the size of the atom, beyond which there
is nothing? Or is there any limit on how far the electron
can be from the proton?
<% end_hw() %>
 % 
<% begin_hw('hydrogenlike') %>__incl(hw/hydrogenlike)<% end_hw() %>
 % 
<% begin_hw('muonic-spectrum') %>
A muon is a subatomic particle that acts exactly like
an electron except that its mass is 207 times greater. Muons
can be created by cosmic rays, and it can happen that one of
an atom's electrons is displaced by a muon, forming a muonic
atom. If this happens to a hydrogen atom, the resulting
system consists simply of a proton plus a muon.\hwendpart
 (a) Based on the results of section \ref{subsec:hydrogen-energies},
how would the size of a muonic hydrogen atom in its ground state
compare with the size of the normal atom?\hwendpart
 (b) If you were
searching for muonic atoms in the sun or in the earth's
atmosphere by spectroscopy, in what part of the electromagnetic
spectrum would you expect to find the absorption lines?
<% end_hw() %>
 % 
 % 
 % 
<% begin_hw('compton') %>__incl(hw/compton)<% end_hw() %>
 % 
<% begin_hw('compton-any-angle') %>__incl(hw/compton-any-angle)<% end_hw() %>
 % 

<% begin_hw('wkb',2) %>
On page \pageref{quantitativetunneling} we derived an expression for the
probability that a particle would tunnel through a
rectangular barrier, i.e., a region in which the interaction
energy $U(x)$ has a graph that looks like a rectangle. Generalize this to a barrier
of any shape. [Hints: First try generalizing to two
rectangular barriers in a row, and then use a series of
rectangular barriers to approximate the actual curve of an
arbitrary function $U(x)$. Note that the width and height of the
barrier in the original equation occur in such a way that
all that matters is the area under the $U$-versus-$x$
curve. Show that this is still true for a series of
rectangular barriers, and generalize using an integral.]  If
you had done this calculation in the 1930's you could have
become a famous physicist.
<% end_hw() %>

<% begin_hw('h-atom-normalization') %>__incl(hw/h-atom-normalization)<% end_hw() %>

<% begin_hw('three-d-forbidden') %>__incl(hw/three-d-forbidden)<% end_hw %>

<% begin_hw('three-d-box') %>
Find the energy levels of a particle in a three-dimensional rectangular box with sides
of length $a$, $b$, and $c$.\answercheck
<% end_hw %>

\pagebreak

<% begin_hw('tritium-decay') %>
Americium-241 is an artificial isotope used in smoke detectors. It undergoes alpha decay, with a half-life of 432 years.
As discussed in example \ref{eg:alpha-tunneling} on page \pageref{eg:alpha-tunneling}, alpha decay can be understood
as a tunneling process, and although the barrier is not rectangular in shape, the equation for the tunneling
probability on page \pageref{tunneling-probability} can still be used as a rough guide to our thinking.
For americium-241, the tunneling probability is about $1\times10^{-29}$. % http://hyperphysics.phy-astr.gsu.edu/hbase/Nuclear/alpdec.html
Suppose that this nucleus were to decay by emitting a tritium (helium-3) nucleus instead of an alpha particle (helium-4).
Estimate the relevant tunneling probability, assuming that the total energy $E$ remains the same.
This higher probability is contrary to the empirical observation that this
nucleus is not observed to decay by tritium emission with any significant probability, and in general
tritium emission is almost unknown in nature; this is mainly because the tritium nucleus is far less stable
than the helium-4 nucleus, and the difference in binding energy reduces the energy available for the decay.
<% end_hw %>

<% begin_hw('photon-mass') %>__incl(hw/photon-mass)<% end_hw %>

<% begin_hw('hydrogen-ratio') %>__incl(hw/hydrogen-ratio)<% end_hw() %>

<% begin_hw('compare-photons') %>__incl(hw/compare-photons)<% end_hw() %>

\pagebreak

<% begin_hw('generalize-hydrogen') %>__incl(hw/generalize-hydrogen)<% end_hw() %>
<% marg(150) %>
<%
  fig(
    'hw-generalize-hydrogen',
    %q{Problem \ref{hw:generalize-hydrogen}.}
  )
%>
<% end_marg %>

<% begin_hw('neutron-as-proton-plus-electron') %> __incl(hw/neutron-as-proton-plus-electron)<% end_hw() %>

\pagebreak

<% begin_hw('easy-electron-normalization') %>__incl(hw/easy-electron-normalization)<% end_hw() %>

<% begin_hw('easy-partials') %>__incl(hw/easy-partials)<% end_hw() %>

<% begin_hw('am-radio-photon-density') %>__incl(hw/am-radio-photon-density)<% end_hw() %>

<% begin_hw('square-wavefunction') %>__incl(hw/square-wavefunction)<% end_hw() %>

<% begin_hw('bogus-laplacian') %>__incl(hw/bogus-laplacian)<% end_hw() %>


<% end_hw_sec %>

\begin{exsection}
 % ==================================================================== 
 % ==================================================================== 
 % ====================================================================  

\extitle{A}{Quantum Versus Classical Randomness}

\noindent 1. Imagine the classical version of the particle in a one-dimensional box. Suppose you insert the particle in the
 box and give it a known, predetermined energy, but a random initial position and a
 random direction of motion. You then pick a random later moment in time to see where it is.
 Sketch the resulting probability distribution by shading on top of a line segment. Does the probability distribution depend on energy?

\noindent 2. Do similar sketches for the first few energy levels of the quantum mechanical particle in a box, and compare with 1.  

\noindent 3. Do the same thing as in 1, but for a classical hydrogen atom in two dimensions, which acts just like a miniature solar
 system. Assume you're always starting out with the same fixed values of energy and angular momentum, but a position
 and direction of motion that are otherwise random. Do this for $L=0$, and compare with a real $L=0$ probability distribution for the hydrogen atom.

\noindent 4. Repeat 3 for a nonzero value of $L$, say L=$\hbar$.

\noindent 5. Summarize: Are the classical probability distributions 
accurate? What qualitative features are possessed by the classical diagrams but not by the quantum mechanical ones, or vice-versa?

\end{exsection}
<% end_chapter() %>
