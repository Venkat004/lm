We sometimes say that quantum mechanics is the set of rules for
describing the world of the very small, but this is a sketchy
generalization, like saying that terriers are untrainable.  How small
is very small? The only distance scales we've discussed have been
wavelengths, and there is no upper limit on wavelengths. The
wavelength of an FM radio photon is bigger than my terrier, who is
very obedient to Newton's laws.  The only scale built in to the
structure of quantum mechanics is Planck's constant, and Planck's
constant has units of joules per hertz, not meters, so it can't be
converted into a distance. 

<% marg(50) %>
<%
  fig(
    'beam-splitter',
    %q{%
      A photon hits a piece of glass that reflects half of the
      light and transmits the other half.
    }
  )
%>
<% end_marg %>

So quantum behavior can occur at any scale, even large ones. For an example
that may be more disturbing, consider the arrangement shown in figure
\figref{beam-splitter}.  A single photon comes
in from the left and encounters a diagonal piece of glass. The glass reflects half
the light and transmits half of it. The photon is a wave, and this is normal wave
behavior. But the photon is also a particle, and we can't have half a particle.
Therefore either camera A will detect a whole photon and B will see none, or it
will be the other way around. The result is random.

There is no limit in principle on how far away the two cameras can be
placed.  (Real-world experiments of this type have been done over
distances of a thousand kilometers, with the photons traveling either
through outer space or through fiber-optic cables.) What may be
disquieting is the question of when the photon ``decides'' which
possibility to do. Clearly there is nothing to trigger this until the
photon interacts with something, presumably one of the cameras. But it
then seems as though the two parts of the wave are like a pair of
criminal suspects who would like to line up their stories but are
being kept in separate jail cells so that they can't communicate.  If
the part of the wave at A is going to be detected, how does the part
at B get the message that it had better not be detected? 

Niels Bohr and two collaborators proposed in 1924 the
seemingly reasonable solution that there \emph{can't} be any such
coordination. Then the random detection of the photon by camera A
and camera B would be independent. Independent probabilities multiply,
so there would be a probability of $(1/2)(1/2)=1/4$ that both cameras
would see photons. This would violate conservation of energy, since
the original energy $E=hf$ would have been detected twice, and the universe
would have gained $1hf$ worth of total energy. But Bohr
pointed out that there would also be the same probability that neither
camera would detect a photon, in which case the change in the universe's
energy would be $-1hf$. On the average, energy would be conserved. According
to Bohr's theory, conservation of energy and momentum would not be absolute
laws of physics but only rules that would be true on the average.

The experimentalists Geiger and Bothe immediately set out to test this
prediction. They performed an experiment analogous to the one in
figure \figref{beam-splitter}, but with x-rays rather than visible
light.  Their results, published in 1926, showed that if one detector
saw the x-ray photon, the other did not, so that energy was always
conserved at the microscopic level, not just on the average.

At a 1927 conference in Brussels, Einstein protested that this was a
problem, because the two detectors could in principle make their
observations simultaneously, and it would then seem that some
influence or communication was being transmitted between them faster
than the speed of light.  ``It seems to me,'' he complained, ''that
this difficulty cannot be overcome unless the description of the
process in terms of the \ldots wave is supplemented by some detailed
specification of the [trajectory of the particle]. \ldots If one works
only with \ldots waves, the interpretation \ldots, I think,
contradicts the postulate of relativity.'' Over the next decade he continued to work
up more detailed arguments to the effect that there was a problem with
this kind of faster-than-light ``spooky action at a distance.''

The experimental fact ends up being that the spooky action at a distance
exists, and it does go faster than light. In 2012, Guerreiro 
\emph{et al.}\footnote{\url{arxiv.org/abs/1204.1712}. The paper is very readable.}
carried out a very direct and conceptually simple enactment of exactly the experiment
in figure \figref{beam-splitter}, with electronic timing precise enough to
prove that the detection events at A and B were separated from each other by
too great a distance to have been linked by any influence traveling at $\le c$.
These findings are summarized by saying that quantum mechanics is \emph{nonlocal}.
A single wave-particle can have be spread out over an arbitrarily
large region of space, but its interactions that transfer energy and momentum
are always correlated over these distances in such a way that the conservation
laws are maintained.

What Einstein had not originally appreciated was that these correlations do
not violate relativity because they do not actually transport any energy, or even any
information, between A and B. For example, if Alice is at detector A, and Bob is at B,
a million kilometers away, Alice can detect the photon and know immediately that
Bob did not detect it. She learns something seemingly instantaneously about Bob --- Bob
is probably sad and disappointed right now. But because Bob does not have any
control over the result, he cannot use this fact to send a message to Alice,
so there is no transmission of information.

